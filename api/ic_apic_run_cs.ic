/* Copyright (C) 2009, 2015 iClaustron AB

   This program is free software; you can redistribute it and/or modify
   it under the terms of the GNU General Public License as published by
   the Free Software Foundation; version 2 of the License.

   This program is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
   GNU General Public License for more details.

   You should have received a copy of the GNU General Public License
   along with this program; if not, write to the Free Software
   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA */

#define IC_CS_HEARTBEAT_INTERVAL 2

static void inc_config_ref_count(IC_INT_RUN_CLUSTER_SERVER *run_obj);
static void dec_config_ref_count(IC_INT_RUN_CLUSTER_SERVER *run_obj);
static void check_ready_to_release_config(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                          gboolean lock_held);
static int check_for_stopped_rcs_threads(void *obj, int not_used);
static int rec_get_version_req(IC_CONNECTION *conn);
static int send_get_version_reply(IC_CONNECTION *conn,
                                  guint64 node_type);
static int rec_get_config_req(IC_CONNECTION *conn,
                              guint64 *version_number,
                              guint64 node_type);
static int send_get_config_reply(IC_CONNECTION *conn,
                                 gchar *config_base64_str,
                                 guint32 config_len);
static int ic_get_base64_config(IC_CLUSTER_CONFIG *clu_conf,
                                guint8 **base64_array,
                                guint32 *base64_array_len,
                                guint64 version_number);

/*
  MODULE: Support functions for Cluster Server
  --------------------------------------------

  get_my_cluster_info: Get my info cluster object
  get_cluster_info: Get any info cluster object
  get_master_node_id: Get node id of master Cluster Server
  ic_cs_master: Are we master Cluster Server
  get_count_cluster_servers: Get current count of Cluster Servers
  get_any_cluster_config: Get any Cluster Config to get config of
    Cluster Servers.
  find_cs_index: Find Cluster Server index in cs_servers array
  get_socket_link_config: Get Socket communication data structure for
    communication between two nodes in a specified cluster.
*/

static IC_INFO_CLUSTER_SERVER*
get_my_cluster_info(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  return &run_obj->state.cs_servers[0];
}

static IC_INFO_CLUSTER_SERVER*
get_cluster_info(IC_INT_RUN_CLUSTER_SERVER *run_obj, guint32 cs_index)
{
  return &run_obj->state.cs_servers[cs_index];
}

static guint32
get_master_node_id(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  return run_obj->state.cs_servers[run_obj->state.master_cs_index].node_id;
}

static gboolean
is_cs_master(IC_RUN_CLUSTER_STATE *rc_state)
{
  return (0 == rc_state->master_cs_index);
}

static guint32 get_count_cluster_servers(IC_CLUSTER_CONFIG *clu_conf)
{
  guint32 count= 0;
  guint32 i;

  /* Count number of Cluster Servers */
  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    if (clu_conf->node_types[i] == IC_CLUSTER_SERVER_NODE)
      count++;
  }
  ic_assert(count != 0 && count <= IC_MAX_CLUSTER_SERVERS);
  return count;
}

/*
  Find a cluster configuration, any will do since Cluster Servers have
  the same host, port and node id in all clusters.
*/
static IC_CLUSTER_CONFIG*
get_any_cluster_config(IC_RC_CONFIG_STATE *config)
{
  guint32 i= 0;
  IC_CLUSTER_CONFIG *clu_conf;

  for (i= 0; i <= config->max_cluster_id; i++)
  {
    if (!(clu_conf= config->conf_objects[i]))
      continue;
  }
  ic_assert(clu_conf);
  return clu_conf;
}

static int
find_cs_index(IC_INT_RUN_CLUSTER_SERVER *run_obj,
              guint32 node_id,
              guint32 *index)
{
  guint32 i;

  for (i= 0; i < run_obj->config.num_cluster_servers; i++)
  {
    if (get_cluster_info(run_obj, i)->node_id == node_id)
    {
      *index= i;
      return 0;
    }
  }
  return IC_PROTOCOL_ERROR;
}

static int
get_socket_link_config(guint32 node1_id,
                       guint32 node2_id,
                       IC_SOCKET_LINK_CONFIG **comm_section,
                       IC_CLUSTER_CONFIG *clu_conf)
{
  int ret_code;
  IC_SOCKET_LINK_CONFIG search_comm_section;
  DEBUG_ENTRY("get_socket_link_config");

  /* Get the dynamic port number of the connection */
  search_comm_section.first_node_id= node1_id;
  search_comm_section.second_node_id= node2_id;
  ret_code= IC_ERROR_SET_CONNECTION_PARAMETER_WRONG_NODES;
  if (!(*comm_section= (IC_SOCKET_LINK_CONFIG*)
       ic_hashtable_search(clu_conf->comm_hash, (void*)&search_comm_section)))
  {
    DEBUG_RETURN_INT(ret_code);
  }
  DEBUG_RETURN_INT(0);
}

/*
  MODULE: Protect Cluster Server State
  ------------------------------------
*/
static gboolean rc_start_update(IC_RUN_CLUSTER_STATE *rc_state);
static void rc_stop_update(IC_RUN_CLUSTER_STATE *rc_state);

static gboolean
rc_start_update(IC_RUN_CLUSTER_STATE *rc_state)
{
  gboolean is_master;

  ic_mutex_lock(rc_state->protect_state);
  is_master= is_cs_master(rc_state);
  /*
    We are the master of the Cluster Servers which means all updates
    need to start in our Cluster Server.
  */
  while (rc_state->update_state)
  {
    /*
      Someone is already performing an update, we need to wait until
      this update is completed, the Cluster Servers can only handle
      one update at a time.
    */
    rc_state->update_waiters++;
    ic_cond_wait(rc_state->update_cond, rc_state->protect_state);
    ic_assert(!rc_state->update_state);
  }
  rc_state->update_state= TRUE;
  ic_mutex_unlock(rc_state->protect_state);
  return is_master;
}

static void
rc_stop_update(IC_RUN_CLUSTER_STATE *rc_state)
{
  ic_mutex_lock(rc_state->protect_state);
  ic_assert(rc_state->update_state);
  if (rc_state->update_waiters)
  {
    rc_state->update_waiters--;
    ic_cond_signal(rc_state->update_cond);
  }
  rc_state->update_state= FALSE;
  ic_mutex_unlock(rc_state->protect_state);
}

/*
  MODULE: Stop Cluster Server
  ---------------------------
  This module is a support module to the Run Cluster Server that implements
  the to stop the Cluster Server by unlocking the configuration and
  communicating the close down to the other Cluster Servers in the grid.
  It implements the stop_cluster_server method in the Run Cluster Server
  interface.
*/
static int unlock_cv_file(IC_INT_RUN_CLUSTER_SERVER *run_obj);

/* Implements the ic_stop_cluster_server method */
static int
stop_cluster_server(IC_RUN_CLUSTER_SERVER *ext_run_obj)
{
  IC_INT_RUN_CLUSTER_SERVER *run_obj= (IC_INT_RUN_CLUSTER_SERVER*)ext_run_obj;
  DEBUG_ENTRY("stop_cluster_server");

  DEBUG_RETURN_INT(unlock_cv_file(run_obj));
}

static int
unlock_cv_file(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  int ret_code= 0;

  if (run_obj->locked_configuration)
  {
    ret_code= write_config_version_file(run_obj->config_dir,
                       get_my_cluster_info(run_obj)->config_version_number,
                              CONFIG_STATE_IDLE,
                              (guint32)0);
  }
  return ret_code;
}

/*
  MODULE: Start Cluster Server
  ----------------------------
  This module is a support module to the Run Cluster Server that implements
  the method to start the Cluster Server by reading the configuration from
  disk and synchronizing with any Cluster Servers already up and running.

  Cluster Server Start Options:
  -----------------------------

  The very first start of a Cluster Server always expects to find the same
  configuration file in all Cluster Server nodes. The idea is that the
  user will write those configuration file in a tool that assists them in
  writing the needed configuration files. Then the cluster client can
  read those files and connect to the process controller on each respective
  node. The process controller will write the files into the correct place
  of the files.

  The process to bootstrap a cluster is the following:

  Installation prerequisites:
  ---------------------------
  1) Install the iClaustron binaries into all machines where the iClaustron
     Grid will be executing.
  2) Start process controller on all machines (some machines might even need
     multiple process controller, one for each port they listen to).
  3) Ensure that all ports to be used by process controllers, Cluster Servers,
     Cluster Managers, SQL Servers, replication servers and data servers are
     not blocked by any firewall application.

  Configuration building
  ----------------------
  4) Start the configuration builder tool in an appropriate directory
  5) Create an initial configuration
  6) The tool writes the configuration files into the directory where the
     tool executes.

  Distribute configuration files
  ------------------------------
  7) Issue the command to distribute the configuration files to the
     appropriate places using the already started process controllers
     to ensure there is an allowed path to distribute the files. This
     means distributing all configuration files to machines running
     the Cluster Servers and the grid configuration files to all machines.

  Start the Cluster Servers and Managers
  --------------------------------------
  8) Issue the command in the configuration builder tool to start the
     Cluster Servers for the first time.
  9) Issue the command in the configuration builder tool to start the
     Cluster Managers for the first time.

  iClaustron expects that Cluster Servers and Cluster Managers for a
  specific Grid is kept up and running all the time. The reason for this is
  that the configuration builder tool only have access to up-to-date
  Grid configuration at initial start. After initial start the configuration
  can change in every way possible. Thus it is harder to restart the Grid
  if Cluster Servers and/or Cluster Managers are no longer available.

  To restart cluster servers and cluster managers if all cluster servers
  and/or all cluster managers are stopped we use a special tool that can
  start these up using the process controller. The user of this tool must
  however have knowledge of the current placement of the Cluster Servers. 

  Start of a Cluster Server
  -------------------------
  When starting the Cluster Server one will read the configuration version
  file, the common grid configuration file, the cluster configuration file for
  this version and the configuration files for each cluster in this version.

  After reading the local configuration files a node will attempt to connect
  to any other Cluster Servers. The node will wait until a quorum has been
  formed before the start is completed.

  If connect is successful and the version read from the connected server is
  equal to our own read version, then we will fetch configuration from the
  server and verify its correctness. If it's unequal then we'll fetch
  configuration from the server connected, verify the received configuration,
  install the new configuration, update the configuration version file,
  remove the old configuration version, update the configuration version
  file again to indicate the old version is removed.

  If connect was unsuccessful and we had local configuration files then we'll
  start-up our server connection. After that we'll in parallel make more
  attempts to connect as clients to the other Cluster Servers while we're
  also allowing other nodes to connect to us.

  If no other Cluster Server is heard from then we'll start replying to
  any requests from other nodes, also other nodes than Cluster Servers.
  If a Cluster Server contacted us through the server interface while we
  were unsuccessful in contacting this node through the client interface,
  then we'll synchronize with this Cluster Server. If we received a
  connection in parallel with managing to connect to the same Cluster
  Server we'll synchronize with this Cluster Server.

  The names of the configuration files is fixed, it is always config.ini for
  the cluster configuration file, and it will be config.version for the file
  that contains the version of the current configuration. If the version is
  3 then the files created will be called config.ini.3 and the configuration
  files of the cluster will always be called the name of the cluster + .ini.
  Thus for a cluster called kalle it will kalle.ini and versioned it will be
  kalle.ini.3.

  The only parameter thus needed for the Cluster Server is which directory
  those files are stored in. The remaining information is always the same
  or provided in configuration files.

  The implementation starts by locking the configuration and retrieving the
  configuration version number by using the ic_load_config_version from
  another module.

  The next step is to load the configuration files from disk by using the
  method:
    load_local_config
  This method uses the ic_load_cluster_config_from_file to retrieve the
  grid configuration, this resides in another module. Then it uses the
    load_config_files
   method to load each cluster configuration, this method loops over all
   the clusters and loads each cluster configuration using the method
   ic_load_config_server_from files which also resides in another module.

   In a bootstrap situation it does also write version 1 of the configuration
   using the method ic_write_full_config_to_disk from another module.
*/

static int set_up_apic(IC_INT_RUN_CLUSTER_SERVER *run_obj);
static void set_up_run_obj(IC_INT_RUN_CLUSTER_SERVER *run_obj);
static int load_local_config(IC_INT_RUN_CLUSTER_SERVER *run_obj);
static int load_config_files(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                             IC_CLUSTER_CONNECT_INFO **clu_infos);
static int start_connect_server_thread(
                             IC_INT_RUN_CLUSTER_SERVER *run_obj);
static int start_connect_other_cluster_servers(
                             IC_INT_RUN_CLUSTER_SERVER *run_obj);
static int handle_cluster_server_up_and_down(
                             IC_INT_RUN_CLUSTER_SERVER *run_obj);
static gpointer connect_server_thread(gpointer data);
static gpointer start_connect_missing_cluster_servers(gpointer data);
static gpointer start_cs_func(gpointer data);

/*
  Set up the run cluster server object with initial data based on
  cluster configurations.
*/
static void
set_up_run_obj_master(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 i;
  guint32 used_index;
  guint32 cs_index= 0;
  IC_CLUSTER_SERVER_CONFIG *cs_conf;
  IC_CLUSTER_CONFIG *clu_conf= get_any_cluster_config(&run_obj->new_config);
  DEBUG_ENTRY("set_up_run_obj_master");

  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    if (clu_conf->node_types[i] == IC_CLUSTER_SERVER_NODE)
    {
      cs_conf= (IC_CLUSTER_SERVER_CONFIG*)clu_conf->node_config[i];
      if (cs_conf->node_id == run_obj->cs_nodeid)
      {
        used_index= 0;
      }
      else
      {
        used_index= ++cs_index;
      }
      get_cluster_info(run_obj, used_index)->master_index=
        IC_MAX_CLUSTER_SERVERS;
    }
  }
  run_obj->state.master_cs_index= IC_MAX_CLUSTER_SERVERS;
  DEBUG_RETURN_EMPTY;
}

static void
set_up_run_obj(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 cs_index= 0;
  guint32 used_index;
  guint32 i;
  IC_CLUSTER_SERVER_CONFIG *cs_conf;
  IC_CLUSTER_CONFIG *clu_conf= get_any_cluster_config(&run_obj->new_config);
  DEBUG_ENTRY("set_up_run_obj");

  run_obj->new_config.num_cluster_servers=
    get_count_cluster_servers(clu_conf);
  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    if (clu_conf->node_types[i] == IC_CLUSTER_SERVER_NODE)
    {
      cs_conf= (IC_CLUSTER_SERVER_CONFIG*)clu_conf->node_config[i];
      if (cs_conf->node_id == run_obj->cs_nodeid)
      {
        used_index= 0;
      }
      else
      {
        used_index= ++cs_index;
      }
      get_cluster_info(run_obj, used_index)->node_id= cs_conf->node_id;
    }
  }
  DEBUG_RETURN_EMPTY;
}

static void
free_apic_from_run_cluster(IC_API_CONFIG_SERVER *ext_apic)
{
  guint32 i;
  IC_INT_API_CONFIG_SERVER *apic= (IC_INT_API_CONFIG_SERVER*)ext_apic;
  DEBUG_ENTRY("free_apic_from_run_cluster");

  for (i= 0; i < IC_MAX_CLUSTER_ID; i++)
  {
    if (apic->conf_objects[i])
    {
      ic_hashtable_destroy(apic->conf_objects[i]->comm_hash, FALSE);
      apic->conf_objects[i]= NULL;
    }
  }
  DEBUG_RETURN_EMPTY;
}

/*
  Ensure that also run cluster server has API config object and
  set it up correctly.
*/
static int
set_up_apic(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  IC_INT_API_CONFIG_SERVER *apic=
    (IC_INT_API_CONFIG_SERVER*)run_obj->new_config.apic;
  IC_CLUSTER_CONFIG *clu_conf;
  IC_CLUSTER_SERVER_CONFIG *cs_conf;
  guint32 i, cs_index, dummy, num_cluster_servers, used_index;
  gchar buf[IC_NUMBER_SIZE], *buf_ptr, *port_number_str;
  int ret_code;
  gboolean found_cs_nodeid= FALSE;
  IC_MEMORY_CONTAINER *mc_ptr= run_obj->conf_mc_ptr;
  IC_API_CLUSTER_CONNECTION *cluster_conn= &apic->cluster_conn;
  DEBUG_ENTRY("set_up_apic");

  set_up_apic_methods(apic);
  apic->api_op.ic_get_config= null_get_cs_config;
  apic->api_op.ic_free_config= free_apic_from_run_cluster;

  apic->max_cluster_id= run_obj->new_config.max_cluster_id;
  apic->use_ic_cs= TRUE;
  memcpy(apic->conf_objects,
         run_obj->new_config.conf_objects,
         sizeof(apic->conf_objects));
  apic->mc_ptr= run_obj->conf_mc_ptr;

  clu_conf= get_any_cluster_config(&run_obj->new_config);
  cluster_conn->num_cluster_servers= get_count_cluster_servers(clu_conf);
  ret_code= IC_ERROR_MEM_ALLOC;
  num_cluster_servers= cluster_conn->num_cluster_servers;
  if (!((cluster_conn->cluster_server_ips= (gchar**)
          mc_ptr->mc_ops.ic_mc_calloc(mc_ptr,
                          sizeof(gchar*)*num_cluster_servers)) &&
        (cluster_conn->cluster_server_ports= (gchar**)
          mc_ptr->mc_ops.ic_mc_calloc(mc_ptr,
                          sizeof(gchar*)*num_cluster_servers)) &&
        (cluster_conn->cs_nodeid= (guint32*)
          mc_ptr->mc_ops.ic_mc_calloc(mc_ptr,
                          sizeof(guint32)*num_cluster_servers))))
    goto error;
  cs_index= 0;
  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    if (clu_conf->node_types[i] == IC_CLUSTER_SERVER_NODE)
    {
      /* We found a Cluster Server Node, record it in cluster_conn */
      cs_conf= (IC_CLUSTER_SERVER_CONFIG*)clu_conf->node_config[i];
      if (cs_conf->node_id == run_obj->cs_nodeid)
      {
        used_index= 0;
        found_cs_nodeid= TRUE;
      }
      else
      {
        used_index= ++cs_index;
      }
      cluster_conn->cluster_server_ips[used_index]= cs_conf->hostname;
      buf_ptr= ic_guint64_str((guint64)cs_conf->cluster_server_port_number,
                              buf,
                              &dummy);
      if ((ret_code= ic_mc_chardup(mc_ptr, &port_number_str, buf_ptr)))
        goto error;
      cluster_conn->cluster_server_ports[used_index]= port_number_str;
      cluster_conn->cs_nodeid[used_index]= cs_conf->node_id;
    }
  }
  if (!found_cs_nodeid)
  {
    DEBUG_RETURN_INT(IC_ERROR_NO_SUCH_CLUSTER_SERVER_NODEID);
  }
  DEBUG_RETURN_INT(0);
error:
  DEBUG_RETURN_INT(ret_code);
}

typedef struct thread_data
{
  IC_INT_RUN_CLUSTER_SERVER *run_obj;
  guint32 cs_index;
} THREAD_DATA;

static int
load_local_config(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  int ret_code;
  IC_CLUSTER_CONNECT_INFO **clu_infos;
  IC_CLUSTER_CONFIG *clu_conf;
  IC_INFO_CLUSTER_SERVER *info_cs= get_my_cluster_info(run_obj);
  DEBUG_ENTRY("load_local_config");

  if (!(clu_infos= ic_load_cluster_config_from_file(run_obj->config_dir,
                                info_cs->config_version_number,
                                run_obj->conf_mc_ptr,
                                &run_obj->state.err_obj)))
  {
    DEBUG_RETURN_INT(run_obj->state.err_obj.err_num);
  }

  run_obj->new_config.clu_infos= clu_infos;

  if ((ret_code= load_config_files(run_obj, clu_infos)))
  {
    DEBUG_RETURN_INT(ret_code);
  }

  /*
    At this point we're ready to confirm that we've been started with a
    valid node id. It needs to be a node id which is a Cluster Server.
    Possibly we should also find a way to check that the node's hostname
    can be reached from this computer, otherwise we've been started on
    the wrong host.

    If we make it possible to change the hostname and/or node id of a
    Cluster Server, then this needs to be carefully considered how this
    will be treated in all error cases. It would definitely be a change
    that can only be done if all Cluster Servers are up and running and
    the PREPARE state needs to be taken into consideration properly.
  */
  clu_conf= get_any_cluster_config(&run_obj->new_config);
  if (clu_conf->node_types[run_obj->cs_nodeid] != IC_CLUSTER_SERVER_NODE)
  {
    /* We've been started with an incorrect node id */
    ic_printf("Error: The Cluster Server have been started with a node id"
              " which isn't a Cluster Server in the configuration");
    DEBUG_RETURN_INT(IC_ERROR_CS_STARTED_WITH_WRONG_NODEID);
  }

  if (info_cs->config_version_number == 0)
  {
    if ((ret_code= ic_write_full_config_to_disk(run_obj->config_dir,
                              &info_cs->config_version_number,
                              clu_infos,
                              run_obj->new_config.conf_objects)))
    {
      DEBUG_RETURN_INT(ret_code);
    }
  }
  DEBUG_RETURN_INT(0);
}

static int
load_config_files_error(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                        IC_CONFIG_ERROR *err_obj)
{
  DEBUG_ENTRY("load_config_files_error");
  ic_mutex_lock(run_obj->state.protect_state);
  if (!run_obj->state.err_obj.err_num)
  {
    memcpy(&run_obj->state.err_obj, err_obj, sizeof(IC_CONFIG_ERROR));
  }
  ic_mutex_unlock(run_obj->state.protect_state);
  DEBUG_RETURN_INT(err_obj->err_num);
}

static int
load_config_files(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                  IC_CLUSTER_CONNECT_INFO **clu_infos)
{
  IC_CONFIG_ERROR err_obj;
  IC_CLUSTER_CONNECT_INFO *clu_info;
  IC_CLUSTER_CONFIG *cluster, *grid_cluster;
  IC_MEMORY_CONTAINER *mc_ptr= run_obj->conf_mc_ptr;
  IC_INFO_CLUSTER_SERVER *info_cs= get_my_cluster_info(run_obj);
  DEBUG_ENTRY("load_config_files");

  if (!(grid_cluster= ic_load_grid_common_config_server_from_file(
                                            run_obj->config_dir,
                                            info_cs->config_version_number,
                                            mc_ptr,
                                            *clu_infos,
                                            &err_obj)))
  {
    DEBUG_RETURN_INT(load_config_files_error(run_obj, &err_obj));
  }
  while (*clu_infos)
  {
    clu_info= *clu_infos;
    clu_infos++;

    /*
      We have now formed the filename of the configuration of this
      cluster. It's now time to open the configuration file and
      convert it into a IC_CLUSTER_CONFIG struct.
    */
    if (!(cluster= ic_load_config_server_from_files(run_obj->config_dir,
                                             info_cs->config_version_number,
                                             run_obj->conf_mc_ptr,
                                             grid_cluster,
                                             clu_info,
                                             &err_obj)))
    {
      DEBUG_RETURN_INT(load_config_files_error(run_obj, &err_obj));
    }

    /*
      Copy information from cluster configuration file which isn't set in
      the configuration and ensure it's allocated on the proper memory
      container.
    */
    if (ic_mc_strdup(mc_ptr, &cluster->clu_info.cluster_name,
                     &clu_info->cluster_name))
    {
      DEBUG_RETURN_INT(IC_ERROR_MEM_ALLOC);
    }
    if (ic_mc_strdup(mc_ptr, &cluster->clu_info.password,
                     &clu_info->password))
    {
      DEBUG_RETURN_INT(IC_ERROR_MEM_ALLOC);
    }

    cluster->clu_info.cluster_id= clu_info->cluster_id;
    cluster->my_node_id= run_obj->cs_nodeid;

    /* Update System section for handling NDB Management Protocol */
    cluster->sys_conf.system_name= cluster->clu_info.cluster_name.str;
    cluster->sys_conf.system_configuration_number=
      (guint32)info_cs->config_version_number;
    cluster->sys_conf.system_primary_cs_node= get_master_node_id(run_obj);

    if (run_obj->new_config.conf_objects[clu_info->cluster_id])
    {
      ic_hashtable_destroy(cluster->comm_hash, FALSE);
      DEBUG_RETURN_INT(IC_ERROR_CONFLICTING_CLUSTER_IDS);
    }
    DEBUG_PRINT(CONFIG_LEVEL, ("Adding cluster id: %u to new config",
                clu_info->cluster_id));
    ic_require(clu_info->cluster_id <= IC_MAX_CLUSTER_ID);
    run_obj->new_config.conf_objects[clu_info->cluster_id]= cluster;
    run_obj->new_config.max_cluster_id=
      IC_MAX(run_obj->new_config.max_cluster_id,
             clu_info->cluster_id);
    run_obj->new_config.num_clusters++;
  }
  DEBUG_RETURN_INT(0);
}

static int
start_connect_cs_threads(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  IC_THREADPOOL_STATE *tp_state= run_obj->tp_state;
  THREAD_DATA thread_data;
  guint32 i;
  guint32 num_threads_started= 0;
  int ret_code;
  IC_INFO_CLUSTER_SERVER *info_cs;
  gboolean thread_started_flag[IC_MAX_CLUSTER_SERVERS];
  IC_INT_API_CONFIG_SERVER *apic= run_obj->config.apic;
  DEBUG_ENTRY("start_connect_cs_threads");

  /* Make sure we start threads based on one state */
  ic_mutex_lock(run_obj->state.protect_state);
  for (i= 0; i < apic->cluster_conn.num_cluster_servers; i++)
  {
    ic_require(i < IC_MAX_CLUSTER_SERVERS);
    thread_started_flag[i]= FALSE;
    info_cs= get_cluster_info(run_obj, i);
    if ((apic->cluster_conn.cs_nodeid[i] == run_obj->cs_nodeid) ||
        info_cs->is_start_thread_active)
      continue;
    num_threads_started++;
    thread_data.run_obj= run_obj;
    thread_data.cs_index= i;
    info_cs->is_start_thread_active= TRUE;
    DEBUG_PRINT(THREAD_LEVEL,
      ("Start new thread with index %u in start_cs_func", i));
    if ((ret_code= tp_state->tp_ops.ic_threadpool_start_thread(
                                  tp_state,
                                  &info_cs->thread_id,
                                  start_cs_func,
                                  &thread_data,
                                  IC_MEDIUM_STACK_SIZE,
                                  TRUE)))
    {
      ic_mutex_unlock(run_obj->state.protect_state);
      DEBUG_RETURN_INT((-ret_code));
    }
    thread_started_flag[i]= TRUE;
  }
  ic_mutex_unlock(run_obj->state.protect_state);

  /* Now start all threads at the same time */
  for (i= 0; i < apic->cluster_conn.num_cluster_servers; i++)
  {
    if (thread_started_flag[i])
    {
      info_cs= get_cluster_info(run_obj, i);
      tp_state->tp_ops.ic_threadpool_run_thread(tp_state,
                                                info_cs->thread_id);
    }
  }
  DEBUG_RETURN_INT(num_threads_started);
}

static int
send_get_cs_node_id_req(IC_CONNECTION *conn)
{
  int ret_code;
  DEBUG_ENTRY("send_get_cs_node_id_req");

  if ((ret_code= ic_send_with_cr(conn, get_cs_nodeid_str)) ||
      (ret_code= ic_send_empty_line(conn)))
  {
    DEBUG_RETURN_INT(ret_code);
  }
  DEBUG_RETURN_INT(0);
}

static int
rec_get_cs_node_id_reply(IC_CONNECTION *conn, guint32 *node_id)
{
  int ret_code;
  DEBUG_ENTRY("rec_get_cs_node_id_reply");

  while (1)
  {
    if ((ret_code= ic_rec_simple_str(conn, get_cs_nodeid_reply_str)))
    {
      /*
         The client have connected but won't start the protocol until
         a sufficient amount of cluster servers have connected. Thus
         we will continue waiting even if a timeout occurs.
      */
      if (ret_code == IC_ERROR_RECEIVE_TIMEOUT)
        continue;
      DEBUG_RETURN_INT(ret_code);
    }
    break;
  }
  if ((ret_code= ic_rec_number(conn, nodeid_str, node_id)) ||
      (ret_code= ic_rec_empty_line(conn)))
  {
    DEBUG_RETURN_INT(ret_code);
  }
  DEBUG_RETURN_INT(0);
}

static int
check_proper_connection(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                        IC_RUN_CLUSTER_STATE *rc_state,
                        IC_INT_API_CONFIG_SERVER *apic,
                        IC_CONNECTION *conn,
                        guint32 client_node_id)
{
  guint32 i;
  IC_INFO_CLUSTER_SERVER *info_cs;
  gboolean equal;
  gchar buf[ERROR_MESSAGE_SIZE];
  DEBUG_ENTRY("check_proper_connection");

  /*
    Search for a Cluster Server which isn't yet connected that should
    connect from this address (ip address). It also needs to reply with
    a node id that is waiting to be connected.
  */
  ic_mutex_lock(rc_state->protect_state);
  for (i= 1; i < run_obj->config.num_cluster_servers; i++)
  {
    equal= FALSE;
    info_cs= get_cluster_info(run_obj, i);
    if (info_cs->wait_connect_server_thread)
    {
      if (!((conn)->conn_op.ic_check_connection(conn,
             apic->cluster_conn.cluster_server_ips[i],
             &equal)) &&
             equal &&
             info_cs->node_id == client_node_id)
      {
        DEBUG_PRINT(CONFIG_LEVEL, ("Connect from node id = %u",
                                   client_node_id));
        info_cs->conn= conn;
        info_cs->wait_connect_server_thread= FALSE;
        info_cs->is_client_side= FALSE;
        ic_cond_broadcast(rc_state->connect_cond);
        ic_mutex_unlock(rc_state->protect_state);
        DEBUG_RETURN_INT(0);
      }
    }
  }
  ic_mutex_unlock(rc_state->protect_state);
  g_snprintf(buf,
             sizeof(buf),
             "Not waiting for Cluster Server connect from %s with node id %u",
             conn->conn_stat.client_ip_addr_str,
             client_node_id);
  ic_printf(buf);
  ic_send_error_message(conn, buf);
  DEBUG_RETURN_INT(1);
}

static gpointer
connect_server_thread(gpointer data)
{
  IC_THREAD_STATE *thread_state= (IC_THREAD_STATE*)data;
  IC_THREADPOOL_STATE *tp_state;
  IC_INT_RUN_CLUSTER_SERVER *run_obj;
  IC_RUN_CLUSTER_STATE *rc_state;
  IC_CONNECTION *conn= NULL;
  IC_CONNECTION *fork_conn;
  IC_INT_API_CONFIG_SERVER *apic;
  IC_INFO_CLUSTER_SERVER *info_cs;
  gboolean found;
  guint32 i, cs_index;
  guint32 client_node_id;
  int ret_code;
  DEBUG_THREAD_ENTRY("connect_server_thread");

  tp_state= thread_state->ic_get_threadpool(thread_state);
  run_obj= (IC_INT_RUN_CLUSTER_SERVER*)
    tp_state->ts_ops.ic_thread_get_object(thread_state);
  tp_state->ts_ops.ic_thread_started(thread_state);
  if (tp_state->ts_ops.ic_thread_startup_done(thread_state))
    goto end;

  rc_state= &run_obj->state;
  /*
    If it becomes allowed to change the address settings for Cluster
    Servers while running, then this code need to be adapted to
    handle this.
  */
  apic= run_obj->config.apic;

  if (!(conn= ic_create_socket_object(FALSE, FALSE, FALSE,
                                      CONFIG_READ_BUF_SIZE)))
  {
    ret_code= IC_ERROR_MEM_ALLOC;
    goto error;
  }

  conn->conn_op.ic_prepare_server_connection(conn,
                         apic->cluster_conn.cluster_server_ips[0],
                         apic->cluster_conn.cluster_server_ports[0],
                         NULL,
                         NULL,
                         0,
                         TRUE);

  if ((ret_code= conn->conn_op.ic_set_up_connection(conn,
                           check_for_stopped_rcs_threads,
                           (void*)tp_state)))
    goto error;

  do
  {
    found= FALSE;
    ic_mutex_lock(rc_state->protect_state);
    if (tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
    {
      ic_mutex_unlock(rc_state->protect_state);
      goto end;
    }
    for (i= 1; i < run_obj->config.num_cluster_servers; i++)
    {
      info_cs= get_cluster_info(run_obj, i);
      if (info_cs->wait_connect_server_thread)
      {
        found= TRUE;
        break;
      }
    }
    if (!found)
    {
      /*
        No one is waiting for a connection so no need to start accepting
        connections yet.
      */
      DEBUG_PRINT(CONFIG_LEVEL,
        ("No one is waiting for connections, wait_server_connect_cond wait"));
      ic_cond_wait(rc_state->wait_server_connect_cond,
                   rc_state->protect_state);
      ic_mutex_unlock(rc_state->protect_state);
      continue;
    }
    ic_mutex_unlock(rc_state->protect_state);

    DEBUG_PRINT(CONFIG_LEVEL,
      ("Accept connections, we're still waiting for more connects"));
    if ((ret_code= conn->conn_op.ic_accept_connection(conn)))
    {
      ic_print_error(ret_code);
      ic_printf("Failed to accept cluster server connect");
      continue;
    }

    if (!(fork_conn=
          conn->conn_op.ic_fork_accept_connection(conn, FALSE)))
    {
      ic_print_error(IC_ERROR_MEM_ALLOC);
      ic_printf("Failed to fork cluster server connect");
      continue;
    }

    /* Ask the client for its node id to ensure it is properly identified */
    if ((ret_code= send_get_cs_node_id_req(fork_conn)) ||
        (ret_code= rec_get_cs_node_id_reply(fork_conn, &client_node_id)) ||
        (ret_code= check_proper_connection(run_obj,
                                           rc_state,
                                           apic,
                                           fork_conn,
                                           client_node_id)) ||
        (ret_code= ic_send_ok(fork_conn)) ||
        (ret_code= find_cs_index(run_obj, client_node_id, &cs_index)))
    {
      /* Connect from unknown place or error */
      if (ret_code != 1)
      {
        ic_print_error(ret_code);
        ic_printf("Failed in get cluster server node id protocol");
      }
      fork_conn->conn_op.ic_free_connection(fork_conn);
    }
    else
    {
      /*
         We have successfully started a connection and verified the client.
         We now need to inform the start-up code about that we have a new
         cluster server node that have connected as part of the startup
         protocol.We now need to inform the start-up code about that we have
         a new cluster server node that have connected as part of the startup
         protocol.
      */
      DEBUG_PRINT(CONFIG_LEVEL,
        ("Successfully received a connect, wake up startup thread"
         " for the node"));
      info_cs= get_cluster_info(run_obj, cs_index);
      ic_mutex_lock(rc_state->protect_state);
      info_cs->conn= fork_conn;
      ic_cond_signal(rc_state->connect_cond);
      ic_mutex_unlock(rc_state->protect_state);
    }
  } while (1);
end:
  if (conn)
  {
    conn->conn_op.ic_free_connection(conn);
  }
  DEBUG_THREAD_RETURN;

error:
  ic_print_error(ret_code);
  tp_state->tp_ops.ic_threadpool_set_stop_flag(tp_state);
  goto end;
}

static int
start_connect_server_thread(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  int ret_code;
  IC_THREADPOOL_STATE *tp_state= run_obj->tp_state;

  DEBUG_PRINT(THREAD_LEVEL, ("Start thread in connect_server_thread"));
  if ((ret_code= tp_state->tp_ops.ic_threadpool_start_thread(
       tp_state,
       &run_obj->connect_server_thread_id,
       connect_server_thread,
       (void*)run_obj,
       IC_MEDIUM_STACK_SIZE,
       TRUE)))
    goto end;
  tp_state->tp_ops.ic_threadpool_run_thread(tp_state,
                        run_obj->connect_server_thread_id);
end:
  return ret_code;
}

static int
handle_cluster_server_up_and_down(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  int ret_code;
  IC_THREADPOOL_STATE *tp_state= run_obj->tp_state;

  DEBUG_PRINT(THREAD_LEVEL,
              ("Start thread in start_connect_missing_cluster_servers"));
  ret_code= tp_state->tp_ops.ic_threadpool_start_thread(
       tp_state,
       &run_obj->bg_thread_id,
       start_connect_missing_cluster_servers,
       (void*)run_obj,
       IC_MEDIUM_STACK_SIZE,
       FALSE);
  return ret_code;
}

static gpointer
start_connect_missing_cluster_servers(gpointer data)
{
  IC_THREAD_STATE *thread_state= (IC_THREAD_STATE*)data;
  IC_THREADPOOL_STATE *tp_state;
  int num_threads_started;
  IC_INT_RUN_CLUSTER_SERVER *run_obj;
  IC_RUN_CLUSTER_STATE *rc_state;
  DEBUG_THREAD_ENTRY("start_connect_missing_cluster_servers");

  tp_state= thread_state->ic_get_threadpool(thread_state);
  run_obj= (IC_INT_RUN_CLUSTER_SERVER*)
    tp_state->ts_ops.ic_thread_get_object(thread_state);
  tp_state->ts_ops.ic_thread_started(thread_state);

  rc_state= &run_obj->state;

  do
  {
    num_threads_started= start_connect_cs_threads(run_obj);
    if (num_threads_started < 0)
    {
      goto end_thread;
    }
    /*
      We will wait here until the Cluster Server needs to restart start
      threads again.
    */
    if (tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
      goto end_thread;
    ic_mutex_lock(rc_state->protect_state);
    ic_cond_wait(rc_state->connect_cond, rc_state->protect_state);
    ic_mutex_unlock(rc_state->protect_state);
  } while (!tp_state->ts_ops.ic_thread_get_stop_flag(thread_state));

end_thread:
  tp_state->ts_ops.ic_thread_stops(thread_state);
  DEBUG_THREAD_RETURN;
}

static int
start_connect_other_cluster_servers(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  int num_threads_started;
  gboolean first_loop= TRUE;
  gboolean sleep_flag= FALSE;
  int ret_code;
  DEBUG_ENTRY("start_connect_other_cluster_servers");

  do
  {
    num_threads_started= start_connect_cs_threads(run_obj);
    if (num_threads_started < 0)
    {
      DEBUG_RETURN_INT((-num_threads_started));
    }

    /* Special handling of case with only 1 Cluster Server */
    if (num_threads_started == 0 && first_loop)
    {
      /*
        Startup is completed, we're the only Cluster Server, record state
        and proceed. We need no special threads to handle heartbeat to
        other Cluster Servers since we're the only Cluster Server in the
        configuration.
      */
      run_obj->state.master_cs_index= 0;
      run_obj->state.cs_started= TRUE;
      DEBUG_RETURN_INT(0);
    }

    /*
      We have at least 2 Cluster Servers in the configuration. We will
      wait here for the threads to signal completion. In some cases one
      or more thread decides to die and wants to be restarted for
      another attempt. We only need to start this thread once, it will
      run until the process stops.
    */
    if (first_loop &&
        (ret_code= start_connect_server_thread(run_obj)))
    {
      DEBUG_RETURN_INT((-ret_code));
    }
    
    ic_mutex_lock(run_obj->state.protect_state);
    do
    {
      DEBUG_PRINT(CONFIG_LEVEL,
                  ("Wait until threads have completed startup phase"));
      ic_cond_wait(run_obj->state.start_cond, run_obj->state.protect_state);
      if (!run_obj->state.cs_started && !run_obj->state.cs_starting)
      {
        sleep_flag= TRUE;
        DEBUG_PRINT(CONFIG_LEVEL, ("Start failed, we need to retry"));
        break;
      }
      if (run_obj->state.cs_started)
      {
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Start successful, synch done in run part if needed"));
        ic_mutex_unlock(run_obj->state.protect_state);
        DEBUG_RETURN_INT(0);
      }
      DEBUG_PRINT(CONFIG_LEVEL,
        ("Start thread completed, but start not completed and not failed"));
    } while (1);
    ic_mutex_unlock(run_obj->state.protect_state);
    if (sleep_flag)
    {
      DEBUG_PRINT(CONFIG_LEVEL,
        ("We sleep for 5 seconds before retrying start to avoid overload"));
      ic_sleep(5);
      sleep_flag= FALSE;
    }
    first_loop= FALSE;
  } while (1);
  DEBUG_RETURN_INT(0);
}

/* Used by Run Cluster Server module to read configuration from disk. */
static int
read_disk_config(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  int ret_code;
  IC_INT_API_CONFIG_SERVER *apic;
  DEBUG_ENTRY("read_disk_config");

  /*
    The apic object is preallocated before calling this method,
    so don't forget it. Clear remainder of new_config object.
  */
  apic= run_obj->new_config.apic;
  ic_zero(&run_obj->new_config, sizeof(run_obj->new_config));
  run_obj->new_config.apic= apic;

  if ((ret_code= load_local_config(run_obj)))
    goto error;
  if ((ret_code= set_up_apic(run_obj)))
    goto error;
  set_up_run_obj(run_obj);
  DEBUG_RETURN_INT(0);
error:
  DEBUG_RETURN_INT(ret_code);
}

/*
  This method is used by the Run Cluster Server module to reread disk
  configuration after a successful update of the configuration and in
  start cases where the original cluster configuration proved to be
  out-of-date or even that our latest version were never committed.
*/
static int
reread_disk_config(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  IC_INT_API_CONFIG_SERVER *new_apic;
  IC_MEMORY_CONTAINER *old_mc_ptr, *new_mc_ptr;
  int ret_code;
  DEBUG_ENTRY("reread_disk_config");

  old_mc_ptr= run_obj->conf_mc_ptr;

  if (!(new_mc_ptr= ic_create_memory_container(MC_DEFAULT_BASE_SIZE,
                                               0, TRUE)))
    goto error;
  if (!(new_apic=
        (IC_INT_API_CONFIG_SERVER*)new_mc_ptr->mc_ops.ic_mc_calloc(
                new_mc_ptr, sizeof(IC_INT_API_CONFIG_SERVER))))
    goto error;

  /*
    This method is either called as part of Cluster Server start-up in
    which case we don't need to lock the configuration since we still
    have no readers of the configuration other than the start-up threads.
    If we are calling the method as part of a configuration update, then
    this method has been preceded by a proper lock of the configuration.

    Thus there is no need to wait for approval to change the configuration.
  */
  ic_mutex_lock(run_obj->config_mutex);
  run_obj->config_change_ongoing= TRUE;
  run_obj->conf_mc_ptr= new_mc_ptr;
  run_obj->old_conf_mc_ptr= old_mc_ptr;
  run_obj->new_config.apic= new_apic;
  ic_mutex_unlock(run_obj->config_mutex);

  if ((ret_code= read_disk_config(run_obj)))
  {
    ic_print_error(ret_code);
    /* Rollback change */
    goto error;
  }
  /*
    We have successfully loaded a new configuration from disk. We can
    now destroy the old configuration, this configuration has a
    reference count to keep track of users of it. This is to ensure
    that we don't destroy it until no more threads are using it.

    TODO: We need to have some special treatment of changes to the
    apic object since it could affect the operations of the start-up
    code by changing the ports, hosts and number of Cluster Servers to
    connect to.
  */
  check_ready_to_release_config(run_obj, FALSE);
  DEBUG_RETURN_INT(0);
error:
  ic_mutex_lock(run_obj->config_mutex);
  run_obj->config_change_ongoing= FALSE;
  run_obj->conf_mc_ptr= old_mc_ptr;
  run_obj->old_conf_mc_ptr= NULL;
  run_obj->locked_configuration= FALSE;
  ic_cond_broadcast(run_obj->config_cond);
  ic_mutex_unlock(run_obj->config_mutex);
  if (new_mc_ptr)
  {
    new_mc_ptr->mc_ops.ic_mc_free(new_mc_ptr);
  }
  DEBUG_RETURN_INT(IC_ERROR_MEM_ALLOC);
}

static void
install_new_config(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  DEBUG_ENTRY("install_new_config");
  /* Install new configuration directly since no config was there before */
  memcpy(&run_obj->config,
         &run_obj->new_config,
         sizeof(IC_RC_CONFIG_STATE));
  ic_zero(&run_obj->new_config, sizeof(run_obj->new_config));
  DEBUG_RETURN_EMPTY;
}

/* Implements the start_cluster_server method */
static int
start_cluster_server(IC_RUN_CLUSTER_SERVER *ext_run_obj)
{
  IC_INT_RUN_CLUSTER_SERVER *run_obj= (IC_INT_RUN_CLUSTER_SERVER*)ext_run_obj;
  int ret_code;
  IC_INFO_CLUSTER_SERVER *info_cs= get_my_cluster_info(run_obj);
  DEBUG_ENTRY("start_cluster_server");

  /* Try to lock the configuration and get configuration version */
  DEBUG_DISABLE(CONFIG_READ_LEVEL);
  if ((ret_code= ic_load_config_version(run_obj->config_dir,
                                        run_obj->process_name,
                                        &info_cs->config_version_number,
                                        &info_cs->state,
                                        &info_cs->pid)))
  {
    DEBUG_ENABLE(CONFIG_READ_LEVEL);
    goto error;
  }

  /* Read configuration from disk configuration */
  ret_code= read_disk_config(run_obj);
  DEBUG_ENABLE(CONFIG_READ_LEVEL);
  if (ret_code)
    goto error;

  set_up_run_obj_master(run_obj);
  install_new_config(run_obj);
  /*
    Completed loading local configuration from disk.
    ------------------------------------------------
    We have completed loading up our configuration from the local disk
    configuration. It is now time to synchronize our view of the world
    with the view of the other Cluster Servers.

    Start synchronization with other Cluster Servers in the iClaustron Grid.
    ------------------------------------------------------------------------
    Before we start-up our server connection to listen to incoming events
    we first create some socket connections to connect to our fellow
    Cluster Servers. In the start-up phase this is necessary to handle
    synchronisation which Cluster Server becomes the master and who is to
    deliver the current configuration state to the other Cluster Servers.

    After the start-up phase these connections are maintained in an open
    state to ensure we can communicate to the other Cluster Servers at all
    times for changes of the configuration. If we don't manage to set-up
    connections to a certain Cluster Server in the start-up phase we'll
    close this client connection and wait for it to connect to our server
    connection.

    We'll connect in separate threads, this is to ensure that we can also
    accept connections from other starting Cluster Server in the case where
    there are more than one Cluster Server that starts up in parallel.

    Thus we will have one thread per other cluster server in the startup
    phase plus this thread that is ready to start threads when cluster
    servers connect to it.

    If another node connects in this phase it will get an error message that
    the Cluster Server is still starting up.
  */

  if ((ret_code= start_connect_other_cluster_servers(run_obj)))
    goto error;

  DEBUG_RETURN_INT(0);

error:
  unlock_cv_file(run_obj);
  DEBUG_RETURN_INT(ret_code);
}

/*
  Protocol between Cluster Servers to start up.

  The first step is to connect to all Cluster Servers. We will wait until
  at least a quorum of Cluster Servers have connected before proceeding
  with the start of the Cluster Server.

  A quorum is by definition a majority of the nodes, thus with one and two
  nodes it is all nodes, with 3 and 4 nodes it is all nodes except one node.
  The recommended number of Cluster Servers is threee and in this case thus
  two nodes are required at least to start the Cluster Servers.

  When a quorum of cluster servers have connected we will start the
  Cluster Server start protocol after waiting for a short time (3 seconds).
  Whoever of the two nodes which became client part in the setup will start
  by sending (handled in handle_start_protocol method):

  start cluster server<CR>
  nodeid: node_number<CR>
  version: version_number<CR>
  status: status_code<CR>
  pid: pid_number<CR>
  <CR>

  In response to this message the receiving node sends:

  start cluster server reply<CR>
  nodeid: node_number<CR>
  version: version_number<CR>
  pid: pid_number<CR>
  master index size: size_number<CR>
  nodeid: node_number_of_master<CR>
  nodeid: first_slave_node_number<CR>
  nodeid: second_slave_node_number<CR>
  nodeid: third_slave_node_number<CR>
  <CR>

  If the node accepts the list of master information it sends:
  
  start cluster server ok<CR>
  <CR>

  Otherwise it sends the list which it think is the proper list of master
  nodes. They can differ if the node which became the server part was
  starting whereas the other node was already started and a running
  Cluster Server. This message is the same "start cluster server reply"
  message as above with a new nodeid list. In response to the the same
  "start cluster server ok" message is sent.

  Handling one round of heartbeats is part of the start protocol (handled
  by the handle_cs_heartbeat method). It is always the client side that
  initiates sending of heartbeat messages.

  heartbeat<CR>
  started<CR> or starting<CR>
  <CR>

  The server side responds by sending the same protocol message. The
  starting/started indicates the state of the sending node. The client side
  sends a heartbeat messsage every 2 seconds, it waits a maximum of 6 seconds
  before declaring the other side as dead.

  If any node fails the start protocol such that it isn't possible to start
  up the Cluster Server, the start cluster server thread will quit and be
  restarted and make another attempt.

  When a node pair has successfully handled its startup logic it will wait for
  up to 2 seconds before doing another round of heartbeats and wait again.

  When a quorum has successfully started, it's time to check our local state
  to the cluster state. If our state is an old outdated state we need to
  fetch the cluster state from one of the starting nodes. Since this means
  that some other node have to be ready to accept this request for the
  grid configuration it means that other nodes that have an up-to-date state
  must proceed to the running of the cluster although still with limited
  service.

  This means that the run cluster server thread must check its state before
  responding to the protocol messages from other clients in the grid.

  After a successful start, the start cluster server thread will remain and
  continue to send the heartbeat messages.

  Here the start_cluster_server method returns.
*/

/*
  Cluster Server Heartbeat MODULE
  -------------------------------
*/
static int send_heartbeat(IC_CONNECTION *conn,
                          IC_INT_RUN_CLUSTER_SERVER *run_obj);
static int rec_heartbeat(IC_CONNECTION *conn,
                         IC_INT_RUN_CLUSTER_SERVER *run_obj,
                         guint32 cs_index);
static void handle_failed_heartbeat(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                    guint32 cs_index);
static void handle_started_cs_node(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                   guint32 cs_index);

static int
handle_cs_heartbeat(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                    IC_THREAD_STATE *thread_state,
                    guint32 cs_index,
                    gboolean single_loop)
{
  IC_INFO_CLUSTER_SERVER *info_cs= get_cluster_info(run_obj, cs_index);
  IC_CONNECTION *conn= info_cs->conn;
  IC_THREADPOOL_STATE *tp_state= thread_state->ic_get_threadpool(thread_state);
  int ret_code;
  DEBUG_ENTRY("handle_cs_heartbeat");

  conn->conn_op.ic_set_rec_wait_ms(conn, 6000); /* Set timeout to 6 seconds */

  if (info_cs->is_client_side)
  {
    /* We are the client side, means we are responsible initiating heartbeat */
    while (!tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
    {
      if ((ret_code= send_heartbeat(conn, run_obj)) ||
          (ret_code= rec_heartbeat(conn, run_obj, cs_index)))
      {
        handle_failed_heartbeat(run_obj, cs_index);
        DEBUG_RETURN_INT(ret_code);
      }
      if (single_loop)
      {
        DEBUG_RETURN_INT(0);
      }
      ic_sleep(IC_CS_HEARTBEAT_INTERVAL);
    }
  }
  else
  {
    /* Server side waits for heartbeat */
    while (!tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
    {
      if ((ret_code= rec_heartbeat(conn, run_obj, cs_index)) ||
          (ret_code= send_heartbeat(conn, run_obj)))
      {
        handle_failed_heartbeat(run_obj, cs_index);
        DEBUG_RETURN_INT(ret_code);
      }
      if (single_loop)
      {
        DEBUG_RETURN_INT(0);
      }
    }
  }
  DEBUG_RETURN_INT(0);
}

static int
send_heartbeat(IC_CONNECTION *conn,
               IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  int ret_code;
  const gchar *start_state_str= run_obj->state.cs_started ?
                                  ic_started_str :
                                  ic_starting_str;
  DEBUG_DISABLE(HEARTBEAT_LEVEL);
  if ((ret_code= ic_send_with_cr(conn, ic_heartbeat_str)) ||
      (ret_code= ic_send_with_cr(conn, start_state_str)) ||
      (ret_code= ic_send_empty_line(conn)))
  {
    (void)ret_code; /* Empty statement */
  }
  DEBUG_ENABLE(HEARTBEAT_LEVEL);
  return ret_code;
}

static int
rec_heartbeat(IC_CONNECTION *conn,
              IC_INT_RUN_CLUSTER_SERVER *run_obj,
              guint32 cs_index)
{
  int ret_code;
  gboolean found;
  IC_INFO_CLUSTER_SERVER *other_info_cs= get_cluster_info(run_obj, cs_index);
  DEBUG_DISABLE(HEARTBEAT_LEVEL);

  if (!(ret_code= ic_rec_simple_str(conn, ic_heartbeat_str)))
  {
    if (!(ret_code= ic_rec_simple_str_opt(conn, ic_started_str, &found)))
    {
      if (found)
      {
         DEBUG_PRINT(CONFIG_LEVEL, ("Found started string"));
         ic_mutex_lock(run_obj->state.protect_state);
         if (other_info_cs->start_state == IC_CS_NOT_STARTED)
         {
           DEBUG_ENABLE(HEARTBEAT_LEVEL);
           DEBUG_PRINT(CONFIG_LEVEL, ("Found started and need to handle it"));
           handle_started_cs_node(run_obj, cs_index);
           DEBUG_DISABLE(HEARTBEAT_LEVEL);
         }
         ic_mutex_unlock(run_obj->state.protect_state);
      }
      else
      {
         if (!(ret_code= ic_rec_simple_str(conn, ic_starting_str)))
         {
           DEBUG_PRINT(CONFIG_LEVEL, ("Found starting string"));
           ic_mutex_lock(run_obj->state.protect_state);
           if (other_info_cs->start_state == IC_CS_STARTED)
           {
             /*
               State change from started to starting
               This is a breach of the protocol and we treat it as if
               we had a heartbeat failure.
             */
             ic_mutex_unlock(run_obj->state.protect_state);
             goto error;
           }
           ic_mutex_unlock(run_obj->state.protect_state);
         }
      }
    }
  }
  if (ret_code)
    goto error;
  ret_code= ic_rec_empty_line(conn);
error:
  DEBUG_ENABLE(HEARTBEAT_LEVEL);
  return ret_code;
}

static void
handle_failed_heartbeat(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                        guint32 cs_index)
{
  DEBUG_PRINT(CONFIG_LEVEL, ("Failed heartbeat for index=%u", cs_index));
  ic_mutex_lock(run_obj->state.protect_state);
  get_cluster_info(run_obj, cs_index)->start_state= IC_CS_NOT_STARTED;
  ic_mutex_unlock(run_obj->state.protect_state);
}

static void
handle_started_cs_node(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                       guint32 cs_index)
{
  DEBUG_ENTRY("handle_started_cs_node");

  /* State change from starting to started */
  DEBUG_PRINT(CONFIG_LEVEL,
    ("State change from starting to started for index=%u", cs_index));
  get_cluster_info(run_obj, cs_index)->start_state= IC_CS_STARTED;
  DEBUG_RETURN_EMPTY;
}

/*
  Handle Start Protocol MODULE
  ----------------------------

  This module takes care of the actions needed to start the Cluster
  Server by interacting with other Cluster Servers.
*/
static int rec_get_cs_nodeid_req(IC_CONNECTION *conn);
static int send_get_cs_nodeid_reply(IC_CONNECTION *conn, guint32 node_id);
static int send_start_cluster_server_req(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                         IC_CONNECTION *conn);
static int rec_start_cluster_server_req(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                        IC_CONNECTION *conn,
                                        guint32 *cs_index,
                                        guint32 *error_line);
static int send_start_cluster_server_reply(IC_INFO_CLUSTER_SERVER *my_info_cs,
                                           IC_CONNECTION *conn,
                                           guint32 *error_line);
static int rec_start_cluster_server_reply(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                          IC_CONNECTION *conn,
                                          guint32 cs_index,
                                          gboolean is_client);
static int rec_start_cluster_server_ok(IC_CONNECTION *conn);
static int send_start_cluster_server_ok(IC_CONNECTION *conn);
static int verify_master_index_view(IC_INT_RUN_CLUSTER_SERVER *run_obj);
static gboolean is_view_from_starting_node(IC_INFO_CLUSTER_SERVER *info_cs);

static int
handle_start_protocol(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                      IC_CONNECTION *conn,
                      gboolean is_client,
                      guint32 *cs_index)
{
  int ret_code;
  guint32 error_line= 0;
  gboolean found;
  IC_INFO_CLUSTER_SERVER *my_info_cs= get_my_cluster_info(run_obj);
  gchar *err_msg;
  gboolean ok_found;
  gchar buf[COMMAND_READ_BUF_SIZE];
  DEBUG_ENTRY("handle_start_protocol");

  if (is_client)
  {
    /* Client starts protocol by giving its node id */
    if ((ret_code= rec_get_cs_nodeid_req(conn)) ||
        (ret_code= send_get_cs_nodeid_reply(conn, my_info_cs->node_id)) ||
        (ret_code= ic_rec_ok_or_error(conn, buf, &ok_found)))
    {
      error_line= __LINE__;
      goto error;
    }
    if (!ok_found)
    {
      ic_printf("Received error message:");
      ic_printf(buf);
      error_line= __LINE__;
      goto error;
    }
    /* Client continue by sending start cluster server */
    if ((ret_code= send_start_cluster_server_req(run_obj, conn)))
    {
      error_line= __LINE__;
      goto error;
    }
  }
  else
  {
    /* Server waits for start request and sends response */
    if ((ret_code= rec_start_cluster_server_req(run_obj,
                                                conn,
                                                cs_index,
                                               &error_line)) ||
        (ret_code= send_start_cluster_server_reply(my_info_cs,
                                                   conn,
                                                   &error_line)))
    {
      error_line= __LINE__;
      goto error;
    }
    /* Wait for ok or corrected response */
    if ((ret_code= ic_rec_simple_str_opt(conn,
                                         ic_start_cluster_server_ok_str,
                                         &found)))
    {
      error_line= __LINE__;
      goto error;
    }
    if (found)
    {
      if ((ret_code= ic_rec_empty_line(conn)))
      {
        error_line= __LINE__;
        goto error;
      }
      /*
        Copy our master index view to view by other node since he
        accepted our view
      */
      ic_mutex_lock(run_obj->state.protect_state);
      memcpy(&get_cluster_info(run_obj, *cs_index)->master_index_view[0],
             &my_info_cs->master_index_view[0],
             sizeof(guint32)*IC_MAX_CLUSTER_SERVERS);
      ic_mutex_unlock(run_obj->state.protect_state);
      DEBUG_RETURN_INT(0);
    }
    /* Not found means we should expect to receive new reply */
  }

  /* Receive reply from other Cluster Server */
  if ((ret_code= rec_start_cluster_server_reply(run_obj,
                                                conn,
                                                *cs_index,
                                                is_client)))
  {
    if (ret_code != IC_ERROR_CHANGE_VIEW || !is_client)
    {
      error_line= __LINE__;
      goto error;
    }
    /* We had a more up-to-date view on master node order */
    if ((ret_code= send_start_cluster_server_reply(my_info_cs,
                                                   conn,
                                                   &error_line)) ||
        (ret_code= rec_start_cluster_server_ok(conn)))
    {
      error_line= __LINE__;
      goto error;
    }
  }
  else
  {
    /* Send ok */
    if ((ret_code= send_start_cluster_server_ok(conn)))
    {
      error_line= __LINE__;
      goto error;
    }
  }
  DEBUG_RETURN_INT(0);

error:
  err_msg= ic_common_fill_error_buffer(NULL,
                                       error_line,
                                       ret_code,
                                       buf);
  ic_printf("%s", err_msg);
  DEBUG_RETURN_INT(ret_code);
}

/* Receive get cluster server node id from client */
static int
rec_get_cs_nodeid_req(IC_CONNECTION *conn)
{
  int ret_code;
  DEBUG_ENTRY("rec_get_cs_nodeid_req");

  if ((ret_code= ic_rec_simple_str(conn, get_cs_nodeid_str)) ||
      (ret_code= ic_rec_empty_line(conn)))
  {
    DEBUG_RETURN_INT(ret_code);
  }
  DEBUG_RETURN_INT(0);
}

/* Send get cluster server node id reply to client */
static int
send_get_cs_nodeid_reply(IC_CONNECTION *conn, guint32 node_id)
{
  int ret_code;
  DEBUG_ENTRY("send_get_cs_nodeid_reply");

  if ((ret_code= ic_send_with_cr(conn, get_cs_nodeid_reply_str)) ||
      (ret_code= ic_send_with_cr_with_number(conn, nodeid_str, node_id)) ||
      (ret_code= ic_send_empty_line(conn)))
  {
    DEBUG_RETURN_INT(ret_code);
  }
  DEBUG_RETURN_INT(0);
}

/* Send start cluster server-message */
static int
send_start_cluster_server_req(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                              IC_CONNECTION *conn)
{
  int ret_code;
  IC_INFO_CLUSTER_SERVER *info_cs= get_my_cluster_info(run_obj);
  DEBUG_ENTRY("send_start_cluster_server_req");

  if ((ret_code= ic_send_with_cr(conn, ic_start_cluster_server_str)) ||
      (ret_code= ic_send_with_cr_with_number(conn,
                                             nodeid_str,
                                             (guint64)run_obj->cs_nodeid)) ||
      (ret_code= ic_send_with_cr_with_number(conn,
                                             ic_version_str,
                                        info_cs->config_version_number)) ||
      (ret_code= ic_send_with_cr_with_number(conn,
                                             ic_status_str,
                                             (guint64)info_cs->state)) ||
      (ret_code= ic_send_with_cr_with_number(conn,
                                             ic_pid_str,
                                             (guint64)info_cs->pid)) ||
      (ret_code= ic_send_empty_line(conn)))
  {
    DEBUG_RETURN_INT(ret_code);
  }
  DEBUG_RETURN_INT(0);
}

/*
  If the new node isn't already a part of the master index view we need
  to add it to this view. In this case we always add it last in the view
  since the other nodes was already put onto the list properly.
*/
static void
update_master_index_view(IC_INFO_CLUSTER_SERVER *my_info_cs, guint32 node_id)
{
  gboolean found= FALSE;
  guint32 i;
  DEBUG_ENTRY("update_master_index_view");

  for (i= 0; i < my_info_cs->master_index_size; i++)
  {
    if (my_info_cs->master_index_view[i] == node_id)
    {
      found= TRUE;
      DEBUG_RETURN_EMPTY;
    }
  }
  if (!found)
  {
    /*
      Node wasnt' found in master index view, this happens when this is
      called after our node has already started.
    */
    my_info_cs->master_index_view[my_info_cs->master_index_size]= node_id;
    my_info_cs->master_index_size++;
    /*
      TODO: Communicate with the other already alive Cluster Server to ensure
      synchronize adding new Cluster Servers properly.
    */
  }
  DEBUG_RETURN_EMPTY;
}

static int
rec_start_cluster_server_req(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                             IC_CONNECTION *conn,
                             guint32 *cs_index,
                             guint32 *error_line)
{
  guint32 node_id, status;
  guint64 pid, version_num;
  int ret_code;
  guint32 found_index;
  IC_INFO_CLUSTER_SERVER *info_cs;
  DEBUG_ENTRY("rec_start_cluster_server_req");

  if ((ret_code= ic_rec_simple_str(conn, ic_start_cluster_server_str)) ||
      (ret_code= ic_rec_number(conn, nodeid_str, &node_id)) ||
      (ret_code= ic_rec_long_number(conn, ic_version_str, &version_num)) ||
      (ret_code= ic_rec_number(conn, ic_status_str, &status)) ||
      (ret_code= ic_rec_long_number(conn, ic_pid_str, &pid)) ||
      (ret_code= ic_rec_empty_line(conn)))
  {
    *error_line= __LINE__; 
    goto error;
  }
  if ((ret_code= find_cs_index(run_obj, node_id, &found_index)))
  {
    *error_line= __LINE__; 
    goto error;
  }
  if (*cs_index != IC_MAX_CLUSTER_SERVERS &&
      *cs_index != found_index)
  {
    *error_line= __LINE__; 
    DEBUG_RETURN_INT(IC_PROTOCOL_ERROR);
  }
  DEBUG_PRINT(CONFIG_LEVEL, ("Found index = %d", found_index));
  *cs_index= found_index;
  info_cs= get_cluster_info(run_obj, found_index);
  ic_mutex_lock(run_obj->state.protect_state);
  info_cs->pid= (IC_PID_TYPE)pid;
  info_cs->state= (IC_CONF_STATE_TYPE)status;
  info_cs->config_version_number= (IC_CONF_VERSION_TYPE)version_num;
  ic_mutex_unlock(run_obj->state.protect_state);
error:
  DEBUG_RETURN_INT(ret_code);
}

/*
  Code executed on behalf of starting Cluster Servers in already started
  Cluster Servers.
*/
static int
send_start_cluster_server_reply(IC_INFO_CLUSTER_SERVER *my_info_cs,
                                IC_CONNECTION *conn,
                                guint32 *error_line)
{
  int ret_code;
  guint32 i;
  DEBUG_ENTRY("send_start_cluster_server_reply");

  if ((ret_code= ic_send_with_cr(conn, ic_start_cluster_server_reply_str)) ||
      (ret_code= ic_send_with_cr_with_number(conn,
                                             nodeid_str,
                                             my_info_cs->node_id)) ||
      (ret_code= ic_send_with_cr_with_number(conn,
                                             ic_version_str,
                                       my_info_cs->config_version_number)) ||
      (ret_code= ic_send_with_cr_with_number(conn,
                                             ic_status_str,
                                             my_info_cs->state)) ||
      (ret_code= ic_send_with_cr_with_number(conn,
                                             ic_pid_str,
                                             my_info_cs->pid)) ||
      (ret_code= ic_send_with_cr_with_number(conn,
                                             ic_master_index_size_str,
                                             my_info_cs->master_index_size)))
  {
    *error_line= __LINE__;
    goto end;
  }
  for (i= 0; i < my_info_cs->master_index_size; i++)
  {
    if ((ret_code= ic_send_with_cr_with_number(conn,
                                               nodeid_str,
                                         my_info_cs->master_index_view[i])))
    {
      *error_line= __LINE__;
      goto end;
    }
  }
  if ((ret_code= ic_send_empty_line(conn)))
  {
    *error_line= __LINE__;
    goto end;
  }
end:
  DEBUG_RETURN_INT(ret_code);
}

/* Receive start cluster server reply-message */
static int
rec_start_cluster_server_reply(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                               IC_CONNECTION *conn,
                               guint32 cs_index,
                               gboolean is_client)
{
  int ret_code;
  guint32 i;
  guint64 version_num, pid;
  guint32 node_id, status, index_size;
  IC_INFO_CLUSTER_SERVER *info_cs;
  guint32 master_node_index[IC_MAX_CLUSTER_SERVERS];
  DEBUG_ENTRY("rec_start_cluster_server_reply");

  while (1)
  {
     if ((ret_code= ic_rec_simple_str(conn,
                                      ic_start_cluster_server_reply_str)))
     {
       /*
          At this point the server can be waiting for other nodes to
          join before continuing with the actual start protocol.
       */
       if (ret_code == IC_ERROR_RECEIVE_TIMEOUT && is_client)
         continue;
       goto no_lock_error;
     }
     break;
  }
  if ((ret_code= ic_rec_number(conn, nodeid_str, &node_id)) ||
      (ret_code= ic_rec_long_number(conn, ic_version_str, &version_num)) ||
      (ret_code= ic_rec_number(conn, ic_status_str, &status)) ||
      (ret_code= ic_rec_long_number(conn, ic_pid_str, &pid)) ||
      (ret_code= ic_rec_number(conn, ic_master_index_size_str, &index_size)))
    goto no_lock_error;
  if (index_size > IC_MAX_CLUSTER_SERVERS)
  {
    DEBUG_RETURN_INT(IC_PROTOCOL_ERROR);
  }
  for (i= 0; i < index_size; i++)
  {
    if ((ret_code= ic_rec_number(conn, nodeid_str, &master_node_index[i])))
      goto no_lock_error;
  }
  if ((ret_code= ic_rec_empty_line(conn)))
    goto no_lock_error;
  /*
    We have received a correct protocol message describing a started
    Cluster Server.
  */
  info_cs= get_cluster_info(run_obj, cs_index);
  ic_mutex_lock(run_obj->state.protect_state);
  /* Verify node is using correct node id */
  if (info_cs->node_id != node_id)
  {
    ret_code= IC_ERROR_WRONG_NODE_ID;
    goto error;
  }
  /* Copy master index into this node's view on it */
  info_cs->master_index_size= index_size;
  memcpy(&info_cs->master_index_view[0],
         &master_node_index[0],
         sizeof(guint32) * index_size);

  /* Note we have a connection to the node */
  info_cs->cs_connect_state= TRUE;

  if ((ret_code= verify_master_index_view(run_obj)))
    goto error;

  /* Update status variables */
  info_cs->pid= (IC_PID_TYPE)pid;
  info_cs->state= (IC_CONF_STATE_TYPE)status;
  info_cs->config_version_number= (IC_CONF_VERSION_TYPE)version_num;
error:
  ic_mutex_unlock(run_obj->state.protect_state);
no_lock_error:
  DEBUG_RETURN_INT(ret_code);
}

static int
rec_start_cluster_server_ok(IC_CONNECTION *conn)
{
  int ret_code;
  DEBUG_ENTRY("rec_start_cluster_server_ok");

  if ((ret_code= ic_rec_simple_str(conn, ic_start_cluster_server_ok_str)) ||
      (ret_code= ic_rec_empty_line(conn)))
  {
    DEBUG_RETURN_INT(ret_code);
  }
  DEBUG_RETURN_INT(0);
}

static int
send_start_cluster_server_ok(IC_CONNECTION *conn)
{
  int ret_code;
  DEBUG_ENTRY("send_start_cluster_server_ok");

  if ((ret_code= ic_send_with_cr(conn, ic_start_cluster_server_ok_str)) ||
      (ret_code= ic_send_empty_line(conn)))
  {
    DEBUG_RETURN_INT(ret_code);
  }
  DEBUG_RETURN_INT(0);
}

static int
verify_master_index_view(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  IC_INFO_CLUSTER_SERVER *my_info_cs;
  IC_INFO_CLUSTER_SERVER *curr_info_cs, *change_info_cs, *keep_info_cs;
  guint32 i;
  int ret_code;
  DEBUG_ENTRY("verify_master_index_view");

  my_info_cs= get_my_cluster_info(run_obj);
  for (i= 1; i < IC_MAX_CLUSTER_SERVERS; i++)
  {
    curr_info_cs= get_cluster_info(run_obj, i);
    if (!curr_info_cs->cs_connect_state)
      continue;
    if (my_info_cs->master_index_size !=
        curr_info_cs->master_index_size)
    {
      DEBUG_PRINT(CONFIG_LEVEL, ("master_index_size differs"));
      goto error;
    }
    if (memcmp(&my_info_cs->master_index_view[0],
               &curr_info_cs->master_index_view[0],
               sizeof(guint32) * my_info_cs->master_index_size))
    {
      /*
        Differing views on master index view can happen when one node
        is already a master node and the others aren't aware of this
        yet.
      */
      if (is_view_from_starting_node(my_info_cs))
      {
        if (is_view_from_starting_node(curr_info_cs))
          goto error;
        change_info_cs= my_info_cs;
        keep_info_cs= curr_info_cs;
        if (curr_info_cs->master_index_view[0] ==
            curr_info_cs->node_id)
          goto change_view;
      }
      else
      {
        if (!is_view_from_starting_node(curr_info_cs))
          goto error;
        change_info_cs= curr_info_cs;
        keep_info_cs= my_info_cs;
        if (my_info_cs->master_index_view[0] ==
            my_info_cs->node_id)
          goto change_view;
      }
      goto error;
    }
  }
  ret_code= 0;
  goto end;

change_view:
  memcpy(&change_info_cs->master_index_view[0],
         &keep_info_cs->master_index_view[0],
         sizeof(guint32)*keep_info_cs->master_index_size);
  ret_code= IC_ERROR_CHANGE_VIEW;
  goto end;
error:
  ret_code= IC_ERROR_MASTER_INDEX_VIEW_DIFFERS;
end:
  DEBUG_RETURN_INT(ret_code);
}

static gboolean
is_view_from_starting_node(IC_INFO_CLUSTER_SERVER *info_cs)
{
  guint32 i;
  
  for (i= 1; i < info_cs->master_index_size; i++)
  {
    if (info_cs->master_index_view[i] < info_cs->master_index_view[i-1])
    {
      return FALSE;
    }
  }
  return TRUE;
}

/*
  MODULE: Start threads for Cluster Server
  ----------------------------------------

  When starting a Cluster Server one thread per other Cluster Server is
  created to handle the communication with the correspondent Cluster
  Server.

  The start method of this thread is start_cs_func.
*/

static int get_cs_connection(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                             IC_THREADPOOL_STATE *tp_state,
                             IC_THREAD_STATE *thread_state,
                             guint32 cs_index,
                             IC_CONNECTION **conn);
static int wait_for_enough_connections(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                       IC_THREADPOOL_STATE *tp_state,
                                       IC_THREAD_STATE *thread_state,
                                       guint32 cs_index);
static int wait_for_enough_started_nodes(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                         IC_THREADPOOL_STATE *tp_state,
                                         IC_THREAD_STATE *thread_state,
                                         guint32 cs_index);
static guint32 get_count_active_start_threads(
                     IC_INT_RUN_CLUSTER_SERVER *run_obj);
static guint32 get_count_connected_cluster_servers(
                     IC_INT_RUN_CLUSTER_SERVER *run_obj);
static void release_cs_connection(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                  IC_CONNECTION *conn,
                                  guint32 cs_index,
                                  gboolean lock);
static void set_up_initial_master_index_view(
                     IC_INT_RUN_CLUSTER_SERVER *run_obj);
static IC_CONF_VERSION_TYPE find_max_version(
                     IC_INT_RUN_CLUSTER_SERVER *run_obj);
static IC_CONF_VERSION_TYPE find_max_committed_version(
                     IC_INT_RUN_CLUSTER_SERVER *run_obj);
static int check_if_cs_started(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                               IC_THREAD_STATE *thread_state,
                               guint32 cs_index);
static gpointer check_if_cs_started_thread_func(gpointer data);

static gpointer
start_cs_func(gpointer data)
{
  IC_THREAD_STATE *thread_state= (IC_THREAD_STATE*)data;
  IC_THREADPOOL_STATE *tp_state;
  THREAD_DATA *thread_data;
  IC_INT_RUN_CLUSTER_SERVER *run_obj;
  guint32 cs_index;
  IC_RUN_CLUSTER_STATE *rc_state;
  IC_INFO_CLUSTER_SERVER *this_info_cs;
  IC_CONNECTION *conn= NULL;
  int ret_code;
  DEBUG_THREAD_ENTRY("start_cs_func");

  tp_state= thread_state->ic_get_threadpool(thread_state);
  thread_data=
    (THREAD_DATA*)tp_state->ts_ops.ic_thread_get_object(thread_state);
  run_obj= thread_data->run_obj;
  cs_index= thread_data->cs_index;
  rc_state= &run_obj->state;
  this_info_cs= get_cluster_info(run_obj, cs_index);

  tp_state->ts_ops.ic_thread_started(thread_state);
  if (tp_state->ts_ops.ic_thread_startup_done(thread_state))
    goto early_end;

  DEBUG_PRINT(CONFIG_LEVEL,
    ("This thread handles startup connection to node id = %u",
     this_info_cs->node_id));
  if (get_cs_connection(run_obj,
                        tp_state,
                        thread_state,
                        cs_index,
                        &conn))
    goto early_end;

  if (wait_for_enough_connections(run_obj,
                                  tp_state,
                                  thread_state,
                                  cs_index))
    goto end;
                                  
  /*
    We are now connected to the Cluster Server this thread is appointed to
    handle. Also enough nodes have connected to form at least a quorum of
    Cluster Servers. We need to give our start up data and receive information
    about the other node's state. If any node fails as part of the start
    protocol we will disconnect all connections and try again.
  */
  if ((ret_code= handle_start_protocol(run_obj,
                                       conn,
                                       this_info_cs->is_client_side,
                                       &cs_index)))
  {
    /* Release connection, return and ensure we make another retry */
    DEBUG_PRINT(CONFIG_LEVEL, ("This thread failed with start protocol"));
    goto end;
  }

  /* Handling one round of heartbeat is part of start protocol */
  if (handle_cs_heartbeat(run_obj, thread_state, cs_index, TRUE))
    goto end;

  if (tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
    goto end;

  if (wait_for_enough_started_nodes(run_obj,
                                    tp_state,
                                    thread_state,
                                    cs_index))
    goto end;

  /*
    We are now connected to the other Cluster Server and we have
    successfully exchanged start-up data. We keep this thread
    until the connection breaks or until we're about to quit.
  */
  handle_cs_heartbeat(run_obj, thread_state, cs_index, FALSE);
end:
  release_cs_connection(run_obj, conn, cs_index, FALSE);
end_thread:
  tp_state->ts_ops.ic_thread_stops(thread_state);
  DEBUG_THREAD_RETURN;

early_end:
  ic_mutex_lock(rc_state->protect_state);
  rc_state->start_threads_stopped++;
  get_cluster_info(run_obj, cs_index)->is_start_thread_active= FALSE;
  ic_mutex_unlock(rc_state->protect_state);
  goto end_thread;
}

typedef struct client_conn_timer_struct CLIENT_CONN_TIMER_STRUCT;
struct client_conn_timer_struct
{
  IC_THREADPOOL_STATE *tp_state;
  IC_THREAD_STATE *thread_state;
  IC_RUN_CLUSTER_STATE *rc_state;
  guint32 cs_index;
};

static int
check_quorum_reached(IC_THREADPOOL_STATE *tp_state,
                     IC_THREAD_STATE *thread_state,
                     IC_RUN_CLUSTER_STATE *rc_state,
                     guint32 cs_index)
{
  DEBUG_ENTRY("check_quorum_reached");

  (void)cs_index;
  if (tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
    goto error;
  ic_mutex_lock(rc_state->protect_state);
  if (rc_state->cs_starting)
  {
    /*
      A quorum have already been reached without us, no use of continuing
      to connect to the node. When the node starts up we will have a port
      open to connect to.
    */
    ic_cond_broadcast(rc_state->connect_cond);
    DEBUG_PRINT(CONFIG_LEVEL,
      ("Close thread for index %u, others already starting", cs_index));
    ic_mutex_unlock(rc_state->protect_state);
    goto error;
  }
  ic_mutex_unlock(rc_state->protect_state);
  DEBUG_RETURN_INT(0);
error:
  DEBUG_RETURN_INT(1);
}

static int
client_conn_timeout_func(void *timeout_obj, int timer)
{
  CLIENT_CONN_TIMER_STRUCT *client_conn_timer_ptr=
    (CLIENT_CONN_TIMER_STRUCT*)timeout_obj;
  (void)timer;
  return check_quorum_reached(client_conn_timer_ptr->tp_state,
                              client_conn_timer_ptr->thread_state,
                              client_conn_timer_ptr->rc_state,
                              client_conn_timer_ptr->cs_index);
}

static int
get_cs_connection(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                  IC_THREADPOOL_STATE *tp_state,
                  IC_THREAD_STATE *thread_state,
                  guint32 cs_index,
                  IC_CONNECTION **conn)
{
  gchar *err_str;
  IC_INFO_CLUSTER_SERVER *this_info_cs, *my_info_cs;
  IC_RUN_CLUSTER_STATE *rc_state;
  gboolean is_client;
  IC_INT_API_CONFIG_SERVER *apic;
  int ret_code;
  CLIENT_CONN_TIMER_STRUCT client_conn_timer_area;
  DEBUG_ENTRY("get_cs_connection");

  apic= run_obj->config.apic;
  my_info_cs= get_my_cluster_info(run_obj);
  this_info_cs= get_cluster_info(run_obj, cs_index);
  rc_state= &run_obj->state;
  is_client= (this_info_cs->node_id > my_info_cs->node_id);

  client_conn_timer_area.tp_state= tp_state;
  client_conn_timer_area.thread_state= thread_state;
  client_conn_timer_area.rc_state= rc_state;
  client_conn_timer_area.cs_index= cs_index;

  while (1)
  {
    err_str= NULL;
    /*
      We use the node id to derive whether to use a server or a client.
      But the first time we always try with a client and we also do this
      regularly in case there is already a number of Cluster Servers
      started which only listens to a client connections.
    */
    if (tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
      goto error;
    if (is_client)
    {
      if (!(ret_code= connect_cluster_server(apic,
                                             conn,
                                             cs_index,
                                             TRUE,
                                             &err_str,
                                             client_conn_timeout_func,
                                             (void*)&client_conn_timer_area)))
      {
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Connect to Cluster Server index %u, client side", cs_index));
        ic_mutex_lock(rc_state->protect_state);
        this_info_cs->is_client_side= TRUE;
        this_info_cs->conn= *conn;
        ic_mutex_unlock(rc_state->protect_state);
        ic_cond_broadcast(rc_state->connect_cond);
        break;
      }
      DEBUG_PRINT(CONFIG_LEVEL,
       ("Failed client side connect to Cluster Server, index %u", cs_index));
      goto error;
    }
    else
    {
      ic_mutex_lock(rc_state->protect_state);
      /*
        Check if we're already connected through the service of the connect
        server thread, otherwise wait for the connect server thread to
        complete its task.
      */
      if (!this_info_cs->conn)
      {
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Wait for connect_server_thread to signal us"));
        this_info_cs->wait_connect_server_thread= TRUE;
        ic_cond_signal(rc_state->wait_server_connect_cond);
        ic_cond_timed_wait(rc_state->connect_cond, rc_state->protect_state,
                           IC_STOP_CHECK_TIMER * IC_MICROSEC_PER_SECOND);
      }
      if (this_info_cs->conn)
      {
        /* The connect server thread connected for us */
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Connect to Cluster Server index %u, server side", cs_index));
        ic_mutex_unlock(rc_state->protect_state);
        *conn= this_info_cs->conn;
        break;
      }
      ic_mutex_unlock(rc_state->protect_state);
    }
    if (check_quorum_reached(tp_state, thread_state, rc_state, cs_index))
      goto error;
  }
  DEBUG_RETURN_INT(0);
error:
  DEBUG_RETURN_INT(1);
}

static int
wait_for_enough_connections(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                            IC_THREADPOOL_STATE *tp_state,
                            IC_THREAD_STATE *thread_state,
                            guint32 cs_index)
{
  IC_INFO_CLUSTER_SERVER *this_info_cs, *my_info_cs;
  IC_RUN_CLUSTER_STATE *rc_state;
  gboolean set_have_waited, have_waited;
  guint32 connected_servers;
  guint32 num_servers;
  guint32 active_start_threads;
  DEBUG_ENTRY("wait_for_enough_connections");

  my_info_cs= get_my_cluster_info(run_obj);
  this_info_cs= get_cluster_info(run_obj, cs_index);
  rc_state= &run_obj->state;

  /* Inform the other cluster server of our intentions */
  have_waited= FALSE;
  ic_mutex_lock(rc_state->protect_state);
  do
  {
    set_have_waited= FALSE;
    if (tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
    {
      DEBUG_PRINT(CONFIG_LEVEL, ("Close thread for index %u, stop flag set",
                  cs_index));
      goto error;
    }
    connected_servers= get_count_connected_cluster_servers(run_obj);
    num_servers= run_obj->config.num_cluster_servers;
    if ((connected_servers + 1) == num_servers)
    {
      /*
        All cluster servers have connected, we will proceed after waking
        all threads which have successfully connected to a Cluster Server.
      */
      ic_cond_broadcast(rc_state->connect_cond);
      break;
    }
    else
    {
      /*
        We don't have a sufficient number of connections to start-up
        immediately. We will check if there is a sufficient number of
        threads still attempting the start, if not we will quit and
        make another attempt later.
        If there is a chance to start with a quorum we will wait for
        a few seconds, after this wait we will check if we've reached
        a quorum, if a quorum has been established we will start
        although not all Cluster servers have connected.
        If all active threads are already connected and they form
        a quorum we can start immediately.
      */
      active_start_threads= get_count_active_start_threads(run_obj);
      if ((active_start_threads + 1) < num_servers ||
          ((active_start_threads == 1) && (num_servers == 2)))
      {
        /* Too few threads to continue, this thread needs to stop */
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Too few threads to start, index %u, stop thread and restart",
           cs_index));
        goto error;
      }
      else if ((active_start_threads + 1 == num_servers) &&
               (num_servers > 2))
      {
        if (have_waited)
        {
          /* We're a quorum and we already waited, proceed */
          ic_cond_broadcast(rc_state->connect_cond);
          break;
        }
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Quorum reached, wait 3 secs for other Cluster Servers index %u",
           cs_index));
        ic_cond_timed_wait(rc_state->connect_cond, rc_state->protect_state,
                     IC_WAIT_OTHER_CS_START_TIMER * IC_MICROSEC_PER_SECOND);
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Wakeup from waiting after quorum reached"));
        have_waited= TRUE;
        set_have_waited= TRUE;
      }
      else if (connected_servers == active_start_threads)
      {
        /*
          We are a quorum and the missing connection thread has already
          finished, thus we are ready to continue
        */
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Quorum reached, index %u other already stopped", cs_index));
        ic_cond_broadcast(rc_state->connect_cond);
        break;
      }
      else
      {
        /*
           We will wait here for someone else to complete the connection
           set-up, whereafter we can continue with the Cluster Server start
           protocol. We'll use a timed wait to ensure we'll notice a
           stop or some other event needing attention.
        */
        DEBUG_PRINT(CONFIG_LEVEL,
          ("No quorum reached, index %u, wait for more nodes", cs_index));
        ic_cond_timed_wait(rc_state->connect_cond,
                           rc_state->protect_state,
                     IC_WAIT_OTHER_CS_START_TIMER * IC_MICROSEC_PER_SECOND);
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Wakeup after waiting for quorum to be reached, we're connected"));
      }
    }
    if (!set_have_waited)
      have_waited= FALSE;
  } while (1);
  if (tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
  {
    DEBUG_PRINT(CONFIG_LEVEL, ("Close thread for index %u, stop flag set",
                cs_index));
    goto error;
  }
  if (rc_state->cs_started)
  {
    ic_mutex_lock(rc_state->protect_state);
    update_master_index_view(my_info_cs,
                             this_info_cs->node_id);
    ic_mutex_unlock(rc_state->protect_state);
  }
  else
  {
    DEBUG_PRINT(CONFIG_LEVEL,
      ("A quorum has been reached, set cs_starting to TRUE"));
    rc_state->cs_starting= TRUE;
    rc_state->threads_ready_connected++;
    set_up_initial_master_index_view(run_obj);
    if (my_info_cs->master_index_size > 2)
    {
      /*
        At least 2 threads will pass this point, we need to ensure that they
        wait for each other before proceeding. Otherwise the first to start
        might decide that the start failed before the next thread has arrived
        here.
      */
      if ((rc_state->threads_ready_connected + 1) ==
          my_info_cs->master_index_size)
      {
        /*
          The last thread arrived here, set threads_ready back to 0 and
          wake all threads so that start handling can proceed.
        */
        rc_state->threads_ready_connected= 0;
        ic_cond_broadcast(rc_state->connect_cond);
        DEBUG_PRINT(CONFIG_LEVEL, ("Wake other threads waiting"));
      }
      else
      {
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Wait for other threads to reach this point"));
        do
        {
          ic_cond_timed_wait(rc_state->connect_cond,
                             rc_state->protect_state,
                             IC_STOP_CHECK_TIMER * IC_MICROSEC_PER_SECOND);
          if (tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
          {
            ic_mutex_unlock(rc_state->protect_state);
            goto error;
          }
        } while (rc_state->threads_ready_connected > 0);
        DEBUG_PRINT(CONFIG_LEVEL, ("Cleared to proceed"));
      }
    }
    ic_mutex_unlock(rc_state->protect_state);
  }
  DEBUG_RETURN_INT(0);
error:
  DEBUG_RETURN_INT(1);
}

static int
wait_for_enough_started_nodes(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                              IC_THREADPOOL_STATE *tp_state,
                              IC_THREAD_STATE *thread_state,
                              guint32 cs_index)
{
  IC_RUN_CLUSTER_STATE *rc_state;
  DEBUG_ENTRY("wait_for_enough_started_nodes");

  rc_state= &run_obj->state;
  ic_mutex_lock(rc_state->protect_state);
  if (rc_state->cs_started)
  {
    /*
      When the Cluster is already in a started state then we obviously
      have enough started nodes.
    */
    ic_mutex_unlock(rc_state->protect_state);
    DEBUG_RETURN_INT(0);
  }
  do
  {
    if (!rc_state->cs_starting)
    {
      /*
        Some other connection failed with start protocol, return and
        ensure we make another attempt at starting up.
      */
      DEBUG_PRINT(CONFIG_LEVEL, ("Other thread failed with start protocol"));
      ic_mutex_unlock(rc_state->protect_state);
      goto error;
    }
    if ((get_count_connected_cluster_servers(run_obj) +
         rc_state->start_threads_stopped + 1) ==
        run_obj->config.num_cluster_servers)
    {
      /*
        Startup is completed, we can now set state to started and
        start working as a Cluster Server to others in the Cluster.
      */
      ic_assert(rc_state->start_threads_stopped < 2);
      ic_assert(!(run_obj->config.num_cluster_servers == 2 &&
                  rc_state->start_threads_stopped > 0));
      ic_mutex_unlock(rc_state->protect_state);
      if (check_if_cs_started(run_obj, thread_state, cs_index))
        goto error;
      ic_mutex_lock(rc_state->protect_state);
      ic_cond_broadcast(rc_state->start_cond);
      DEBUG_PRINT(CONFIG_LEVEL, ("Completed startup of Cluster Server"));
      break;
    }
    else
    {
      /* Wait for all other Cluster Servers to complete */
      DEBUG_PRINT(CONFIG_LEVEL,
        ("Wait for other threads to complete startup"));
      ic_cond_timed_wait(rc_state->start_cond, rc_state->protect_state,
                         IC_CS_HEARTBEAT_INTERVAL * IC_MICROSEC_PER_SECOND);
      ic_mutex_unlock(rc_state->protect_state);
      if (tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
        goto error;
      if (handle_cs_heartbeat(run_obj, thread_state, cs_index, TRUE))
        goto error;
      ic_mutex_lock(rc_state->protect_state);
    }
  } while (1);
  ic_mutex_unlock(rc_state->protect_state);
  DEBUG_RETURN_INT(0);
error:
  DEBUG_RETURN_INT(1);
}

static guint32
get_count_active_start_threads(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 i;
  guint32 num_started= 0;

  for (i= 0; i < run_obj->config.num_cluster_servers; i++)
  {
    if (get_cluster_info(run_obj, i)->is_start_thread_active)
    {
      num_started++;
    }
  }
  return num_started;
}

static guint32
get_count_connected_cluster_servers(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 i;
  guint32 num_connected= 0;

  for (i= 0; i < run_obj->config.num_cluster_servers; i++)
  {
    if (get_cluster_info(run_obj, i)->conn)
    {
      num_connected++;
    }
  }
  return num_connected;
}

static void
release_cs_connection(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                      IC_CONNECTION *conn,
                      guint32 cs_index,
                      gboolean lock)
{
  IC_INFO_CLUSTER_SERVER *info_cs= get_cluster_info(run_obj, cs_index);
  DEBUG_DECL(char buf[64];)
  DEBUG_ENTRY("release_cs_connection");
  DEBUG_DECL(ic_guint64_hex_str((guint64)conn, buf);)
  DEBUG_PRINT(CONFIG_LEVEL, ("conn = %s", buf));

  if (!lock)
  {
    ic_mutex_lock(run_obj->state.protect_state);
  }

  info_cs->is_start_thread_active= FALSE;
  info_cs->conn= NULL;
  run_obj->state.start_threads_stopped++;
  /*
    With only 2 Cluster Servers we know that losing one is sufficient
    to not be able to proceed with the starting. If we have more than
    2 (3 or 4 that is) then we can survive losing one of them, so
    e.g. if we have 3 Cluster Servers, 2 connections is fully connected,
    and 1 connection means there are 2 Cluster Servers that are connected
    which is sufficient, thus number of connections + 2 must not be
    smaller than the number of Cluster Servers.
  */
  if (run_obj->state.cs_starting &&
      (run_obj->config.num_cluster_servers == 2 ||
       get_count_connected_cluster_servers(run_obj) + 2 <
       run_obj->config.num_cluster_servers))
  {
    run_obj->state.cs_starting= FALSE;
  }

  ic_cond_broadcast(run_obj->state.start_cond);
  ic_mutex_unlock(run_obj->state.protect_state);
  if (conn)
  {
    conn->conn_op.ic_free_connection(conn);
  }
  DEBUG_RETURN_EMPTY;
}

static void
set_up_initial_master_index_view(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 min_node_id= 0;
  guint32 lowest_node_id;
  guint32 index_size= 0;
  guint32 i, j;
  IC_INFO_CLUSTER_SERVER *my_info_cs= get_my_cluster_info(run_obj);
  IC_INFO_CLUSTER_SERVER *info_cs;
  DEBUG_ENTRY("set_up_initial_master_index_view");

  /*
    Initial view is to sort in node id order with lowest node id at the
    beginning of the list.
  */
  for (i= 0; i < run_obj->config.num_cluster_servers; i++)
  {
    lowest_node_id= IC_MAX_NODE_ID;
    for (j= 0; j < run_obj->config.num_cluster_servers; j++)
    {
      /*
        Get the lowest node number not yet put into the master index view.
        Don't forget to account for own nodeid which we don't have a
        connection to.
      */
       info_cs= get_cluster_info(run_obj, j);
       if (info_cs->node_id > min_node_id &&
           info_cs->node_id < lowest_node_id &&
           (info_cs->conn || info_cs == my_info_cs))
       {
         lowest_node_id= run_obj->state.cs_servers[j].node_id;
       }
    }

    if (lowest_node_id == IC_MAX_NODE_ID)
      break;
    index_size++;

    DEBUG_PRINT(CONFIG_LEVEL, ("Master index view id = %d, nodeid = %d",
                               (int)i, (int)lowest_node_id));
    my_info_cs->master_index_view[i]= lowest_node_id;
    min_node_id= lowest_node_id;
  }
  /* No more node founds, at least two are needed here */
  ic_require(index_size > 1);
  my_info_cs->master_index_size= index_size;
  DEBUG_RETURN_EMPTY;
}

static IC_CONF_VERSION_TYPE
find_max_version(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 i;
  IC_CONF_VERSION_TYPE version_number= (IC_CONF_VERSION_TYPE)0;

  for (i= 0; i < run_obj->config.num_cluster_servers; i++)
  {
    if (run_obj->state.cs_servers[i].config_version_number > version_number)
    {
      version_number= run_obj->state.cs_servers[i].config_version_number;
    }
  }
  ic_require(version_number > 0);
  return version_number;
}

static IC_CONF_VERSION_TYPE
find_max_committed_version(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 i;
  IC_CONF_VERSION_TYPE version_number= (IC_CONF_VERSION_TYPE)0;

  for (i= 0; i < run_obj->config.num_cluster_servers; i++)
  {
    if (run_obj->state.cs_servers[i].state == CONFIG_STATE_BUSY &&
        run_obj->state.cs_servers[i].config_version_number > version_number)
    {
      version_number= run_obj->state.cs_servers[i].config_version_number;
    }
  }
  if (version_number == (IC_CONF_VERSION_TYPE)0)
  {
    /* All servers are in PREPARE state, max committed is -1 of max version */
    return (IC_CONF_VERSION_TYPE)(find_max_version(run_obj) - 1);
  }
  return version_number;
}

static gpointer
check_if_cs_started_thread_func(gpointer data)
{
  IC_THREAD_STATE *thread_state= (IC_THREAD_STATE*)data;
  IC_THREADPOOL_STATE *tp_state= thread_state->ic_get_threadpool(thread_state);
  IC_INT_RUN_CLUSTER_SERVER *run_obj= (IC_INT_RUN_CLUSTER_SERVER*)
    tp_state->ts_ops.ic_thread_get_object(thread_state);
  IC_INFO_CLUSTER_SERVER *my_info_cs;
  IC_CONF_VERSION_TYPE config_version_number;
  IC_CONF_STATE_TYPE status;
  IC_CONF_VERSION_TYPE max_committed_version;
  guint32 i;
  int ret_code= 1;
  gchar err_str[ERROR_MESSAGE_SIZE];
  gchar *err_ptr= &err_str[0];
  IC_CONF_VERSION_TYPE old_version;
  IC_API_CONFIG_SERVER *new_apic;
  IC_API_CLUSTER_CONNECTION api_cluster_conn;
  IC_RUN_CLUSTER_STATE *rc_state= &run_obj->state;
  DEBUG_THREAD_ENTRY("check_if_cs_started_thread_func");

  /* Wait for start order */
  tp_state->ts_ops.ic_thread_started(thread_state);
  if (tp_state->ts_ops.ic_thread_startup_done(thread_state))
    goto error;

beginning:

  ic_mutex_lock(rc_state->protect_state);
  my_info_cs= get_my_cluster_info(run_obj);
  config_version_number= my_info_cs->config_version_number;
  status= my_info_cs->state;

  for (i= 1; i < run_obj->config.num_cluster_servers; i++)
  {
    if (run_obj->state.cs_servers[i].config_version_number !=
        config_version_number ||
        run_obj->state.cs_servers[i].state != status)
    {
      max_committed_version= find_max_committed_version(run_obj);
      /* We haven't started yet, we still need to synchronize configs */
      if (my_info_cs->config_version_number > max_committed_version)
      {
        ic_mutex_unlock(rc_state->protect_state);
        /*
          Our most recent version is a version in a prepared state which
          which will not be committed, we need to remove this version
          and set our state to committed of the previous version number.
          After this our Clusteonr Server is in synch with the committed
          version of the Cluster Servers. We can start our Cluster Server
          and accept reads of the configuration, but we cannot accept writes
          until all Cluster Servers are up-to-date.
        */
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Our version was never committed, go back one version"));
        (void)remove_config_files(run_obj->config_dir,
                                  run_obj->config.clu_infos,
                                  my_info_cs->config_version_number);
        if ((ret_code= write_config_version_file(run_obj->config_dir,
                                                 max_committed_version,
                                                 CONFIG_STATE_BUSY,
                                                 my_info_cs->pid)))
        {
          ic_print_error(ret_code);
          ic_printf("Failed to write config version file at line %d",
                    __LINE__);
          goto error;
        }
        ic_mutex_lock(rc_state->protect_state);
        my_info_cs->state= CONFIG_STATE_BUSY;
        my_info_cs->config_version_number= max_committed_version;
        ic_mutex_unlock(rc_state->protect_state);
        /*
          At this point we have the wrong configuration version loaded,
          we need to get rid of the in-memory version and load the correct
          version instead.
        */
        if ((ret_code= reread_disk_config(run_obj)))
          goto error;
      }
      else if (my_info_cs->config_version_number < max_committed_version)
      {
        ic_mutex_unlock(rc_state->protect_state);
        /*
          Our configuration is out-of-date. We don't have access to the
          latest committed configuration version. We need to get this using
          the get configuration protocol and then save this version on file
          before the Cluster Servers are ready to accept updates of the
          configuration. This can happen when our node failed and the
          remaining Cluster Servers formed a quorum that continued
          operating and performing updates.

          After reading the configuration from the network and writing it
          to disk, we reread it from disk to ensure that we allocated the
          configuration from the same code path in all instances of the
          Run Cluster Server module.
        */
        DEBUG_PRINT(CONFIG_LEVEL, ("Our version is out-of-date"));
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Fetch config from up-to-date Cluster Server"));
        if ((new_apic= ic_get_configuration(&api_cluster_conn,
                run_obj->config_dir,
                run_obj->cs_nodeid,
                (guint32)1,
                &run_obj->config.apic->cluster_conn.cluster_server_ips[1],
                &run_obj->config.apic->cluster_conn.cluster_server_ports[1],
                (guint32)3,
                TRUE,
                &ret_code,
                &err_ptr)))
        {
          ic_printf("Failed to get config from network: %s", err_str);
          ic_print_error(ret_code);
          goto error;
        }
        DEBUG_PRINT(CONFIG_LEVEL, ("Successful fetch of new configuration"));

        /* Remove old config files in case we missed several updates */
        DEBUG_PRINT(CONFIG_LEVEL, ("Remove old config files"));
        ic_mutex_lock(rc_state->protect_state);
        if (my_info_cs->state == CONFIG_STATE_PREPARE_UPDATE)
        {
          ic_mutex_unlock(rc_state->protect_state);
          /*
            Make sure all config files are cleaned up,
            also the old committed
          */
          (void)remove_config_files(run_obj->config_dir,
                                    run_obj->config.clu_infos,
                                    my_info_cs->config_version_number - 1);
        }
        ic_mutex_unlock(rc_state->protect_state);
        (void)remove_config_files(run_obj->config_dir,
                                  run_obj->config.clu_infos,
                                  my_info_cs->config_version_number);

        /* Write new configuration */
        DEBUG_PRINT(CONFIG_LEVEL, ("Write configuration to disk"));
        old_version= max_committed_version - 1;
        if ((ret_code= ic_write_full_config_to_disk(run_obj->config_dir,
                      &old_version,
                      run_obj->config.clu_infos,
                      new_apic->api_op.ic_get_all_cluster_config(new_apic))))
          goto error;
        ic_mutex_lock(rc_state->protect_state);
        my_info_cs->config_version_number= max_committed_version;
        my_info_cs->state= CONFIG_STATE_BUSY;
        ic_mutex_unlock(rc_state->protect_state);

        DEBUG_PRINT(CONFIG_LEVEL, ("Reread configuration from disk"));
        if ((ret_code= reread_disk_config(run_obj)))
          goto error;
      }
      else if (my_info_cs->state == CONFIG_STATE_PREPARE_UPDATE)
      {
        ic_mutex_unlock(rc_state->protect_state);
        /*
          We have the right version of the configuration but it hasn't been
          committed yet, since someone else have committed we are safe to
          commit it by updating the config version file.
        */
        DEBUG_PRINT(CONFIG_LEVEL, ("Commit our prepared version"));
        if ((ret_code= write_config_version_file(run_obj->config_dir,
                                      my_info_cs->config_version_number,
                                      CONFIG_STATE_BUSY,
                                      my_info_cs->pid)))
        {
          ic_print_error(ret_code);
          ic_printf("Failed to write config version file at line %d",
                    __LINE__);
          goto error;
        }
        ic_mutex_lock(rc_state->protect_state);
        my_info_cs->state= CONFIG_STATE_BUSY;
        ic_mutex_unlock(rc_state->protect_state);
      }
      else
      {
        ic_mutex_unlock(rc_state->protect_state);
        /*
          Our cluster server is in synch, however someone else is not in
          in synch, we need to proceed with start and be ready to accept
          get config requests from this cluster server. We should be safe
          since the Cluster Server not in synch only can occur when we
          make updates with a quorum. So a quorum (1 of 1, 2 of 2, 2 of 3
          or 3 of 4) must have existed when the update occurred and these
          nodes made a transactional change and thus neither of these
          nodes will be out-of-date. Thus a maximum of one node can be
          out-of-date and the other nodes are ok and will start up without
          needing to get configuration from another node. All other nodes
          should be able to start without assistance and thus when a
          quorum starts, at least one node should be able to answer the
          get config request.
        */
        DEBUG_PRINT(CONFIG_LEVEL, ("Our Cluster Server is in synch"));
        goto end;
      }
      /*
        We have changed our state, it's possible that all Cluster Servers are
        in synch now, so we retry check for this by a recursive call here.
        Since our state will be CONFIG_STATE_BUSY and equal to
        max_committed_version here, there is no risk of more than one
        loop here.
      */
      goto beginning;
    }
  }
end:
  DEBUG_PRINT(CONFIG_LEVEL, ("All Cluster Servers are in synch"));
  ic_mutex_lock(rc_state->protect_state);
  rc_state->cs_started= TRUE;
  rc_state->cs_starting= FALSE;
  rc_state->check_if_cs_started_ret_code= 0;
  ic_cond_signal(rc_state->check_if_cs_started_cond);
  ic_mutex_unlock(rc_state->protect_state);
end_thread:
  tp_state->ts_ops.ic_thread_stops(thread_state);
  DEBUG_THREAD_RETURN;

error:
  ic_mutex_lock(rc_state->protect_state);
  rc_state->check_if_cs_started_ret_code= ret_code;
  ic_cond_signal(rc_state->check_if_cs_started_cond);
  ic_mutex_unlock(rc_state->protect_state);
  goto end_thread;
}

static int
check_if_cs_started(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                    IC_THREAD_STATE *thread_state,
                    guint32 cs_index)
{
  int ret_code;
  guint32 thread_id;
  IC_THREADPOOL_STATE *tp_state= run_obj->tp_state;
  IC_RUN_CLUSTER_STATE *rc_state= &run_obj->state;
  DEBUG_ENTRY("check_if_cs_started");

  DEBUG_PRINT(THREAD_LEVEL,
              ("Starting thread in check_if_cs_started_thread_func"));
  if ((ret_code= tp_state->tp_ops.ic_threadpool_start_thread(
                tp_state,
                &thread_id,
                check_if_cs_started_thread_func,
                (void*)run_obj,
                IC_MEDIUM_STACK_SIZE,
                TRUE)))
  {
    DEBUG_RETURN_INT(ret_code);
  }
  tp_state->tp_ops.ic_threadpool_run_thread(tp_state, thread_id);

  ic_mutex_lock(rc_state->protect_state);
  rc_state->check_if_cs_started_ret_code= (int)-1;
  while (rc_state->check_if_cs_started_ret_code == (int)-1)
  {
    ic_cond_timed_wait(rc_state->check_if_cs_started_cond,
                       rc_state->protect_state,
                       IC_CS_HEARTBEAT_INTERVAL * IC_MICROSEC_PER_SECOND);
    if (rc_state->check_if_cs_started_ret_code != (int)-1)
      break;
    ic_mutex_unlock(rc_state->protect_state);
    ret_code= handle_cs_heartbeat(run_obj, thread_state, cs_index, TRUE);
    ic_mutex_lock(rc_state->protect_state);
    if (ret_code)
    {
      rc_state->check_if_cs_started_ret_code= ret_code;
      break;
    }
  }
  ret_code= rc_state->check_if_cs_started_ret_code;
  ic_mutex_unlock(rc_state->protect_state);
  /* Ensure the thread is completely gone before we proceed */
  tp_state->tp_ops.ic_threadpool_join(tp_state, thread_id);
  DEBUG_RETURN_INT(ret_code);
}
 

/*
  MODULE: Complete Startup Handling as part of Run Cluster Server
  ---------------------------------------------------------------

  This function is called from run_cluster_server to complete the startup
  handling. Some nodes may need to connect to other Cluster Servers to get
  configuration before the node is able to start. State of start is
  communicated through the heartbeat messages started in Cluster Server
  Start handling.
*/
static int
complete_startup_handling(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  int ret_code;
  gchar err_str[ERROR_MESSAGE_SIZE];
  DEBUG_ENTRY("complete_startup_handling");

  /*
    Now we have discovered the proper configuration to use in the
    Cluster Server and we can create Data API object based on this
    configuration.
  */
  if (!(run_obj->apid_global= ic_create_apid_global(
                     (IC_API_CONFIG_SERVER*)run_obj->config.apic,
                     TRUE,
                     &ret_code,
                     err_str)))
  {
    ic_printf("%s", err_str);
    DEBUG_RETURN_INT(ret_code);
  }
  DEBUG_RETURN_INT(0);
}

static void
report_startup_completed(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  DEBUG_PRINT(CONFIG_LEVEL, ("Startup is completed"));
  ic_mutex_lock(run_obj->state.protect_state);
  run_obj->state.cs_started= TRUE;
  ic_mutex_unlock(run_obj->state.protect_state);
}

/*
  MODULE: Run Cluster Server
  --------------------------
    This is the module that provided with a configuration data structures
    ensures that anyone can request this configuration through a given
    socket and port.

    The module implements the IC_RUN_CLUSTER_SERVER interface.
    This interface has seven methods:

    ic_create_run_cluster: This creates the IC_RUN_CLUSTER_SERVER object
    ic_start_cluster_server: This starts the cluster server, implemented in
                           the routine start_cluster_server
                           Implemented in the start cluster server module.
    ic_fill_error_buffer:  This creates an error message in error cases.
                           Implemented in rcs_fill_error_buffer.
    ic_run_cluster_server: This runs the cluster server, it is implemented
                           in the routine run_cluster_server
    ic_stop_cluster_server: Stops the cluster server in an orderly manner.
                            Implemented in stop cluster server
                            Implemented in the stop cluster server module.
    ic_free_run_cluster:   This routine frees the IC_RUN_CLUSTER_SERVER
                           object and all other data allocated by cluster
                           server.
*/

static void free_run_cluster(IC_RUN_CLUSTER_SERVER *run_obj);
static int run_cluster_server(IC_RUN_CLUSTER_SERVER *run_obj);
static gchar* rcs_fill_error_buffer(IC_RUN_CLUSTER_SERVER *run_obj,
                                    int error_code,
                                    gchar *error_buffer);
static void free_run_cluster_protect(IC_INT_RUN_CLUSTER_SERVER *run_obj);

/*
 Here starts the code part of the Run Cluster Server Module
 ============================================================
*/

IC_RUN_CLUSTER_SERVER*
ic_create_run_cluster(IC_STRING *config_dir,
                      const gchar *process_name,
                      guint32 my_node_id)
{
  IC_INT_RUN_CLUSTER_SERVER *run_obj= NULL;
  IC_MEMORY_CONTAINER *conf_mc_ptr;
  IC_THREADPOOL_STATE *tp_state= NULL;
  DEBUG_ENTRY("ic_create_run_cluster");

  if (!(run_obj= (IC_INT_RUN_CLUSTER_SERVER*)ic_calloc(
                sizeof(IC_INT_RUN_CLUSTER_SERVER))))
  {
    DEBUG_RETURN_PTR(NULL);
  }
  if (!(conf_mc_ptr= ic_create_memory_container(MC_DEFAULT_BASE_SIZE,
                                                0, TRUE)))
    goto error;
  if (!(run_obj->new_config.apic=
        (IC_INT_API_CONFIG_SERVER*)conf_mc_ptr->mc_ops.ic_mc_calloc(
                conf_mc_ptr, sizeof(IC_INT_API_CONFIG_SERVER))))
    goto error;
  if (!(run_obj->state.protect_state= ic_mutex_create()))
    goto error;
  if (!(run_obj->state.start_cond= ic_cond_create()))
    goto error;
  if (!(run_obj->state.connect_cond= ic_cond_create()))
    goto error;
  if (!(run_obj->state.update_cond= ic_cond_create()))
    goto error;
  if (!(run_obj->state.check_if_cs_started_cond= ic_cond_create()))
    goto error;
  if (!(run_obj->state.wait_server_connect_cond= ic_cond_create()))
    goto error;
  if (!(run_obj->config_mutex= ic_mutex_create()))
    goto error;
  if (!(run_obj->config_cond= ic_cond_create()))
    goto error;
  if (!(tp_state= ic_create_threadpool(IC_DEFAULT_MAX_THREADPOOL_SIZE, FALSE)))
    goto error;

  /*
    Initialise the Cluster Server state, the state is protected by a mutex to
    ensure when several connections receive requests only one at a time can
    change the Cluster Server state.
  */
  run_obj->conf_mc_ptr= conf_mc_ptr;

  run_obj->process_name= process_name;
  run_obj->config_dir= config_dir;
  run_obj->cs_nodeid= my_node_id;
  run_obj->locked_configuration= FALSE;
  run_obj->config_change_ongoing= FALSE;
  run_obj->conf_ref_count= 0;

  run_obj->new_config.max_cluster_id= 0;
  run_obj->new_config.num_clusters= 0;

  run_obj->state.err_obj.err_num= 0;
  run_obj->state.err_obj.line_number= 0;
  run_obj->tp_state= tp_state;

  /* Create the socket object for the Cluster Server */
  if (!(run_obj->conn= ic_create_socket_object(
                           FALSE, /* Server connection */
                           FALSE, /* Don't use mutex */
                           FALSE, /* Don't use connect thread */
                           CONFIG_READ_BUF_SIZE)))
    goto error;

  run_obj->run_op.ic_start_cluster_server= start_cluster_server;
  run_obj->run_op.ic_fill_error_buffer= rcs_fill_error_buffer;
  run_obj->run_op.ic_run_cluster_server= run_cluster_server;
  run_obj->run_op.ic_stop_cluster_server= stop_cluster_server;
  run_obj->run_op.ic_free_run_cluster= free_run_cluster;
  DEBUG_RETURN_PTR((IC_RUN_CLUSTER_SERVER*)run_obj);

error:
  if (conf_mc_ptr)
  {
    if (tp_state)
    {
      tp_state->tp_ops.ic_threadpool_stop(tp_state);
    }
    free_run_cluster_protect(run_obj);
    if (run_obj->conn)
    {
      run_obj->conn->conn_op.ic_free_connection(run_obj->conn);
    }
    conf_mc_ptr->mc_ops.ic_mc_free(conf_mc_ptr);
  }
  ic_free(run_obj);
  DEBUG_RETURN_PTR(NULL);
}

/* Implements the ic_fill_error_buffer method */
static gchar*
rcs_fill_error_buffer(IC_RUN_CLUSTER_SERVER *ext_run_obj,
                      int error_code,
                      gchar *error_buffer)
{
  IC_INT_RUN_CLUSTER_SERVER *run_obj= (IC_INT_RUN_CLUSTER_SERVER*)ext_run_obj;
  gchar *extra_err_str= NULL;
  guint32 err_line= 0;

  if (error_code != run_obj->state.err_obj.err_num &&
      run_obj->state.err_obj.err_num != 0)
  {
    extra_err_str= ic_get_error_message(error_code);
    err_line= run_obj->state.err_obj.line_number;
    error_code= run_obj->state.err_obj.err_num;
  }
  return ic_common_fill_error_buffer(extra_err_str,
                                     err_line,
                                     error_code,
                                     error_buffer);
}

/* Free hash on communication objects per cluster */
static void
release_hash_on_cluster_config(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 i;
  IC_CLUSTER_CONFIG *clu_conf;

  for (i= 0; i < IC_MAX_CLUSTER_ID; i++)
  {
    clu_conf= run_obj->config.conf_objects[i];
    if (clu_conf)
    {
      clu_conf->ic_free_cluster_config(clu_conf);
    }
  }
}

/* Free IC_RUN_CLUSTER_SERVER object, implements ic_free_run_cluster method */
static void
free_run_cluster(IC_RUN_CLUSTER_SERVER *ext_run_obj)
{
  IC_INT_RUN_CLUSTER_SERVER *run_obj= (IC_INT_RUN_CLUSTER_SERVER*)ext_run_obj;
  IC_APID_GLOBAL *apid_global;
  IC_THREADPOOL_STATE *tp_state;
  IC_CONNECTION *conn;
  DEBUG_ENTRY("free_run_cluster");

  if (run_obj)
  {
    tp_state= run_obj->tp_state;
    if (tp_state)
    {
      tp_state->tp_ops.ic_threadpool_set_stop_flag(tp_state);
      /* Ensure no threads is sleeping on ic_cond_wait */
      ic_mutex_lock(run_obj->state.protect_state);
      ic_cond_broadcast(run_obj->config_cond);
      ic_cond_broadcast(run_obj->state.start_cond);
      ic_cond_broadcast(run_obj->state.connect_cond);
      ic_cond_broadcast(run_obj->state.update_cond);
      ic_cond_broadcast(run_obj->state.check_if_cs_started_cond);
      ic_cond_signal(run_obj->state.wait_server_connect_cond);
      ic_mutex_unlock(run_obj->state.protect_state);
    }
    apid_global= run_obj->apid_global;
    if (apid_global)
    {
      apid_global->apid_global_ops->ic_free_apid_global(apid_global);
    }
    tp_state= run_obj->tp_state;
    if (tp_state)
    {
      tp_state->tp_ops.ic_threadpool_stop(tp_state);
    }
    conn= run_obj->conn;
    if (conn)
    {
      conn->conn_op.ic_free_connection(conn);
    }
    release_hash_on_cluster_config(run_obj);
    free_run_cluster_protect(run_obj);
    if (run_obj->conf_mc_ptr)
    {
      run_obj->conf_mc_ptr->mc_ops.ic_mc_free(run_obj->conf_mc_ptr);
    }
    ic_free(run_obj);
  }
  DEBUG_RETURN_EMPTY;
}

static void
free_run_cluster_protect(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  if (run_obj->state.protect_state)
  {
    ic_mutex_destroy(&run_obj->state.protect_state);
  }
  if (run_obj->state.start_cond)
  {
    ic_cond_destroy(&run_obj->state.start_cond);
  }
  if (run_obj->state.connect_cond)
  {
    ic_cond_destroy(&run_obj->state.connect_cond);
  }
  if (run_obj->state.update_cond)
  {
    ic_cond_destroy(&run_obj->state.update_cond);
  }
  if (run_obj->state.check_if_cs_started_cond)
  {
    ic_cond_destroy(&run_obj->state.check_if_cs_started_cond);
  }
  if (run_obj->state.wait_server_connect_cond)
  {
    ic_cond_destroy(&run_obj->state.wait_server_connect_cond);
  }
  if (run_obj->config_mutex)
  {
    ic_mutex_destroy(&run_obj->config_mutex);
  }
  if (run_obj->config_cond)
  {
    ic_cond_destroy(&run_obj->config_cond);
  }
}

/*
  MODULE: Run Cluster Server method
  ---------------------------------
 *
 * Uses the methods complete_startup_handling and report_startup_completed
 * which are part of the Cluster Server Start Module before completing
 * startup properly.
 *
 * The RUN CLUSTER SERVER has a number of support methods internal to its
 * implementation. The run_cluster_server method listens to the socket for
 * the cluster server. As soon as someone connects, it creates a thread to
 * service the client request. It's in this thread where most of the
 * complexity of this module resides.
 *
 * To start the new thread the start_cluster_server_thread is used, using
 * a socket object which was forked from the socket which was listened to
 * in run_cluster_server.
 *
 * The new thread is executed in the run_cluster_server_thread method.
 * This method implements the high level parts of the NDB Management Server
 * protocol. For each action that is available there is a method handling
 * that action. These are the handler routines:
 * check_config_req: Check for configuration request and call
 *                   handle_config_request in case it is
 * handle_get_cluster_list: Request to get a list of cluster id and names
 * handle_report_event: Report an event from the client to the Cluster Server
 * handle_get_mgmd_nodeid_req: Get node id of Cluster Server
 * handle_convert_transporter_request: Convert socket to NDB Protocol
 * handle_start_protocol: Handles connect from starting Cluster Server
 *   This method is handled by the Cluster Server Start Module.
 * handle_set_connection_parameter_req: Set connection parameters for new
 *   NDB Protocol socket
 * handle_config_request: A request from the client to get a cluster
 *   configuration
 *
 * One connection can contain a number of these protocol actions and the
 * socket can as mentioned above also be converted to a NDB Protocol
 * socket.
 *
 * The only routine above with some complexity is the handle_config_request.
 */


struct ic_rc_param
{
  guint64 node_number;
  guint64 version_number;
  guint64 node_type;
  guint64 cluster_id;
  guint64 client_nodeid;
};
typedef struct ic_rc_param IC_RC_PARAM;

static int start_cluster_server_thread(IC_INT_RUN_CLUSTER_SERVER* run_obj,
                                       IC_CONNECTION *conn,
                                       IC_THREADPOOL_STATE *tp_state,
                                       guint32 thread_id);
static gpointer run_cluster_server_thread(gpointer data);
static int handle_get_cluster_list(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                   IC_CONNECTION *conn);
static int handle_get_connection_parameter(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                           IC_CONNECTION *conn,
                                           gchar *read_buf,
                                           guint32 read_size,
                                           guint32 *error_line);
static int handle_report_event(IC_CONNECTION *conn);
static int handle_get_mgmd_nodeid_req(IC_CONNECTION *conn,
                                      guint32 cs_nodeid,
                                      gchar *read_buf,
                                      guint32 read_size,
                                      guint32 *error_line);
static int handle_set_connection_parameter_req(
                                IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                IC_CONNECTION *conn,
                                guint32 client_nodeid);
static int handle_convert_transporter_request(
                                       IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                       IC_CONNECTION *conn,
                                       IC_RC_PARAM *param,
                                       gchar *read_buf,
                                       guint32 read_size,
                                       guint32 *error_line);
static int handle_config_request(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                 IC_CONNECTION *conn,
                                 IC_RC_PARAM *param);

/* Implements ic_run_cluster_server method.  */
static int
run_cluster_server(IC_RUN_CLUSTER_SERVER *ext_run_obj)
{
  IC_INT_RUN_CLUSTER_SERVER *run_obj= (IC_INT_RUN_CLUSTER_SERVER*)ext_run_obj;
  IC_THREADPOOL_STATE *tp_state= run_obj->tp_state;
  gchar str_port[IC_NUMBER_SIZE];
  int ret_code= 0;
  gboolean allocated_thread= FALSE;
  guint32 thread_id;
  guint32 my_nodeid;
  IC_CONNECTION *conn, *fork_conn;
  IC_CLUSTER_SERVER_CONFIG *cs_conf;
  IC_CLUSTER_CONFIG *clu_conf;
  guint32 str_len;
  DEBUG_ENTRY("run_cluster_server");

  if ((ret_code= complete_startup_handling(run_obj)))
  {
    DEBUG_RETURN_INT(ret_code);
  }

  if (tp_state->tp_ops.ic_threadpool_get_stop_flag(tp_state))
    goto error;

  report_startup_completed(run_obj);

  if ((ret_code= handle_cluster_server_up_and_down(run_obj)))
  {
    DEBUG_RETURN_INT(ret_code);
  }

  if (tp_state->tp_ops.ic_threadpool_get_stop_flag(tp_state))
    goto error;

  /*
    Startup for our Cluster Server node is completed. It's now time to
    start the socket that listens to incoming requests. We get our
    hostname and port number from the Grid configuration. At this
    point no one can change the configuration, so we don't need to
    lock it while reading it.
  */
  conn= run_obj->conn;
  my_nodeid= run_obj->cs_nodeid;
  clu_conf= get_any_cluster_config(&run_obj->config);
  cs_conf= (IC_CLUSTER_SERVER_CONFIG*)clu_conf->node_config[my_nodeid];
  ic_require(clu_conf->node_types[my_nodeid] == IC_CLUSTER_SERVER_NODE);
  ic_guint64_str((guint64)cs_conf->cluster_server_port_number,
                 str_port,
                 &str_len);

  conn->conn_op.ic_prepare_server_connection(conn,
                                             cs_conf->hostname,
                                             str_port,
                                             NULL,
                                             NULL,
                                             0,
                                             TRUE);

  if ((ret_code= conn->conn_op.ic_set_up_connection(conn,
                                               check_for_stopped_rcs_threads,
                                               (void*)tp_state)))
  {
    DEBUG_PRINT(CONFIG_LEVEL,
      ("Failed to set-up listening connection"));
    goto error;
  }
  while (!tp_state->tp_ops.ic_threadpool_get_stop_flag(tp_state))
  {
    if (allocated_thread)
    {
      /**
       * If for some reason we fail to come to the point of starting the
       * thread, then we need to release the allocated thread id.
       * If we manage to start the thread, this will be handled by the
       * thread that was started as part of thread stop code.
       */
      tp_state->tp_ops.ic_threadpool_free_thread_id(tp_state, thread_id);
    }
    if ((ret_code= tp_state->tp_ops.ic_threadpool_get_thread_id_wait(tp_state,
                                                     &thread_id,
                                                     IC_MAX_THREAD_WAIT_TIME)))
    {
      /**
       * Either we've waited for 60 seconds without getting a free thread
       * which spells trouble since the Cluster Server doesn't handle
       * that many long-running activities. Or we have received the stop
       * flag and it's time to stop simply.
       */
      DEBUG_PRINT(THREAD_LEVEL, ("Failed to get new thread id"));
      goto error;
    }
    allocated_thread= TRUE;
    if ((ret_code= conn->conn_op.ic_accept_connection(conn)))
    {
      DEBUG_PRINT(COMM_LEVEL,
        ("Failed to accept a new connection"));
      continue;
    }
    DEBUG_PRINT(CONFIG_LEVEL,
      ("Run cluster server has set up connection and has received"
       " a connection"));
    if (!(fork_conn=
           conn->conn_op.ic_fork_accept_connection(conn,
                                          FALSE))) /* No mutex */
    {
      DEBUG_PRINT(COMM_LEVEL,
      ("Failed to fork a new connection from an accepted connection"));
      continue;
    }
    if ((ret_code= start_cluster_server_thread(run_obj,
                                               fork_conn,
                                               tp_state,
                                               thread_id)))
    {
      DEBUG_PRINT(THREAD_LEVEL,
        ("Start new thread to handle configuration request failed"));
      /**
       * We failed to start thread for new connection. We need to close the
       * connection and continue processing.
       */
      fork_conn->conn_op.ic_close_connection(fork_conn);
      continue;
    }
    /**
     * Now we have successfully started the new thread, it is now this thread
     * that has the responsibility to return the thread id to the thread pool.
     */
    allocated_thread= FALSE;
    DEBUG_PRINT(CONFIG_LEVEL,
      ("Ready to accept a new connection"));
  }
  if (allocated_thread)
  {
    tp_state->tp_ops.ic_threadpool_free_thread_id(tp_state, thread_id);
  }

error:
  DEBUG_RETURN_INT(ret_code);
}

static int
check_for_stopped_rcs_threads(void *object, int not_used)
{
  IC_THREADPOOL_STATE *tp_state= (IC_THREADPOOL_STATE*)object;
  (void) not_used;

  tp_state->tp_ops.ic_threadpool_check_threads(tp_state);
  if (tp_state->tp_ops.ic_threadpool_get_stop_flag(tp_state))
  {
    return 1;
  }
  return 0;
}

/* Start a new Cluster Server thread */
static int
start_cluster_server_thread(IC_INT_RUN_CLUSTER_SERVER* run_obj,
                            IC_CONNECTION *conn,
                            IC_THREADPOOL_STATE *tp_state,
                            guint32 thread_id)
{
  int ret_code;

  conn->conn_op.ic_set_param(conn, (void*)run_obj);
  DEBUG_PRINT(THREAD_LEVEL, ("Starting thread in run_cluster_server_thread"));
  ret_code= tp_state->tp_ops.ic_threadpool_start_thread_with_thread_id(
                                              tp_state,
                                              thread_id,
                                              run_cluster_server_thread,
                                              conn,
                                              IC_SMALL_STACK_SIZE,
                                              FALSE);
  return ret_code;
}

/* Run a Cluster Server thread */
static gpointer
run_cluster_server_thread(gpointer data)
{
  IC_THREAD_STATE *thread_state= (IC_THREAD_STATE*)data;
  gchar *read_buf;
  guint32 read_size;
  IC_THREADPOOL_STATE *rcs_tp;
  IC_CONNECTION *conn;
  IC_INT_RUN_CLUSTER_SERVER *run_obj;
  int ret_code;
  guint32 error_line= 0;
  int state= INITIAL_STATE;
  IC_RC_PARAM param;
  DEBUG_THREAD_ENTRY("run_cluster_server_thread");
  rcs_tp= thread_state->ic_get_threadpool(thread_state);
  conn= (IC_CONNECTION*) rcs_tp->ts_ops.ic_thread_get_object(thread_state);

  param.cluster_id= 0; /* Only support cluster id 0 for now */
  rcs_tp->ts_ops.ic_thread_started(thread_state);
  run_obj= (IC_INT_RUN_CLUSTER_SERVER*)conn->conn_op.ic_get_param(conn);
  /*
    Continue receiving until stop or timeout occurs, if a timeout occurs
    we will continue also in the case of start ongoing yet.
  */
  while ((!(ret_code= ic_rec_with_cr(conn, &read_buf, &read_size)) ||
          (run_obj->state.cs_starting &&
           ret_code == IC_ERROR_RECEIVE_TIMEOUT)) &&
         !rcs_tp->ts_ops.ic_thread_get_stop_flag(thread_state))
  {
    switch (state)
    {
      case INITIAL_STATE:
        if (!ic_check_buf(read_buf,
                          read_size,
                          get_cluster_list_str,
                          strlen(get_cluster_list_str)))
        {
          if ((ret_code= handle_get_cluster_list(run_obj, conn)))
          {
            error_line= __LINE__;
            goto error;
          }
          state= WAIT_GET_NODEID;
          break;
        }
        if (!ic_check_buf(read_buf,
                          read_size,
                          get_nodeid_str,
                          strlen(get_nodeid_str)))
        {
          if ((ret_code= handle_config_request(run_obj, conn, &param)))
          {
            error_line= __LINE__;
            goto error;
          }
          state= WAIT_GET_MGMD_NODEID;
          break;
        }
        if (!ic_check_buf(read_buf,
                          read_size,
                          report_event_str,
                          strlen(report_event_str)))
        {
          if ((ret_code= handle_report_event(conn)))
          {
            error_line= __LINE__;
            goto error;
          }
          break; /* The report event is always done in separate connection */
        }
        if (!ic_check_buf(read_buf,
                          read_size,
                          set_connection_parameter_str,
                          strlen(set_connection_parameter_str)))
        {
          if ((ret_code= handle_set_connection_parameter_req(run_obj,
                                                          conn,
                                                          (guint32)0)))
          {
            error_line= __LINE__;
            goto error;
          }
          /* Always sent as only message in separate connection */
          goto end_thread;
        }
        if (!ic_check_buf(read_buf,
                          read_size,
                          get_version_str,
                          strlen(get_version_str)))
        {
          /* Only used by NDB data nodes */
          ic_step_back_rec_with_cr(conn, read_size);
          if ((ret_code= rec_get_version_req(conn)) ||
              (ret_code= send_get_version_reply(conn,
                                                (guint64)IC_DATA_SERVER_NODE)))
          {
            error_line= __LINE__;
            goto error;
          }
          /* Keep initial state */
          break;
        }
        if (!ic_check_buf(read_buf,
                          read_size,
                          get_config_str,
                          strlen(get_config_str)))
        {
          guint8 *config_base64_str;
          guint32 config_len;
          guint64 version_number= 0;
          ic_step_back_rec_with_cr(conn, read_size);
          if ((ret_code= rec_get_config_req(conn,
                                            &version_number,
                                            IC_DATA_SERVER_TYPE_PROTOCOL)) ||
              (ret_code= ic_get_base64_config(
                    run_obj->config.conf_objects[0],
                    &config_base64_str,
                    &config_len,
                    version_number)))
          {
            error_line= __LINE__;
            goto error;
          }
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Converted configuration to a base64 representation"));
          ret_code= send_get_config_reply(conn,
                                          (gchar*)config_base64_str,
                                          config_len);
          ic_free((gchar*)config_base64_str);
          if (ret_code)
          {
            error_line= __LINE__;
            goto error;
          }
          state= WAIT_GET_MGMD_NODEID;
          break;
        }
        /* Fall through since get connection parameter is also allowed */
      case GET_CONNECTION_PARAMETER:
        if ((ret_code= handle_get_connection_parameter(run_obj,
                                                       conn,
                                                       read_buf,
                                                       read_size,
                                                       &error_line)))
          goto error;
        state= GET_CONNECTION_PARAMETER;
        break; /* Always handled in separate connection */
      case WAIT_GET_NODEID:
        if (!ic_check_buf(read_buf,
                          read_size,
                          get_nodeid_str,
                          strlen(get_nodeid_str)))
        {
          if ((ret_code= handle_config_request(run_obj, conn, &param)))
          {
            error_line= __LINE__;
            goto error;
          }
          state= WAIT_GET_MGMD_NODEID;
          break;
        }
        error_line= __LINE__;
        goto error;
      case WAIT_GET_MGMD_NODEID:
        if ((ret_code= handle_get_mgmd_nodeid_req(conn,
                                                  run_obj->cs_nodeid,
                                                  read_buf,
                                                  read_size,
                                                  &error_line)))
          goto error;
        state= WAIT_SET_CONNECTION;
        break;
      case WAIT_SET_CONNECTION:
        if (!ic_check_buf(read_buf,
                          read_size,
                          set_connection_parameter_str,
                          strlen(set_connection_parameter_str)))
        {
          if ((ret_code= handle_set_connection_parameter_req(
                            run_obj,
                            conn,
                            (guint32)param.client_nodeid)))
          {
            error_line= __LINE__;
            goto error;
          }
          break;
        }
        /*
          Here it is ok to fall through, the WAIT_SET_CONNECTION is an
          optional state. We can receive zero or many set connection
          messages. At any time we can also receive a convert transporter
          message.
        */
      case WAIT_CONVERT_TRANSPORTER:
        if ((ret_code= handle_convert_transporter_request(run_obj,
                                                          conn,
                                                          &param,
                                                          read_buf,
                                                          read_size,
                                                          &error_line)))
          goto error;
        conn= NULL;
        goto break_out;
      default:
        abort();
        break;
    }
  }
break_out:
  if (conn)
  {
    DEBUG_PRINT(CONFIG_LEVEL, ("Connection closed by other side"));
  }
  else
  {
    DEBUG_PRINT(CONFIG_LEVEL, ("Connection taken over by Data API"));
  }
end_thread:
  if (conn)
  {
    conn->conn_op.ic_free_connection(conn);
  }
  rcs_tp->ts_ops.ic_thread_stops(thread_state);
  DEBUG_THREAD_RETURN;

error:
  read_buf[read_size]= 0;
  ic_printf("Protocol error line %d", error_line);
  ic_printf("Protocol message: %s", read_buf);
  goto end_thread;
}

/*
  MODULE: Cluster Configuration Concurrency Control
  -------------------------------------------------
  Methods to handle configuration reads and writes.
  We can have two active configurations. Reads of a
  configuration is allowed to continue until done. Then
  when all readers of the old configuration are done
  we allow new readers to start reading the new configuration.
*/
static void
inc_config_ref_count(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  ic_mutex_lock(run_obj->config_mutex);
  while (run_obj->config_change_ongoing)
  {
    ic_cond_wait(run_obj->config_cond,
                 run_obj->config_mutex);
  }
  run_obj->conf_ref_count++;
  ic_mutex_unlock(run_obj->config_mutex);
}

static void
dec_config_ref_count(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  ic_mutex_lock(run_obj->config_mutex);
  run_obj->conf_ref_count--;
  if (run_obj->config_change_ongoing)
  {
    /* check_ready_to_release_config will release mutex */
    check_ready_to_release_config(run_obj, TRUE);
    return;
  }
  ic_mutex_unlock(run_obj->config_mutex);
}

static void
check_ready_to_release_config(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                              gboolean lock_held)
{
  IC_API_CONFIG_SERVER *old_apic;
  IC_MEMORY_CONTAINER *old_mc_ptr;

  if (!lock_held)
  {
    ic_mutex_lock(run_obj->config_mutex);
  }
  if (run_obj->conf_ref_count == 0)
  {
    run_obj->config_change_ongoing= FALSE;
    run_obj->locked_configuration= FALSE;
    old_apic= (IC_API_CONFIG_SERVER*)run_obj->config.apic;
    old_mc_ptr= run_obj->old_conf_mc_ptr;
    release_hash_on_cluster_config(run_obj);
    install_new_config(run_obj);
    ic_cond_broadcast(run_obj->config_cond);
    ic_mutex_unlock(run_obj->config_mutex);
    old_apic->api_op.ic_free_config(old_apic); /* Free old apic version */
    old_mc_ptr->mc_ops.ic_mc_free(old_mc_ptr); /* Release old config */
    return;
  }
  ic_mutex_unlock(run_obj->config_mutex);
}

/*
  MODULE: Get cluster list
  ------------------------
  Handle get cluster list protocol action in Cluster Server
*/

static int
handle_get_cluster_list(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                        IC_CONNECTION *conn)
{
  gchar cluster_name_buf[128];
  guint32 i;
  int ret_code;
  IC_CLUSTER_CONFIG *clu_conf;
  IC_STRING cluster_name;
  DEBUG_ENTRY("handle_get_cluster_list");

  inc_config_ref_count(run_obj);
  if ((ret_code= ic_rec_empty_line(conn)))
    goto error;
  if ((ret_code= ic_send_with_cr(conn, get_cluster_list_reply_str)))
    goto error;
  for (i= 0; i <= run_obj->config.max_cluster_id; i++)
  {
    clu_conf= run_obj->config.conf_objects[i];
    if (!clu_conf)
      continue;
    cluster_name_buf[0]= 0;
    IC_INIT_STRING(&cluster_name, cluster_name_buf, 0, TRUE);
    ic_add_string(&cluster_name, cluster_name_string);
    ic_add_ic_string(&cluster_name, &clu_conf->clu_info.cluster_name);
    if ((ret_code= ic_send_with_cr(conn, cluster_name.str)) ||
        (ret_code= ic_send_with_cr_with_number(conn,
                                               cluster_id_str,
                                               (guint64)i)))
      goto error;
  }
  if ((ret_code= ic_send_with_cr(conn, end_get_cluster_list_str)) ||
      (ret_code= ic_send_empty_line(conn)))
    goto error;
  dec_config_ref_count(run_obj);
  DEBUG_RETURN_INT(0);
error:
  dec_config_ref_count(run_obj);
  DEBUG_PRINT(CONFIG_LEVEL,
              ("Protocol error in get cluster list, code = %d", ret_code));
  PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
}

/*
  MODULE: Handle Get Connection Parameter Request
  -----------------------------------------------
*/
static int
handle_get_connection_parameter(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                IC_CONNECTION *conn,
                                gchar *read_buf,
                                guint32 read_size,
                                guint32 *error_line)
{
  int ret_code;
  guint32 cluster_id, node1_id, node2_id, param;
  IC_CLUSTER_CONFIG *clu_conf;
  guint32 port_number;
  IC_SOCKET_LINK_CONFIG *comm_section;
  DEBUG_ENTRY("handle_get_connection_parameter");

  if (ic_check_buf(read_buf, read_size, get_connection_parameter_str,
                   strlen(get_connection_parameter_str)))
  {
    *error_line= __LINE__;
    DEBUG_RETURN_INT(IC_PROTOCOL_ERROR);
  }
  if ((ret_code= ic_rec_cluster_id(conn, &cluster_id)) ||
      (ret_code= ic_rec_number(conn, node1_str, &node1_id)) ||
      (ret_code= ic_rec_number(conn, node2_str, &node2_id)) ||
      (ret_code= ic_rec_number(conn, param_str, &param)) ||
      (ret_code= ic_rec_empty_line(conn)) ||
      (ret_code= IC_ERROR_SET_CONNECTION_PARAMETER_WRONG_PARAM, FALSE) ||
      (param != SOCKET_SERVER_PORT_NUMBER))
    goto error;
  if (cluster_id > run_obj->config.max_cluster_id ||
      !(clu_conf= run_obj->config.conf_objects[cluster_id]))
  {
    ret_code= IC_ERROR_NO_SUCH_CLUSTER;
    goto error;
  }
  if ((ret_code= get_socket_link_config(node1_id,
                                        node2_id,
                                        &comm_section,
                                        clu_conf)))
    goto error;
  /*
    Found a valid communication section.
    We return the configured port number if it is nonzero in which case it
    is permanent. If it is zero we instead return the dynamic port number.
    This could be a zero as well which indicates we don't know the value
    currently in which case the node asking will have to ask again later.
  */
  if (comm_section->server_port_number)
  {
    port_number= comm_section->server_port_number;
  }
  else
  {
    port_number= comm_section->dynamic_server_port_number;
  }

  if ((ret_code= ic_send_with_cr(conn, get_connection_parameter_reply_str)) ||
      (ret_code= ic_send_with_cr_with_number(conn, node1_str, node1_id)) ||
      (ret_code= ic_send_with_cr_with_number(conn, node2_str, node2_id)) ||
      (ret_code= ic_send_with_cr_with_number(conn,
                                             param_str,
                                             SOCKET_SERVER_PORT_NUMBER)) ||
      (ret_code= ic_send_with_cr_with_number(conn,
                                             value_str,
                                             (guint64)port_number)) ||
      (ret_code= ic_send_empty_line(conn)))
    goto error;
  DEBUG_RETURN_INT(0);
error:
  DEBUG_PRINT(CONFIG_LEVEL,
    ("Error from handle_get_connection_parameter, code = %u", ret_code));
  *error_line= __LINE__;
  DEBUG_RETURN_INT(ret_code);
}

/*
  MODULE: report event
  --------------------

  This protocol is used by the data server nodes to report shutdown of their
  process.

  The data contained in the protocol message is the same as the data sent in
  a EVENT_REP signal used by the NDB Protocol but instead a separate NDB MGM
  Protocol connection is opened up and used to report this special event.

  Data[0]:
  Bit 0-15 contains Event Type (always 27 in this case which means a shutdown
           has been completed).
  Bit 16-31 contains the node id of the node being shutdown.
  Data[1]:
  0:       Means it isn't restarting
  1:       Means restart and not initial restart
  2:       Means start from initial state
  4:       Another variant of initial restart
  Data[2]:
  OS Signal which caused shutdown (e.g. 11 for segmentation fault)

  If the shutdown was caused by an error there are three more words, for
  graceful shutdown only the above words are set.

  Data[4]:
  Error number
  Data[5]:
  Start phase when error occurred
  Data[6]:
  Always equal to 0
  TODO: Should direct output to file instead
*/
static int
handle_report_event(IC_CONNECTION *conn)
{
  guint64 num_array[32], length;
  gchar *read_buf;
  guint32 read_size;
  int ret_code;
  guint32 report_node_id, os_signal_num, error_num= 0, start_phase= 0;
  DEBUG_ENTRY("handle_report_event");

  if ((ret_code= ic_rec_with_cr(conn, &read_buf, &read_size)) ||
      (ic_check_buf_with_int(read_buf, read_size, length_str,
                             strlen(length_str), &length)) ||
      (ret_code= ic_rec_with_cr(conn, &read_buf, &read_size)) ||
      (ic_check_buf_with_many_int(read_buf, read_size, data_str,
                                  strlen(data_str), (guint32)length,
                                  &num_array[0])))
  {
    ret_code= ret_code ? ret_code : IC_PROTOCOL_ERROR;
    PROTOCOL_CONN_CHECK_ERROR_GOTO(ret_code);
  }
  if ((ret_code= ic_rec_empty_line(conn)))
    PROTOCOL_CONN_CHECK_ERROR_GOTO(ret_code);
  if ((ret_code= ic_send_with_cr(conn, report_event_reply_str)) ||
      (ret_code= ic_send_with_cr(conn, result_ok_str)) ||
      (ret_code= ic_send_empty_line(conn)))
    goto error;
  report_node_id= (guint32)(num_array[0] >> 16);
  ic_assert((num_array[0] & 0xFFFF) == 59);
  if (num_array[1] == 0)
  {
    ic_printf("Node %u has shutdown", report_node_id);
  }
  else if (num_array[1] == 1)
  {
    ic_printf("Node %u has restarted", report_node_id);
  }
  else
  {
    ic_printf("Node %u has performed initial restart", report_node_id);
  }
  if (length == (guint64)3)
  {
    ic_printf(" due to graceful shutdown");
  }
  else
  {
    ic_assert(length == (guint64)6);
    ic_assert(num_array[5] == 0);
    os_signal_num= (guint32)num_array[2];
    error_num= (guint32)num_array[3];
    start_phase= (guint32)num_array[4];
    ic_printf(" due to error %u, OS Signal %u in startphase %u",
              error_num, os_signal_num, start_phase);
  }
  DEBUG_RETURN_INT(0);
error:
  DEBUG_PRINT(CONFIG_LEVEL,
    ("Error from handle_report_event, code = %u", ret_code));
  DEBUG_RETURN_INT(ret_code);
}

/*
  MODULE: Handle Get Cluster Server node id request
  -------------------------------------------------
*/
static int
handle_get_mgmd_nodeid_req(IC_CONNECTION *conn,
                           guint32 cs_nodeid,
                           gchar *read_buf,
                           guint32 read_size,
                           guint32 *error_line)
{
  int ret_code;
  DEBUG_ENTRY("handle_get_mgmd_nodeid_req");

  if (ic_check_buf(read_buf, read_size, get_mgmd_nodeid_str,
                   strlen(get_mgmd_nodeid_str)))
  {
    *error_line= __LINE__;
    DEBUG_RETURN_INT(IC_PROTOCOL_ERROR);
  }
  if ((ret_code= ic_rec_empty_line(conn)) ||
      (ret_code= ic_send_with_cr(conn, get_mgmd_nodeid_reply_str)) ||
      (ret_code= ic_send_with_cr_with_number(conn,
                                             nodeid_str,
                                             (guint64)cs_nodeid)) ||
      (ret_code= ic_send_empty_line(conn)))
  {
    DEBUG_PRINT(CONFIG_LEVEL,
                ("Protocol error in get mgmd nodeid, code = %d", ret_code));
    *error_line= __LINE__;
    DEBUG_RETURN_INT(ret_code);
  }
  DEBUG_RETURN_INT(0);
}

/*
  MODULE: Handle Set Connection Parameter Request
  -----------------------------------------------
  This request can be sent from client nodes when they have set a dynamic
  port and want to spread the information about this dynamic assignment.
  It can also be received from another Cluster Server that needs to
  replicate this information.
*/
static int send_set_connection_param_message(IC_CONNECTION *conn,
                                             guint32 cluster_id,
                                             guint32 node1_id,
                                             guint32 node2_id,
                                             guint32 port_number);
static int rec_set_connection_param_message(IC_CONNECTION *conn);
static int send_set_connection_parameter(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                         guint32 cluster_id,
                                         guint32 node1_id,
                                         guint32 node2_id,
                                         guint32 port_number);

/*
  handle_set_connection_parameter_req

  client_nodeid:     Node id of client, 0 if sent by Cluster Server
*/
static int
handle_set_connection_parameter_req(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                    IC_CONNECTION *conn,
                                    guint32 client_nodeid)
{
  int ret_code;
  guint32 cluster_id, node1_id, node2_id, param;
  int value;
  const gchar *the_result_str, *the_message_str;
  IC_SOCKET_LINK_CONFIG *comm_section;
  IC_CLUSTER_CONFIG *clu_conf;
  DEBUG_ENTRY("handle_set_connection_parameter_req");

  if ((ret_code= ic_rec_cluster_id(conn, &cluster_id)) ||
      (ret_code= ic_rec_number(conn, node1_str, &node1_id)) ||
      (ret_code= ic_rec_number(conn, node2_str, &node2_id)) ||
      (ret_code= ic_rec_number(conn, param_str, &param)) ||
      (ret_code= ic_rec_int_number(conn, value_str, &value)) ||
      (ret_code= ic_rec_empty_line(conn)))
    goto error;
  /*
    We received a correct set connection parameter protocol message.
    Now we need to verify also that the data is reasonable and also
    perform the action associated with it.
  */
 
  if (param != SOCKET_SERVER_PORT_NUMBER)
  {
    ret_code= IC_ERROR_SET_CONNECTION_PARAMETER_WRONG_PARAM;
  }
  else if (client_nodeid != 0 && node1_id != client_nodeid)
  {
    ret_code= IC_ERROR_SET_CONNECTION_PARAMETER_WRONG_NODES;
  }
  else if (cluster_id > run_obj->config.max_cluster_id ||
           !(clu_conf= run_obj->config.conf_objects[cluster_id]))
  {
    ret_code= IC_ERROR_NO_SUCH_CLUSTER;
  }
  else if (!(ret_code= get_socket_link_config(node1_id,
                                              node2_id,
                                              &comm_section,
                                              clu_conf)))
  {
    if (comm_section->server_port_number)
    {
      ret_code= IC_ERROR_SET_CONNECTION_NO_DYNAMIC;
    }
  }

  if (!ret_code && client_nodeid)
  {
    /*
      We have successfully received a request to update the dynamic
      port number. Before we make the actual update we need to update
      the other cluster servers successfully first since the request
      was received from a client node.
    */
    ret_code= send_set_connection_parameter(run_obj,
                                            cluster_id,
                                            node1_id,
                                            node2_id,
                                            value);
  }
  if (ret_code)
  {
    the_result_str= ic_error_str;
    the_message_str= ic_get_error_message(ret_code);
  }
  else
  {
    /* Update the actual dynamic server port number */
    comm_section->dynamic_server_port_number= value;

    the_message_str= ic_empty_string;
    the_result_str= ic_ok_str;
    /*
      We have received information about a dynamic port assignment.
      We need to spread this information to all other cluster servers
      in the grid, otherwise they cannot assist nodes starting up.
      We also need to update the configuration in memory in the cluster
      server, this is done by accessing the communication object and
      updating it.
      
      We also update the configuration information on disk to ensure
      a cluster server crash doesn't lose important information. However
      it is important to synchronize this information with any alive
      cluster server at start since this information can be changed also
      when not all cluster servers are up and running. It cannot be changed
      however when no cluster server is up since no node can start without
      a cluster server to read configuration information from.
    */
  }
  /* Now it's time to send the prepared response */
  if ((ret_code= ic_send_with_cr(conn, set_connection_parameter_reply_str)) ||
      (ret_code= ic_send_with_cr_two_strings(conn, message_str,
                                          the_message_str)) ||
      (ret_code= ic_send_with_cr_two_strings(conn, result_str,
                                          the_result_str)) ||
      (ret_code= ic_send_empty_line(conn)))
    goto error;
  /*
    We have now received a new port number to use for the nodes the
    starting node will communicate with.
  */
  DEBUG_RETURN_INT(0);
error:
  DEBUG_PRINT(CONFIG_LEVEL,
    ("Protocol error in set connection parameter, code = %d", ret_code));
  DEBUG_RETURN_INT(ret_code);
}

static int
send_set_connection_parameter(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                              guint32 cluster_id,
                              guint32 node1_id,
                              guint32 node2_id,
                              guint32 port_number)
{
  IC_INT_API_CONFIG_SERVER *apic= run_obj->config.apic;
  guint32 i;
  int ret_code= 0;
  IC_CONNECTION *conn;
  IC_RUN_CLUSTER_STATE *rc_state= &run_obj->state;
  gboolean is_master;
  DEBUG_ENTRY("send_set_connection_parameter");

  is_master= rc_start_update(rc_state);
  if (is_master)
  {
    for (i= 0; i < IC_MAX_CLUSTER_SERVERS; i++)
    {
      if (apic->cluster_conn.cs_nodeid[i] == run_obj->cs_nodeid ||
          run_obj->state.cs_servers[i].conn == NULL)
        continue;
      conn= run_obj->state.cs_servers[i].conn;
      if ((ret_code= send_set_connection_param_message(conn,
                                                       cluster_id,
                                                       node1_id,
                                                       node2_id,
                                                       port_number)) ||
          (ret_code= rec_set_connection_param_message(conn)))
      {
        ;
      }
    }
  }
  else
  {
  }
  rc_stop_update(rc_state);
  DEBUG_RETURN_INT(ret_code);
}

static int
send_set_connection_param_message(IC_CONNECTION *conn,
                                  guint32 cluster_id,
                                  guint32 node1_id,
                                  guint32 node2_id,
                                  guint32 port_number)
{
  int ret_code;
  DEBUG_ENTRY("send_set_connection_param_message");

  if ((ret_code= ic_send_with_cr(conn, set_connection_parameter_str)) ||
      (ret_code= ic_send_cluster_id(conn, cluster_id, TRUE)) ||
      (ret_code= ic_send_with_cr_with_number(conn, node1_str, node1_id)) ||
      (ret_code= ic_send_with_cr_with_number(conn, node2_str, node2_id)) ||
      (ret_code= ic_send_with_cr_with_number(conn,
                                             param_str,
                                             SOCKET_SERVER_PORT_NUMBER)) ||
      (ret_code= ic_send_with_cr_with_number(conn, value_str, port_number)) ||
      (ret_code= ic_send_empty_line(conn)))
  {
    ;
  }
  DEBUG_RETURN_INT(ret_code);
}

static int
rec_set_connection_param_message(IC_CONNECTION *conn)
{
  int ret_code;
  gchar rec_message_buf[32];
  gchar rec_result_buf[IC_MAX_ERROR_STRING_SIZE];
  DEBUG_ENTRY("rec_set_connection_param_message");

  if ((ret_code= ic_rec_simple_str(conn,
                                   set_connection_parameter_reply_str)) ||
      (ret_code= ic_rec_string(conn, message_str, rec_message_buf)) ||
      (ret_code= ic_rec_string(conn, result_str, rec_result_buf)) ||
      (ret_code= ic_rec_empty_line(conn)))
  {
    DEBUG_RETURN_INT(ret_code);
  }
  if (strcmp(ic_empty_string, rec_message_buf) != 0 ||
      strcmp(ic_ok_str, rec_result_buf) != 0)
  {
    ret_code= ic_translate_error_string(rec_result_buf);
  }
  DEBUG_RETURN_INT(ret_code);
}

/*
  MODULE: Handle Convert Transporter Request
  ------------------------------------------
*/
static int
handle_convert_transporter_request(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                   IC_CONNECTION *conn,
                                   IC_RC_PARAM *param,
                                   gchar *read_buf,
                                   guint32 read_size,
                                   guint32 *error_line)
{
  int ret_code;
  guint32 loc_read_size;
  guint32 len;
  gchar *loc_read_buf;
  guint64 number;
  int trp_type= IC_TCP_TRANSPORTER_TYPE;
  gchar cs_buf[2*IC_MAX_INT_STRING + 2];
  DEBUG_ENTRY("handle_convert_transporter_request");

  if (ic_check_buf(read_buf,
                   read_size,
                   convert_transporter_str,
                   strlen(convert_transporter_str)))
  {
    *error_line= __LINE__;
    DEBUG_RETURN_INT(IC_PROTOCOL_ERROR);
  }
  g_snprintf(cs_buf, sizeof(cs_buf), "%d %d",
             (guint32)run_obj->cs_nodeid, trp_type);
  if ((ret_code= ic_rec_empty_line(conn)))
    goto error;
  if ((ret_code= ic_rec_with_cr(conn, &loc_read_buf, &loc_read_size)))
    goto error;

  /*
    We get node_id_remote trp_type where node_id_remote is the string
    of the remote node id, then we have a space and finally a 0 as a
    character giving a TCP transporter type.
  */
  if ((ic_conv_str_to_int(loc_read_buf, &number, &len) != 0) ||
      (number > IC_MAX_NODE_ID) ||
      (loc_read_buf[len] != ' ') ||
      (((int)(loc_read_buf[len + 1] - '0')) != trp_type) ||
      (loc_read_size != (len + 2)))
  {
    *error_line= __LINE__;
    DEBUG_RETURN_INT(IC_PROTOCOL_ERROR);
  }
  param->node_number= (guint32)number;
  if ((ret_code= ic_send_with_cr(conn, cs_buf)) ||
      (ret_code= conn->conn_op.ic_flush_connection(conn)))
  {
    goto error;
  }
  /*
    At this point the connection is turned into a NDB Protocol
    connection. We do this by giving to a send node connection
    in the Data API part. We also need to inform the Cluster
    Server Data API thread that this connection exists. This
    thread is responsible for ensuring that heartbeats are
    sent properly but also all other traffic between cluster
    server and other nodes using the NDB Protocol.
  */
  DEBUG_PRINT(CONFIG_LEVEL,
    ("We are now converting connection to a NDB Protocol connection"));
  if ((ret_code=
         run_obj->apid_global->apid_global_ops->ic_external_connect(
                   run_obj->apid_global,
                   (guint32)param->cluster_id,
                   (guint32)param->node_number,
                   conn)))
  {
    *error_line= __LINE__;
    DEBUG_RETURN_INT(ret_code);
  }
  DEBUG_RETURN_INT(0);

error:
  DEBUG_PRINT(CONFIG_LEVEL,
    ("Protocol error in converting to transporter, code = %d", ret_code));
  DEBUG_RETURN_INT(ret_code);
}

/*
  MODULE: Handle Cluster Configuration Request
  --------------------------------------------

 * This is implemented through a number of subroutine levels.
 * At first there is a set of methods to handle the protocol actions which
 * is part of this piece of the protocol. These are:
 * rec_get_nodeid_req: 
 * send_get_nodeid_reply:
 * rec_get_version_req:
 * send_get_version_req:
 * rec_get_config_req:
 * send_get_config_reply:
 *
 * All of the above protocol methods are fairly simple using the standard
 * techniques used in the iClaustron protocol methods.
 *
 * The final routine is the method to get the base64-encoded configuration
 * string. This is implemented in the routine:
 * ic_get_base64_config
 * This method in turn uses a routine that gets the configuration as a large
 * of unsigned 32 bit values. This is implemented in the routine:
 * ic_get_key_value_sections_config
 *
 * This routine firsts calculates the length of the array and this is 
 * supported by the routines:
 * get_length_of_section: Calculates length of a configuration section
 * get_comm_section: Gets a communication section to calculate its length
 * ndb_mgm_str_word_len: Calculates length of ?
 * 
 * Finally the array is filled in, this is supported by the method:
 * fill_key_value_section: Fill in the key-value pairs for one section
 * This method uses the support method:
 * is_iclaustron_version
 * 
*/

static int rec_get_nodeid_req(IC_CONNECTION *conn,
                              guint64 *node_number,
                              guint64 *version_number,
                              guint64 *node_type,
                              guint64 *cluster_id);
static int send_get_nodeid_reply(IC_CONNECTION *conn, guint32 node_id);
static int rec_get_version_req(IC_CONNECTION *conn);
static int send_get_version_reply(IC_CONNECTION *conn, guint64 node_type);
static int send_get_config_reply(IC_CONNECTION *conn,
                                 gchar *config_base64_str,
                                 guint32 config_len);
static int rec_get_config_req(IC_CONNECTION *conn,
                              guint64 *version_number,
                              guint64 node_type);
static int ic_get_base64_config(IC_CLUSTER_CONFIG *clu_conf,
                                guint8 **base64_array,
                                guint32 *base64_array_len,
                                guint64 version_number);
static int ic_get_key_value_sections_config(IC_CLUSTER_CONFIG *clu_conf,
                                            guint32 **key_value_array,
                                            guint32 *key_value_array_len,
                                            guint64 version_number);
static guint32 get_length_of_section(IC_CONFIG_TYPES config_type,
                                     gchar *conf,
                                     guint64 version_number);
static IC_SOCKET_LINK_CONFIG*
get_comm_section(IC_CLUSTER_CONFIG *clu_conf,
                 IC_SOCKET_LINK_CONFIG *comm_section,
                 guint32 node1, guint32 node2);
static guint32 ndb_mgm_str_word_len(guint32 str_len);
static int fill_key_value_section(IC_CONFIG_TYPES config_type,
                                  gchar *conf,
                                  guint32 sect_id,
                                  guint32 *key_value_array,
                                  guint32 *key_value_array_len,
                                  guint64 version_number);
static gboolean is_iclaustron_version(guint64 version_number);

static int
handle_config_request(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                      IC_CONNECTION *conn,
                      IC_RC_PARAM *param)
{
  int ret_code;
  guint8 *config_base64_str;
  guint32 config_len;
  IC_RUN_CLUSTER_STATE *rcs_state= &run_obj->state;
  IC_MUTEX *state_mutex= rcs_state->protect_state;
  DEBUG_ENTRY("handle_config_request");

  if ((ret_code= rec_get_nodeid_req(conn,
                                    &param->node_number,
                                    &param->version_number,
                                    &param->node_type,
                                    &param->cluster_id)))
    goto end;
  DEBUG_INDENT_LEVEL_CHECK(2);
  ic_mutex_lock(state_mutex);
  if (rcs_state->cs_started &&
      is_cs_master(rcs_state))
  {
    ;
  }
  else if (rcs_state->cs_started &&
           !is_cs_master(rcs_state))
  {
    /* Send an error message to indicate we're not master */
    ;
  }
  else
  {
    /* Send an error message to indicate we're still in start-up phase */
    ;
  }
  ic_mutex_unlock(state_mutex);
  if (param->node_number == 0)
  {
    /* Here we need to discover which node id to use */
    param->client_nodeid= 1; /* Fake for now */
  }
  else
  {
    /* Here we ensure that the requested node id is correct */
    param->client_nodeid= param->node_number;
  }
  if ((ret_code= send_get_nodeid_reply(conn, (guint32)param->client_nodeid)) ||
      (ret_code= rec_get_version_req(conn)) ||
      (ret_code= send_get_version_reply(conn, param->node_type)) ||
      (ret_code= rec_get_config_req(conn,
                                    &param->version_number,
                                    param->node_type)) ||
      (ret_code= ic_get_base64_config(
              run_obj->config.conf_objects[param->cluster_id],
              &config_base64_str,
              &config_len,
              param->version_number)))
    goto end;
  DEBUG_PRINT(CONFIG_LEVEL,
    ("Converted configuration to a base64 representation"));
  ret_code= send_get_config_reply(conn, (gchar*)config_base64_str, config_len);
  ic_free((gchar*)config_base64_str);
end:
  if (ret_code)
  {
    DEBUG_PRINT(CONFIG_LEVEL,
      ("Error from handle_config_request, code = %u", ret_code));
  }
  DEBUG_RETURN_INT(ret_code);
}

/* Handle receive of get node id request */
static int
rec_get_nodeid_req(IC_CONNECTION *conn,
                   guint64 *node_number,
                   guint64 *version_number,
                   guint64 *node_type,
                   guint64 *cluster_id)
{
  IC_STRING process_name;
  gchar *read_buf;
  guint32 read_size;
  guint32 endian_req_len;
  guint32 state= VERSION_REQ_STATE; /* get nodeid already received */
  int ret_code;
  gchar endian_buf[32];
  DEBUG_ENTRY("rec_get_nodeid_req");

  while (!(ret_code= ic_rec_with_cr(conn, &read_buf, &read_size)))
  {
    switch (state)
    {
      case VERSION_REQ_STATE:
        if (ic_check_buf_with_int(read_buf,
                                  read_size,
                                  ic_version_str,
                                  strlen(ic_version_str),
                                  version_number))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in version request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= NODETYPE_REQ_STATE;
        break;
      case NODETYPE_REQ_STATE:
        if (ic_check_buf_with_int(read_buf,
                                  read_size,
                                  nodetype_str,
                                  strlen(nodetype_str),
                                  node_type))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in nodetype request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= NODEID_REQ_STATE;
        break;
      case NODEID_REQ_STATE:
        if (ic_check_buf_with_int(read_buf,
                                  read_size,
                                  nodeid_str,
                                  strlen(nodeid_str),
                                  node_number) ||
            (*node_number > IC_MAX_NODE_ID))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in nodeid request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= USER_REQ_STATE;
        break;
      case USER_REQ_STATE:
        if (ic_check_buf(read_buf,
                         read_size,
                         user_str,
                         strlen(user_str)))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in user request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= PASSWORD_REQ_STATE;
        break;
      case PASSWORD_REQ_STATE:
        if (ic_check_buf(read_buf,
                         read_size,
                         password_str,
                         strlen(password_str)))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in password request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= PUBLIC_KEY_REQ_STATE;
        break;
      case PUBLIC_KEY_REQ_STATE:
        if (ic_check_buf(read_buf,
                         read_size,
                         public_key_str,
                         strlen(public_key_str)))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in public key request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= ENDIAN_REQ_STATE;
        break;
      case ENDIAN_REQ_STATE:
        g_snprintf(endian_buf, 32, "%s %s", endian_str, little_endian_str);
        endian_req_len= strlen(endian_buf);
        if ((read_size < endian_req_len) ||
            (memcmp(read_buf, endian_buf, endian_req_len) != 0))
        {
          g_snprintf(endian_buf, 32, "%s %s", endian_str, big_endian_str);
          endian_req_len= strlen(endian_buf);
          if ((read_size < endian_req_len) ||
              (memcmp(read_buf, endian_buf, endian_req_len) != 0))
          {
            DEBUG_PRINT(CONFIG_LEVEL,
              ("Protocol error in endian request state"));
            PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
          }
        }
        if (!ic_is_bit_set(*version_number, IC_PROTOCOL_BIT))
        {
          state= LOG_EVENT_REQ_STATE;
        }
        else
        {
          state= NAME_REQ_STATE;
        }
        break;
      case NAME_REQ_STATE:
        if (ic_check_buf_with_string(read_buf,
                                     read_size,
                                     name_str,
                                     strlen(name_str),
                                     &process_name))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in name request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        /* Currently we make no use of process name */
        state= LOG_EVENT_REQ_STATE;
        break;
      case LOG_EVENT_REQ_STATE:
        if (ic_check_buf(read_buf,
                         read_size,
                         log_event_str,
                         strlen(log_event_str)))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in log_event request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        if (!ic_is_bit_set(*version_number, IC_PROTOCOL_BIT))
        {
          state= EMPTY_LINE_REQ_STATE;
          *cluster_id= 0;
        }
        else
        {
          state= CLUSTER_ID_REQ_STATE;
        }
        break;
      case CLUSTER_ID_REQ_STATE:
        if (ic_check_buf_with_int(read_buf,
                                  read_size,
                                  cluster_id_str,
                                  strlen(cluster_id_str),
                                  cluster_id))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in cluster id request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= EMPTY_LINE_REQ_STATE;
        break;
      case EMPTY_LINE_REQ_STATE:
        if (read_size == 0)
        {
          DEBUG_RETURN_INT(0);
        }
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Protocol error in empty line state"));
        PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        break;
      default:
        abort();
        break;
    }
  }
  DEBUG_PRINT(CONFIG_LEVEL,
    ("Error in receiving get node id request, error = %d", ret_code));
  DEBUG_RETURN_INT(ret_code);
}

/* Handle send get node id reply protocol part of get configuration */
static int
send_get_nodeid_reply(IC_CONNECTION *conn, guint32 node_id)
{
  int ret_code= 0;
  DEBUG_ENTRY("send_get_nodeid_reply");

  if (ic_send_with_cr(conn, get_nodeid_reply_str) ||
      ic_send_with_cr_with_number(conn, nodeid_str, (guint64)node_id) ||
      ic_send_with_cr(conn, result_ok_str) ||
      ic_send_empty_line(conn))
  {
    ret_code= conn->conn_op.ic_get_error_code(conn);
  }
  DEBUG_RETURN_INT(ret_code);
}

/* Handle receive get version request protocol action */
static int
rec_get_version_req(IC_CONNECTION *conn)
{
  int ret_code;
  DEBUG_ENTRY("rec_get_version_req");

  if ((ret_code= ic_rec_simple_str(conn, get_version_str)) ||
      (ret_code= ic_rec_empty_line(conn)))
  {
    ;
  }
  DEBUG_RETURN_INT(ret_code);
}

/* Handle send get version reply protocol action */
static int
send_get_version_reply(IC_CONNECTION *conn,
                       guint64 node_type)
{
  guint64 major_number= NDB_VERSION >> 8;
  guint64 minor_number= (NDB_VERSION >> 4) & 0xF;
  guint64 build_number= NDB_VERSION & 0xF;
  guint64 mysql_major_number= MYSQL_VERSION >> 8;
  guint64 mysql_minor_number= (MYSQL_VERSION >> 4) & 0xF;
  guint64 mysql_build_number= MYSQL_VERSION & 0xF;
  guint64 version_no=
    get_iclaustron_protocol_version(node_type != IC_DATA_SERVER_TYPE_PROTOCOL);
  DEBUG_ENTRY("send_get_version_reply");

  if (ic_send_with_cr(conn, version_str) ||
      ic_send_with_cr_with_number(conn, id_str, version_no) ||
      ic_send_with_cr_with_number(conn, major_str, major_number) ||
      ic_send_with_cr_with_number(conn, minor_str, minor_number) ||
      ic_send_with_cr_with_number(conn, build_str, build_number) ||
      ic_send_with_cr_two_strings(conn, string_str, MYSQL_VERSION_STRING) ||
      ic_send_with_cr_with_number(conn, mysql_major_str, mysql_major_number) ||
      ic_send_with_cr_with_number(conn, mysql_minor_str, mysql_minor_number) ||
      ic_send_with_cr_with_number(conn, mysql_build_str, mysql_build_number) ||
      ic_send_empty_line(conn))
  {
    DEBUG_RETURN_INT(conn->conn_op.ic_get_error_code(conn));
  }
  DEBUG_RETURN_INT(0);
}

/* Handle receive get configuration request protocol action */
static int
rec_get_config_req(IC_CONNECTION *conn,
                   guint64 *version_number,
                   guint64 node_type)
{
  gchar *read_buf;
  guint32 read_size;
  guint32 state= GET_CONFIG_REQ_STATE;
  guint64 read_version_num;
  guint64 read_node_type;
  int ret_code;
  DEBUG_ENTRY("rec_get_config_req");

  while (!(ret_code= ic_rec_with_cr(conn, &read_buf, &read_size)))
  {
    switch(state)
    {
      case GET_CONFIG_REQ_STATE:
        if (ic_check_buf(read_buf, read_size, get_config_str, GET_CONFIG_LEN))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in get config request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= VERSION_REQ_STATE;
        break;
      case VERSION_REQ_STATE:
        if (ic_check_buf_with_int(read_buf,
                                  read_size,
                                  ic_version_str,
                                  strlen(ic_version_str),
                                  &read_version_num) ||
            (((*version_number) != 0) && 
             ((*version_number) != read_version_num)))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in version request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        if ((*version_number) == 0)
        {
          (*version_number)= read_version_num;
        }
        state= NODETYPE_REQ_STATE;
        break;
      case NODETYPE_REQ_STATE:
        if (ic_check_buf_with_int(read_buf,
                                  read_size,
                                  nodetype_str,
                                  strlen(nodetype_str),
                                  &read_node_type) ||
            (node_type != read_node_type))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in nodetype request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= EMPTY_STATE;
        break;
      case EMPTY_STATE:
        if (read_size != 0)
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in wait empty state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        DEBUG_RETURN_INT(0);
      default:
        abort();
        break;
    }
  }
  DEBUG_PRINT(CONFIG_LEVEL,
    ("Error in receiving get config request, error = %d", ret_code));
  DEBUG_RETURN_INT(ret_code);
}

/* Handle send configuration reply protocol action */
static int
send_get_config_reply(IC_CONNECTION *conn,
                      gchar *config_base64_str,
                      guint32 config_len)
{
  gchar content_buf[2*IC_MAX_INT_STRING];
  int ret_code= 0;
  DEBUG_ENTRY("send_get_config_reply");
 
  g_snprintf(content_buf,
             sizeof(content_buf),
             "%s%u",
             content_len_str,
             config_len);
  if (ic_send_with_cr(conn, get_config_reply_str) ||
      ic_send_with_cr(conn, result_ok_str) ||
      ic_send_with_cr(conn, content_buf) ||
      ic_send_with_cr(conn, octet_stream_str) ||
      ic_send_with_cr(conn, content_encoding_str) ||
      ic_send_empty_line(conn) ||
      conn->conn_op.ic_write_connection(conn, (const void*)config_base64_str,
                                        config_len, 1) ||
      ic_send_empty_line(conn))
    ret_code= conn->conn_op.ic_get_error_code(conn);
  DEBUG_RETURN_INT(ret_code);
}

/* Get base64 encoded string to send to client */
static int
ic_get_base64_config(IC_CLUSTER_CONFIG *clu_conf,
                     guint8 **base64_array,
                     guint32 *base64_array_len,
                     guint64 version_number)
{
  guint32 *key_value_array;
  guint32 key_value_array_len= 0;
  int ret_code;
  DEBUG_ENTRY("ic_get_base64_config");

  *base64_array= 0;
  if ((ret_code= ic_get_key_value_sections_config(clu_conf, &key_value_array,
                                                  &key_value_array_len,
                                                  version_number)))
    DEBUG_RETURN_INT(ret_code);
  ret_code= ic_base64_encode(base64_array,
                             base64_array_len,
                             (const guint8*)key_value_array,
                             key_value_array_len*4);
  DEBUG_PRINT_BUF(CONFIG_LEVEL, *(gchar**)base64_array);
  ic_free(key_value_array);
  DEBUG_RETURN_INT(ret_code);
}

/*
 * This routine is used to create an array of guint32 values which
 * can be base64-encoded for distribution to any node in an
 * iClaustron grid.
 *
 * It's ok for several threads in the iClaustron Cluster Server to
 * concurrently use this routine.
 */
static int
ic_get_key_value_sections_config(IC_CLUSTER_CONFIG *clu_conf,
                                 guint32 **key_value_array,
                                 guint32 *key_value_array_len,
                                 guint64 version_number)
{
  guint32 len= 0, num_comms= 0, api_nodes= 0;
  guint32 node_sect_len, i, j, checksum, system_len, data_server_section;
  guint32 section_id, comm_meta_section, node_meta_section;
  guint32 system_meta_section, data_server_start_section;
  guint32 *loc_key_value_array;
  guint32 loc_key_value_array_len= 0;
  int ret_code;
  IC_SOCKET_LINK_CONFIG test1, *comm_section;
  DEBUG_ENTRY("ic_get_key_value_sections_config");

  /*
   * Add 2 words for verification string in beginning
   * Add 3 key-value pairs for section 0
   * Add one key-value pair for each node section
   *   - This is section 1
   */
  len+= 2;
  len+= 6;
  len+= clu_conf->num_nodes * 2;
  DEBUG_PRINT(CONFIG_LEVEL, ("1: len=%u", len));
  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    /* Add length of each node section */
    if (clu_conf->node_config[i])
    {
      node_sect_len= get_length_of_section(
                          (IC_CONFIG_TYPES)clu_conf->node_types[i],
                                           clu_conf->node_config[i],
                                           version_number);
      if (node_sect_len == 0)
      {
        DEBUG_RETURN_INT(IC_ERROR_INCONSISTENT_DATA);
      }
      len+= node_sect_len;
      DEBUG_PRINT(CONFIG_LEVEL, ("2: len=%u", len));
      if (clu_conf->node_types[i] != IC_DATA_SERVER_NODE)
      {
        api_nodes++;
      }
    }
  }
  /* Add length of each comm section */
  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    if (clu_conf->node_config[i])
    {
      for (j= i+1; j <= clu_conf->max_node_id; j++)
      {
        if (clu_conf->node_config[j])
        {
          /* iClaustron uses a fully connected cluster */
          if (clu_conf->node_types[i] == IC_DATA_SERVER_NODE ||
              clu_conf->node_types[j] == IC_DATA_SERVER_NODE ||
              ic_is_bit_set(version_number, IC_PROTOCOL_BIT))
          {
            /* We have found two nodes needing a comm section */
            comm_section= get_comm_section(clu_conf, &test1, i, j);
            len+= get_length_of_section(IC_COMM_TYPE, (gchar*)comm_section,
                                        version_number);
            num_comms++;
            DEBUG_PRINT(CONFIG_LEVEL, ("3: len=%u", len));
          }
        }
      }
    }
  }
  /* Add one key-value pair for meta section of system section */
  len+= 2;
  /* Add length of the system section */
  system_len= get_length_of_section(IC_SYSTEM_TYPE,
                                    (gchar*)&clu_conf->sys_conf,
                                    version_number);
  if (system_len == 0)
  {
    DEBUG_RETURN_INT(IC_ERROR_INCONSISTENT_DATA);
  }
  len+= system_len;
  DEBUG_PRINT(CONFIG_LEVEL, ("4: len=%u", len));
  /*
   * Add one key-value pair for each comm section
   *   - This is meta section for communication
   */
  len+= num_comms * 2;
  DEBUG_PRINT(CONFIG_LEVEL, ("5: len=%u", len));
  /* Finally add 1 word for checksum at the end */
  len+= 1;

  DEBUG_PRINT(CONFIG_LEVEL, ("6: len=%u", len));
  /*
     Allocate memory for key-value pairs, this memory is only temporary for
     this method and its caller, so memory will be freed soon again
  */

  ic_require(num_comms);

  if (!(loc_key_value_array= (guint32*)ic_calloc(4*len)))
  {
    DEBUG_RETURN_INT(IC_ERROR_MEM_ALLOC);
  }
  *key_value_array= loc_key_value_array;

  /*
    Put in verification section
  */
  memcpy((gchar*)loc_key_value_array, ver_string, 8);

  /*
    Fill Section 0
      Id 2000 specifies section 1 as a section that specifies node sections
      Id 3000 specifies section number of the section that describes the
      communication sections
  */
  section_id= 0;
  node_meta_section= 1;
  system_meta_section= 2 + api_nodes;
  comm_meta_section= 2 + system_meta_section;
  loc_key_value_array[2]= 
     g_htonl((IC_SECTION_TYPE << IC_CL_KEY_SHIFT) +
             (section_id << IC_CL_SECT_SHIFT) +
             1000);
  loc_key_value_array[3]= g_htonl(system_meta_section << IC_CL_SECT_SHIFT);
  loc_key_value_array[4]= 
     g_htonl((IC_SECTION_TYPE << IC_CL_KEY_SHIFT) +
             (section_id << IC_CL_SECT_SHIFT) +
             2000);
  loc_key_value_array[5]= g_htonl(node_meta_section << IC_CL_SECT_SHIFT);

  loc_key_value_array[6]= 
    g_htonl((IC_SECTION_TYPE << IC_CL_KEY_SHIFT) +
            (section_id << IC_CL_SECT_SHIFT) +
            3000);
  loc_key_value_array[7]= g_htonl(comm_meta_section << IC_CL_SECT_SHIFT);
  loc_key_value_array_len= 8;

  /*
    Fill Section 1
    One key-value for each section that specifies a node, starting at
    section 2 and ending at section 2+num_nodes-1. First fill in
    API nodes and then the ones for Data Server nodes.
  */
  section_id++;
  for (i= 0; i < api_nodes; i++)
  {
    loc_key_value_array[loc_key_value_array_len++]=
              g_htonl((IC_CL_INT32_TYPE << IC_CL_KEY_SHIFT) +
                      (section_id << IC_CL_SECT_SHIFT) +
                      i);
    loc_key_value_array[loc_key_value_array_len++]=
              g_htonl((2+i) << IC_CL_SECT_SHIFT);
  }
  data_server_section= comm_meta_section + num_comms + 1;
  data_server_start_section= data_server_section;
  for (i= api_nodes; i < clu_conf->num_nodes; i++)
  {
    loc_key_value_array[loc_key_value_array_len++]=
              g_htonl((IC_CL_INT32_TYPE << IC_CL_KEY_SHIFT) +
                      (section_id << IC_CL_SECT_SHIFT) +
                      i);
    loc_key_value_array[loc_key_value_array_len++]=
      g_htonl(data_server_section << IC_CL_SECT_SHIFT);
    data_server_section++;
  }
  DEBUG_PRINT(CONFIG_LEVEL,
    ("1: fill_len=%u", loc_key_value_array_len));

  /* Fill API node sections */
  section_id++;
  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    if (clu_conf->node_config[i] &&
        clu_conf->node_types[i] != IC_DATA_SERVER_NODE &&
        (ret_code= fill_key_value_section(
                         (IC_CONFIG_TYPES)clu_conf->node_types[i],
                                          clu_conf->node_config[i],
                                          section_id++,
                                          loc_key_value_array,
                                          &loc_key_value_array_len,
                                          version_number)))
      goto error;
    DEBUG_PRINT(CONFIG_LEVEL, ("2: fill_len=%u", loc_key_value_array_len));
  }

  /* Fill system meta section */
  ic_assert(system_meta_section == section_id);
  loc_key_value_array[loc_key_value_array_len++]=
                  g_htonl((IC_CL_INT32_TYPE << IC_CL_KEY_SHIFT) +
                          ((system_meta_section) << IC_CL_SECT_SHIFT));
  loc_key_value_array[loc_key_value_array_len++]=
                  g_htonl((system_meta_section + 1) << IC_CL_SECT_SHIFT);

  section_id++;
  DEBUG_PRINT(CONFIG_LEVEL, ("3: fill_len=%u", loc_key_value_array_len));
  /* Fill system section */
  if ((ret_code= fill_key_value_section(IC_SYSTEM_TYPE,
                                        (gchar*)&clu_conf->sys_conf,
                                        section_id,
                                        loc_key_value_array,
                                        &loc_key_value_array_len,
                                        version_number)))
    goto error;
  section_id++;
  DEBUG_PRINT(CONFIG_LEVEL, ("4: fill_len=%u", loc_key_value_array_len));
  /*
    Fill the communication sections, one for each pair of nodes
    that need to communicate and one meta section with pointers to
    each communication section.
  */
  ic_assert(comm_meta_section == section_id);
  for (i= 0; i < num_comms; i++)
  {
    loc_key_value_array[loc_key_value_array_len++]= g_htonl(
                                   (IC_UINT32 << IC_CL_KEY_SHIFT) +
                                   (comm_meta_section << IC_CL_SECT_SHIFT) +
                                   i);
    loc_key_value_array[loc_key_value_array_len++]= g_htonl(
                              (comm_meta_section+i+1) << IC_CL_SECT_SHIFT);
  }

  DEBUG_PRINT(CONFIG_LEVEL,
    ("5: fill_len=%u", loc_key_value_array_len));
  section_id++;
  /* Fill comm sections */
  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    if (clu_conf->node_config[i])
    {
      for (j= i+1; j <= clu_conf->max_node_id; j++)
      {
        if (clu_conf->node_config[j])
        {
          /* iClaustron uses a fully connected cluster */
          if (clu_conf->node_types[i] == IC_DATA_SERVER_NODE ||
              clu_conf->node_types[j] == IC_DATA_SERVER_NODE ||
              ic_is_bit_set(version_number, IC_PROTOCOL_BIT))
          {
            /* We have found two nodes needing a comm section */
            comm_section= get_comm_section(clu_conf, &test1, i, j);
            if ((ret_code= fill_key_value_section(IC_COMM_TYPE,
                                                  (gchar*)comm_section,
                                                  section_id++,
                                                  loc_key_value_array,
                                                  &loc_key_value_array_len,
                                                  version_number)))
              goto error;
            DEBUG_PRINT(CONFIG_LEVEL,
              ("6: fill_len=%u", loc_key_value_array_len));
          }
        }
      }
    }
  }
  /* Fill in Data Server node sections */
  ic_assert(data_server_start_section == section_id);
  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    if (clu_conf->node_config[i] &&
        clu_conf->node_types[i] == IC_DATA_SERVER_NODE &&
        (ret_code= fill_key_value_section(
                         (IC_CONFIG_TYPES)clu_conf->node_types[i],
                                          clu_conf->node_config[i],
                                          section_id++,
                                          loc_key_value_array,
                                          &loc_key_value_array_len,
                                          version_number)))
      goto error;
    DEBUG_PRINT(CONFIG_LEVEL, ("7: fill_len=%u", loc_key_value_array_len));
  }
  /* Calculate and fill out checksum */
  checksum= 0;
  for (i= 0; i < loc_key_value_array_len; i++)
  {
    checksum^= g_ntohl(loc_key_value_array[i]);
  }
  loc_key_value_array[loc_key_value_array_len++]= g_ntohl(checksum);
  DEBUG_PRINT(CONFIG_LEVEL,
    ("8: fill_len=%u", loc_key_value_array_len));
  /* Perform final set of checks */
  *key_value_array_len= loc_key_value_array_len;
  if (len == loc_key_value_array_len)
  {
    DEBUG_RETURN_INT(0);
  }

  ret_code= IC_ERROR_INCONSISTENT_DATA;
error:
  ic_free(*key_value_array);
  DEBUG_RETURN_INT(ret_code);
}

/*
 * Get communication section for calculation of its section length
 * This routine depends on that node1 < node2
 */
static IC_SOCKET_LINK_CONFIG*
get_comm_section(IC_CLUSTER_CONFIG *clu_conf,
                 IC_SOCKET_LINK_CONFIG *comm_section,
                 guint32 node1, guint32 node2)
{
  IC_SOCKET_LINK_CONFIG *local_comm_section;
  IC_DATA_SERVER_CONFIG *server_conf;
  IC_DATA_SERVER_CONFIG *client_conf;

  comm_section->first_node_id= node1;
  comm_section->second_node_id= node2;
  if ((local_comm_section= (IC_SOCKET_LINK_CONFIG*)
                           ic_hashtable_search(clu_conf->comm_hash,
                                               (void*)comm_section)))
  {
    return local_comm_section;
  }
  /* Check if we ever get here now */
  ic_require(FALSE);
  init_config_object((gchar*)comm_section, sizeof(IC_COMM_LINK_CONFIG),
                     IC_COMM_TYPE);
  comm_section->first_node_id= node1;
  comm_section->second_node_id= node2;
  if (clu_conf->node_types[node1] == IC_DATA_SERVER_NODE ||
      clu_conf->node_types[node2] != IC_DATA_SERVER_NODE)
  {
    comm_section->server_node_id= node1;
    server_conf= (IC_DATA_SERVER_CONFIG*)clu_conf->node_config[node1];
    client_conf= (IC_DATA_SERVER_CONFIG*)clu_conf->node_config[node2];
  }
  else
  {
    comm_section->server_node_id= node2;
    server_conf= (IC_DATA_SERVER_CONFIG*)clu_conf->node_config[node2];
    client_conf= (IC_DATA_SERVER_CONFIG*)clu_conf->node_config[node1];
  }
  comm_section->server_port_number= server_conf->port_number;
  comm_section->client_port_number= client_conf->port_number;
  comm_section->first_hostname= server_conf->hostname;
  comm_section->second_hostname= client_conf->hostname;
  return comm_section;
}

/* Special handling of string lengths in NDB Management Protocol */
static guint32
ndb_mgm_str_word_len(guint32 str_len)
{
  guint32 word_len;
  str_len++; /* To accomodate for final NULL */
  /* str_len++; */ /* For bug compatability with NDB MGM Protocol */
  word_len= (str_len+3)/4;
  return word_len;
}

/* Calculate length of a node, communication section */
static guint32
get_length_of_section(IC_CONFIG_TYPES config_type,
                      gchar *conf, guint64 version_number)
{
  IC_CONFIG_ENTRY *conf_entry;
  gchar **charptr;
  guint32 len= 0, i, str_len;

  for (i= 0; i < MAX_CONFIG_ID; i++)
  {
    conf_entry= &glob_conf_entry[i];
    if ((conf_entry->config_types & (1 << ((guint32)config_type))) &&
        (!conf_entry->is_not_sent) &&
        is_entry_used_in_version(conf_entry, version_number))
    {
      switch (conf_entry->data_type)
      {
        case IC_BOOLEAN:
        case IC_UINT16:
        case IC_UINT32:
          break;
        case IC_UINT64:
          len++;
          break;
        case IC_CHARPTR:
        case IC_ENUM:
        {
          charptr= (gchar**)(conf+conf_entry->offset);
          str_len= 0;
          if (*charptr)
          {
            str_len= strlen(*charptr);
          }
          len+= ndb_mgm_str_word_len(str_len);
          break;
        }
        default:
          ic_require(FALSE);
          break;
      }
      len+= 2;
    }
  }
  len+= 2; /* One key-value pair for node type */
  len+= 2; /* One key-value pair for parent node id */
  return len;
}

/* Fill in key-value pairs for a node or communication section */
static int
fill_key_value_section(IC_CONFIG_TYPES config_type,
                       gchar *conf,
                       guint32 sect_id,
                       guint32 *key_value_array,
                       guint32 *key_value_array_len,
                       guint64 version_number)
{
  IC_CONFIG_ENTRY *conf_entry;
  guint32 len= 0, i, key, config_id, value, data_type, str_len;
  guint32 *assign_array;
  gchar **charptr;
  guint32 loc_key_value_array_len= *key_value_array_len;
  DEBUG_ENTRY("fill_key_value_section");

  for (i= 0; i < MAX_CONFIG_ID; i++)
  {
    conf_entry= &glob_conf_entry[i];
    if ((conf_entry->config_types & (1 << ((guint32)config_type))) &&
        (!conf_entry->is_not_sent) &&
        is_entry_used_in_version(conf_entry, version_number))
    {
      assign_array= &key_value_array[loc_key_value_array_len];
      switch (conf_entry->data_type)
      {
        case IC_BOOLEAN:
        case IC_CHAR:
        {
          guint8 *entry= (guint8*)(conf+conf_entry->offset);
          value= (guint32)*entry;
          data_type= IC_CL_INT32_TYPE;
          break;
        }
        case IC_UINT16:
        {
          guint16 *entry= (guint16*)(conf+conf_entry->offset);
          value= (guint32)*entry;
          data_type= IC_CL_INT32_TYPE;
          break;
        }
        case IC_UINT32:
        {
          guint32 *entry= (guint32*)(conf+conf_entry->offset);
          value= (guint32)*entry;
          data_type= IC_CL_INT32_TYPE;
          break;
        }
        case IC_UINT64:
        {
          guint64 *entry= (guint64*)(conf+conf_entry->offset);
          value= *entry & 0xFFFFFFFF;
          assign_array[2]= g_htonl(value);
          value= (guint32)((guint64)(*entry >> 32));
          loc_key_value_array_len++;
          data_type= IC_CL_INT64_TYPE;
          DEBUG_PRINT(CONFIG_LEVEL,
                      ("64-bit value (low part) = %u, high part in next line",
                       g_ntohl(assign_array[2])));
          break;
        }
        case IC_CHARPTR:
        case IC_ENUM:
        {
          charptr= (gchar**)(conf+conf_entry->offset);
          str_len= 0;
          if (*charptr)
          {
            str_len= strlen(*charptr);
          }
          value= str_len + 1; /* Reported length includes NULL byte */
          /* 
             Adjust to number of words with one word removed and
             an extra null byte calculated for
           */
          len= ndb_mgm_str_word_len(str_len);
          /* We don't need to copy null byte since we initialised to 0 */
          if (str_len)
          {
            memcpy((gchar*)&assign_array[2],
                   *charptr,
                   str_len);
          }
          DEBUG_PRINT(CONFIG_LEVEL,
                      ("String value = %s, str_len= %u",
                       (gchar*)&assign_array[2], str_len));
          loc_key_value_array_len+= len;
          data_type= IC_CL_CHAR_TYPE;
          break;
        }
        default:
        {
          DEBUG_RETURN_INT(IC_ERROR_INCONSISTENT_DATA);
        }
      }
      /*
         Assign the key consisting of:
         1) Data Type
         2) Section id
         3) Config id
       */
      config_id= map_inx_to_config_id[i];
      key= (data_type << IC_CL_KEY_SHIFT) +
           (sect_id << IC_CL_SECT_SHIFT) +
           (config_id);
      assign_array[0]= g_htonl(key);
      assign_array[1]= g_htonl(value);
      DEBUG_PRINT(CONFIG_LEVEL,
                  ("sectid = %u, data_type = %u, config_id = %u, value = %u",
                   sect_id, data_type, config_id, value));
      loc_key_value_array_len+= 2;
    }
  }
  /* Add node type for all sections */
  assign_array= &key_value_array[loc_key_value_array_len];
  config_id= IC_NODE_TYPE;
  key= (IC_CL_INT32_TYPE << IC_CL_KEY_SHIFT) +
       (sect_id << IC_CL_SECT_SHIFT) +
       config_id;
  switch (config_type)
  {
    case IC_COMM_TYPE:
      value= 0;
      break;
    case IC_DATA_SERVER_TYPE:
    case IC_CLUSTER_SERVER_TYPE:
    /*
      config_type value is one higher than the value used in the
      protocol.
     */
      value= ((guint32)config_type - 1);
      break;
    default:
      if (!is_iclaustron_version(version_number))
      {
        value= IC_CLIENT_TYPE;
      }
      else
      {
        value= ((guint32)config_type - 1);
      }
      break;
  }
  DEBUG_PRINT(CONFIG_LEVEL,
              ("sectid = %u, config_id = %u, value = %u",
                sect_id, config_id, value));
  assign_array[0]= g_htonl(key);
  assign_array[1]= g_htonl(value);
  loc_key_value_array_len+= 2;

  /* Add parent id == 0 for all sections */
  assign_array= &key_value_array[loc_key_value_array_len];
  config_id= IC_PARENT_ID;
  key= (IC_CL_INT32_TYPE << IC_CL_KEY_SHIFT) +
       (sect_id << IC_CL_SECT_SHIFT) +
       config_id;
  value= (guint32)0;
  DEBUG_PRINT(CONFIG_LEVEL,
              ("sectid = %u, config_id = %u, value = %u",
                sect_id, config_id, value));
  assign_array[0]= g_htonl(key);
  assign_array[1]= g_htonl(value);
  loc_key_value_array_len+= 2;

  *key_value_array_len= loc_key_value_array_len;
  DEBUG_RETURN_INT(0);
}

static gboolean
is_iclaustron_version(guint64 version_number)
{
  if (ic_is_bit_set(version_number, IC_PROTOCOL_BIT))
  {
    return TRUE;
  }
  return FALSE;
}
