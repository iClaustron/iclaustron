/* Copyright (C) 2009-2010 iClaustron AB

   This program is free software; you can redistribute it and/or modify
   it under the terms of the GNU General Public License as published by
   the Free Software Foundation; version 2 of the License.

   This program is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
   GNU General Public License for more details.

   You should have received a copy of the GNU General Public License
   along with this program; if not, write to the Free Software
   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA */

/*
  MODULE: Support functions for Cluster Server
  --------------------------------------------

  get_my_cluster_info: Get my info cluster object
  get_cluster_info: Get any info cluster object
  get_master_node_id: Get node id of master Cluster Server
  ic_cs_master: Are we master Cluster Server
  get_count_cluster_servers: Get current count of Cluster Servers
  get_any_cluster_config: Get any Cluster Config to get config of
    Cluster Servers.
  find_cs_index: Find Cluster Server index in cs_servers array
  get_socket_link_config: Get Socket communication data structure for
    communication between two nodes in a specified cluster.
*/

static IC_INFO_CLUSTER_SERVER*
get_my_cluster_info(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  return &run_obj->state.cs_servers[0];
}

static IC_INFO_CLUSTER_SERVER*
get_cluster_info(IC_INT_RUN_CLUSTER_SERVER *run_obj, guint32 cs_index)
{
  return &run_obj->state.cs_servers[cs_index];
}

static guint32
get_master_node_id(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  return run_obj->state.cs_servers[run_obj->state.master_cs_index].node_id;
}

static gboolean
is_cs_master(IC_RUN_CLUSTER_STATE *rc_state)
{
  return (0 == rc_state->master_cs_index);
}

static guint32 get_count_cluster_servers(IC_CLUSTER_CONFIG *clu_conf)
{
  guint32 count= 0;
  guint32 i;

  /* Count number of Cluster Servers */
  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    if (clu_conf->node_types[i] == IC_CLUSTER_SERVER_NODE)
      count++;
  }
  g_assert(count != 0 && count <= IC_MAX_CLUSTER_SERVERS);
  return count;
}

/*
  Find a cluster configuration, any will do since Cluster Servers have
  the same host, port and node id in all clusters.
*/
static IC_CLUSTER_CONFIG*
get_any_cluster_config(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 i= 0;
  IC_CLUSTER_CONFIG *clu_conf;

  for (i= 0; i <= run_obj->max_cluster_id; i++)
  {
    if (!(clu_conf= run_obj->conf_objects[i]))
      continue;
  }
  g_assert(clu_conf);
  return clu_conf;
}

static int
find_cs_index(IC_INT_RUN_CLUSTER_SERVER *run_obj,
              guint32 node_id,
              guint32 *index)
{
  guint32 i;

  for (i= 0; i < run_obj->num_cluster_servers; i++)
  {
    if (get_cluster_info(run_obj, i)->node_id == node_id)
    {
      *index= i;
      return 0;
    }
  }
  return IC_PROTOCOL_ERROR;
}

static int
get_socket_link_config(guint32 node1_id,
                       guint32 node2_id,
                       IC_SOCKET_LINK_CONFIG **comm_section,
                       IC_CLUSTER_CONFIG *clu_conf)
{
  int error;
  IC_SOCKET_LINK_CONFIG search_comm_section;
  DEBUG_ENTRY("get_socket_link_config");

  /* Get the dynamic port number of the connection */
  search_comm_section.first_node_id= node1_id;
  search_comm_section.second_node_id= node2_id;
  error= IC_ERROR_SET_CONNECTION_PARAMETER_WRONG_NODES;
  if (!(*comm_section= (IC_SOCKET_LINK_CONFIG*)
       ic_hashtable_search(clu_conf->comm_hash, (void*)&search_comm_section)))
  {
    DEBUG_RETURN(error);
  }
  DEBUG_RETURN(0);
}

/*
  MODULE: Protect Cluster Server State
  ------------------------------------
*/
static gboolean rc_start_update(IC_RUN_CLUSTER_STATE *rc_state);
static void rc_stop_update(IC_RUN_CLUSTER_STATE *rc_state);

static gboolean
rc_start_update(IC_RUN_CLUSTER_STATE *rc_state)
{
  gboolean is_master;

  g_mutex_lock(rc_state->protect_state);
  is_master= is_cs_master(rc_state);
  /*
    We are the master of the Cluster Servers which means all updates
    need to start in our Cluster Server.
  */
  while (rc_state->update_state)
  {
    /*
      Someone is already performing an update, we need to wait until
      this update is completed, the Cluster Servers can only handle
      one update at a time.
    */
    rc_state->update_waiters++;
    g_cond_wait(rc_state->update_cond, rc_state->protect_state);
    g_assert(!rc_state->update_state);
  }
  rc_state->update_state= TRUE;
  g_mutex_unlock(rc_state->protect_state);
  return is_master;
}

static void
rc_stop_update(IC_RUN_CLUSTER_STATE *rc_state)
{
  g_mutex_lock(rc_state->protect_state);
  g_assert(rc_state->update_state);
  if (rc_state->update_waiters)
  {
    rc_state->update_waiters--;
    g_cond_signal(rc_state->update_cond);
  }
  rc_state->update_state= FALSE;
  g_mutex_unlock(rc_state->protect_state);
}

/*
  MODULE: Stop Cluster Server
  ---------------------------
  This module is a support module to the Run Cluster Server that implements
  the to stop the Cluster Server by unlocking the configuration and
  communicating the close down to the other Cluster Servers in the grid.
  It implements the stop_cluster_server method in the Run Cluster Server
  interface.
*/
static int unlock_cv_file(IC_INT_RUN_CLUSTER_SERVER *run_obj);

/* Implements the ic_stop_cluster_server method */
static int
stop_cluster_server(IC_RUN_CLUSTER_SERVER *ext_run_obj)
{
  IC_INT_RUN_CLUSTER_SERVER *run_obj= (IC_INT_RUN_CLUSTER_SERVER*)ext_run_obj;
  DEBUG_ENTRY("stop_cluster_server");

  DEBUG_RETURN(unlock_cv_file(run_obj));
}

static int
unlock_cv_file(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  int error= 0;

  if (run_obj->locked_configuration)
  {
    error= write_config_version_file(run_obj->config_dir,
                       get_my_cluster_info(run_obj)->config_version_number,
                              CONFIG_STATE_IDLE,
                              (guint32)0);
  }
  return error;
}

/*
  MODULE: Start Cluster Server
  ----------------------------
  This module is a support module to the Run Cluster Server that implements
  the method to start the Cluster Server by reading the configuration from
  disk and synchronizing with any Cluster Servers already up and running.

  Cluster Server Start Options:
  -----------------------------

  The very first start of a Cluster Server always expects to find the same
  configuration file in all Cluster Server nodes. The idea is that the
  user will write those configuration file in a tool that assists them in
  writing the needed configuration files. Then the cluster client can
  read those files and connect to the process controller on each respective
  node. The process controller will write the files into the correct place
  of the files.

  The process to bootstrap a cluster is the following:

  Installation prerequisites:
  ---------------------------
  1) Install the iClaustron binaries into all machines where the iClaustron
     Grid will be executing.
  2) Start process controller on all machines (some machines might even need
     multiple process controller, one for each port they listen to).
  3) Ensure that all ports to be used by process controllers, Cluster Servers,
     Cluster Managers, SQL Servers, replication servers and data servers are
     not blocked by any firewall application.

  Configuration building
  ----------------------
  4) Start the configuration builder tool in an appropriate directory
  5) Create an initial configuration
  6) The tool writes the configuration files into the directory where the
     tool executes.

  Distribute configuration files
  ------------------------------
  7) Issue the command to distribute the configuration files to the
     appropriate places using the already started process controllers
     to ensure there is an allowed path to distribute the files. This
     means distributing all configuration files to machines running
     the Cluster Servers and the grid configuration files to all machines.

  Start the Cluster Servers and Managers
  --------------------------------------
  8) Issue the command in the configuration builder tool to start the
     Cluster Servers for the first time.
  9) Issue the command in the configuration builder tool to start the
     Cluster Managers for the first time.

  iClaustron expects that Cluster Servers and Cluster Managers for a
  specific Grid is kept up and running all the time. The reason for this is
  that the configuration builder tool only have access to up-to-date
  Grid configuration at initial start. After initial start the configuration
  can change in every way possible. Thus it is harder to restart the Grid
  if Cluster Servers and/or Cluster Managers are no longer available.

  To restart cluster servers and cluster managers if all cluster servers
  and/or all cluster managers are stopped we use a special tool that can
  start these up using the process controller. The user of this tool must
  however have knowledge of the current placement of the Cluster Servers. 

  Start of a Cluster Server
  -------------------------
  When starting the Cluster Server one will read the configuration version
  file, the common grid configuration file, the cluster configuration file for
  this version and the configuration files for each cluster in this version.

  After reading the local configuration files a node will attempt to connect
  to any other Cluster Servers. The node will wait until a quorum has been
  formed before the start is completed.

  If connect is successful and the version read from the connected server is
  equal to our own read version, then we will fetch configuration from the
  server and verify its correctness. If it's unequal then we'll fetch
  configuration from the server connected, verify the received configuration,
  install the new configuration, update the configuration version file,
  remove the old configuration version, update the configuration version
  file again to indicate the old version is removed.

  If connect was unsuccessful and we had local configuration files then we'll
  start-up our server connection. After that we'll in parallel make more
  attempts to connect as clients to the other Cluster Servers while we're
  also allowing other nodes to connect to us.

  If no other Cluster Server is heard from then we'll start replying to
  any requests from other nodes, also other nodes than Cluster Servers.
  If a Cluster Server contacted us through the server interface while we
  were unsuccessful in contacting this node through the client interface,
  then we'll synchronize with this Cluster Server. If we received a
  connection in parallel with managing to connect to the same Cluster
  Server we'll synchronize with this Cluster Server.

  The names of the configuration files is fixed, it is always config.ini for
  the cluster configuration file, and it will be config.version for the file
  that contains the version of the current configuration. If the version is
  3 then the files created will be called config.ini.3 and the configuration
  files of the cluster will always be called the name of the cluster + .ini.
  Thus for a cluster called kalle it will kalle.ini and versioned it will be
  kalle.ini.3.

  The only parameter thus needed for the Cluster Server is which directory
  those files are stored in. The remaining information is always the same
  or provided in configuration files.

  The implementation starts by locking the configuration and retrieving the
  configuration version number by using the ic_load_config_version from
  another module.

  The next step is to load the configuration files from disk by using the
  method:
    load_local_config
  This method uses the ic_load_cluster_config_from_file to retrieve the
  grid configuration, this resides in another module. Then it uses the
    load_config_files
   method to load each cluster configuration, this method loops over all
   the clusters and loads each cluster configuration using the method
   ic_load_config_server_from files which also resides in another module.

   In a bootstrap situation it does also write version 1 of the configuration
   using the method ic_write_full_config_to_disk from another module.
*/

static int set_up_apic(IC_INT_RUN_CLUSTER_SERVER *run_obj);
static void set_up_run_obj(IC_INT_RUN_CLUSTER_SERVER *run_obj);
static int load_local_config(IC_INT_RUN_CLUSTER_SERVER *run_obj);
static int load_config_files(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                             IC_CLUSTER_CONNECT_INFO **clu_infos);
static int start_connect_other_cluster_servers(
                             IC_INT_RUN_CLUSTER_SERVER *run_obj);
static gpointer start_cs_func(gpointer data);

/*
  Set up the run cluster server object with initial data based on
  cluster configurations.
*/
static void
set_up_run_obj_master(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 i;
  guint32 used_index;
  guint32 cs_index= 0;
  IC_CLUSTER_SERVER_CONFIG *cs_conf;
  IC_CLUSTER_CONFIG *clu_conf= get_any_cluster_config(run_obj);

  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    if (clu_conf->node_types[i] == IC_CLUSTER_SERVER_NODE)
    {
      cs_conf= (IC_CLUSTER_SERVER_CONFIG*)clu_conf->node_config[i];
      if (cs_conf->node_id == run_obj->cs_nodeid)
        used_index= 0;
      else
        used_index= ++cs_index;
      get_cluster_info(run_obj, used_index)->master_index=
        IC_MAX_CLUSTER_SERVERS;
    }
  }
  run_obj->state.master_cs_index= IC_MAX_CLUSTER_SERVERS;
}

static void
set_up_run_obj(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 cs_index= 0;
  guint32 used_index;
  guint32 i;
  IC_CLUSTER_SERVER_CONFIG *cs_conf;
  IC_CLUSTER_CONFIG *clu_conf= get_any_cluster_config(run_obj);

  run_obj->num_cluster_servers= get_count_cluster_servers(clu_conf);
  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    if (clu_conf->node_types[i] == IC_CLUSTER_SERVER_NODE)
    {
      cs_conf= (IC_CLUSTER_SERVER_CONFIG*)clu_conf->node_config[i];
      if (cs_conf->node_id == run_obj->cs_nodeid)
        used_index= 0;
      else
        used_index= ++cs_index;
      get_cluster_info(run_obj, used_index)->node_id= cs_conf->node_id;
    }
  }
}

static void
free_apic_from_run_cluster(IC_API_CONFIG_SERVER *ext_apic)
{
  guint32 i;
  IC_INT_API_CONFIG_SERVER *apic= (IC_INT_API_CONFIG_SERVER*)ext_apic;

  for (i= 0; i < IC_MAX_CLUSTER_ID; i++)
  {
    if (apic->conf_objects[i])
    {
      ic_hashtable_destroy(apic->conf_objects[i]->comm_hash);
      apic->conf_objects[i]= NULL;
    }
  }
}

/*
  Ensure that also run cluster server has API config object and
  set it up correctly.
*/
static int
set_up_apic(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  IC_INT_API_CONFIG_SERVER *apic= (IC_INT_API_CONFIG_SERVER*)run_obj->apic;
  IC_CLUSTER_CONFIG *clu_conf;
  IC_CLUSTER_SERVER_CONFIG *cs_conf;
  guint32 i, cs_index, dummy, num_cluster_servers, used_index;
  gchar buf[32], *buf_ptr, *port_number_str;
  int error;
  gboolean found_cs_nodeid= FALSE;
  IC_MEMORY_CONTAINER *mc_ptr= run_obj->conf_mc_ptr;
  IC_API_CLUSTER_CONNECTION *cluster_conn= &apic->cluster_conn;

  set_up_apic_methods(apic);
  apic->api_op.ic_get_config= null_get_cs_config;
  apic->api_op.ic_free_config= free_apic_from_run_cluster;

  apic->max_cluster_id= run_obj->max_cluster_id;
  apic->use_ic_cs= TRUE;
  apic->conf_objects= (IC_CLUSTER_CONFIG**)run_obj->conf_objects;
  apic->mc_ptr= run_obj->conf_mc_ptr;

  clu_conf= get_any_cluster_config(run_obj);
  cluster_conn->num_cluster_servers= get_count_cluster_servers(clu_conf);
  error= IC_ERROR_MEM_ALLOC;
  num_cluster_servers= cluster_conn->num_cluster_servers;
  if (!((cluster_conn->cluster_server_ips= (gchar**)
          mc_ptr->mc_ops.ic_mc_calloc(mc_ptr,
                          sizeof(gchar*)*num_cluster_servers)) &&
        (cluster_conn->cluster_server_ports= (gchar**)
          mc_ptr->mc_ops.ic_mc_calloc(mc_ptr,
                          sizeof(gchar*)*num_cluster_servers)) &&
        (cluster_conn->cs_nodeid= (guint32*)
          mc_ptr->mc_ops.ic_mc_calloc(mc_ptr,
                          sizeof(guint32)*num_cluster_servers))))
    goto error;
  cs_index= 0;
  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    if (clu_conf->node_types[i] == IC_CLUSTER_SERVER_NODE)
    {
      /* We found a Cluster Server Node, record it in cluster_conn */
      cs_conf= (IC_CLUSTER_SERVER_CONFIG*)clu_conf->node_config[i];
      if (cs_conf->node_id == run_obj->cs_nodeid)
      {
        used_index= 0;
        found_cs_nodeid= TRUE;
      }
      else
        used_index= ++cs_index;
      cluster_conn->cluster_server_ips[used_index]= cs_conf->hostname;
      buf_ptr= ic_guint64_str((guint64)cs_conf->cluster_server_port_number,
                              buf,
                              &dummy);
      if ((error= ic_mc_chardup(mc_ptr, &port_number_str, buf_ptr)))
        goto error;
      cluster_conn->cluster_server_ports[used_index]= port_number_str;
      cluster_conn->cs_nodeid[used_index]= cs_conf->node_id;
    }
  }
  if (!found_cs_nodeid)
    return IC_ERROR_NO_SUCH_CLUSTER_SERVER_NODEID;
  return 0;
error:
  return error;
}

typedef struct thread_data
{
  IC_INT_RUN_CLUSTER_SERVER *run_obj;
  guint32 cs_index;
} THREAD_DATA;

static int
load_local_config(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  int error= 1;
  IC_CLUSTER_CONNECT_INFO **clu_infos;
  IC_INFO_CLUSTER_SERVER *info_cs= get_my_cluster_info(run_obj);

  if (!(clu_infos= ic_load_cluster_config_from_file(run_obj->config_dir,
                                info_cs->config_version_number,
                                run_obj->conf_mc_ptr,
                                &run_obj->state.err_obj)))
    return error;
  run_obj->clu_infos= clu_infos;
  if ((error= load_config_files(run_obj, clu_infos)))
    return error;
  if (info_cs->config_version_number == 0)
  {
    if ((error= ic_write_full_config_to_disk(run_obj->config_dir,
                              &info_cs->config_version_number,
                              clu_infos,
                              run_obj->conf_objects)))
      return error;
  }
  return 0;
}

static int
load_config_files(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                  IC_CLUSTER_CONNECT_INFO **clu_infos)
{
  IC_CLUSTER_CONNECT_INFO *clu_info;
  IC_CLUSTER_CONFIG *cluster, *grid_cluster;
  IC_MEMORY_CONTAINER *mc_ptr= run_obj->conf_mc_ptr;
  IC_STRING file_name_string;
  gchar file_name[IC_MAX_FILE_NAME_SIZE];
  IC_INFO_CLUSTER_SERVER *info_cs= get_my_cluster_info(run_obj);

  ic_create_config_file_name(&file_name_string,
                             file_name,
                             run_obj->config_dir,
                             &ic_grid_common_config_string,
                             info_cs->config_version_number);

  if (!(grid_cluster= ic_load_grid_common_config_server_from_file(file_name,
                                            mc_ptr,
                                            *clu_infos,
                                            &run_obj->state.err_obj)))
    return run_obj->state.err_obj.err_num;

  while (*clu_infos)
  {
    clu_info= *clu_infos;
    clu_infos++;
    ic_create_config_file_name(&file_name_string,
                               file_name,
                               run_obj->config_dir,
                               &clu_info->cluster_name,
                               info_cs->config_version_number);
    /*
      We have now formed the filename of the configuration of this
      cluster. It's now time to open the configuration file and
      convert it into a IC_CLUSTER_CONFIG struct.
    */
    if (!(cluster= ic_load_config_server_from_files(run_obj,
                                              file_name,
                                              grid_cluster,
                                              clu_info)))
      return run_obj->state.err_obj.err_num;

    /*
      Copy information from cluster configuration file which isn't set in
      the configuration and ensure it's allocated on the proper memory
      container.
    */
    if (ic_mc_strdup(mc_ptr, &cluster->clu_info.cluster_name,
                     &clu_info->cluster_name))
      return IC_ERROR_MEM_ALLOC;
    if (ic_mc_strdup(mc_ptr, &cluster->clu_info.password,
                     &clu_info->password))
      return IC_ERROR_MEM_ALLOC;

    cluster->clu_info.cluster_id= clu_info->cluster_id;
    cluster->my_node_id= run_obj->cs_nodeid;

    /* Update System section for handling NDB Management Protocol */
    cluster->sys_conf.system_name= cluster->clu_info.cluster_name.str;
    cluster->sys_conf.system_configuration_number=
      (guint32)info_cs->config_version_number;
    cluster->sys_conf.system_primary_cs_node= get_master_node_id(run_obj);

    if (run_obj->conf_objects[clu_info->cluster_id])
    {
      ic_hashtable_destroy(cluster->comm_hash);
      return IC_ERROR_CONFLICTING_CLUSTER_IDS;
    }
    run_obj->conf_objects[clu_info->cluster_id]= cluster;
    run_obj->max_cluster_id= IC_MAX(run_obj->max_cluster_id,
                                    clu_info->cluster_id);
    run_obj->num_clusters++;
  }
  return 0;
}

static int
start_connect_other_cluster_servers(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  IC_THREADPOOL_STATE *tp_state= run_obj->tp_state;
  guint32 i;
  int error;
  guint32 num_threads_started= 0;
  THREAD_DATA thread_data;
  IC_INT_API_CONFIG_SERVER *apic= run_obj->apic;
  gboolean break_flag;
  gboolean first_loop= TRUE;
  IC_INFO_CLUSTER_SERVER *info_cs;
  DEBUG_ENTRY("start_connect_other_cluster_servers");

  do
  {
    break_flag= FALSE;
    g_mutex_lock(run_obj->state.protect_state);
    for (i= 0; i < apic->cluster_conn.num_cluster_servers; i++)
    {
      info_cs= get_cluster_info(run_obj, i);
      if (apic->cluster_conn.cs_nodeid[i] == run_obj->cs_nodeid ||
          info_cs->is_start_thread_active)
        continue;
      num_threads_started++;
      thread_data.run_obj= run_obj;
      thread_data.cs_index= i;
      info_cs->is_start_thread_active= TRUE;
      DEBUG_PRINT(CONFIG_LEVEL, ("Start new thread in start_cs_func"));
      if ((error= tp_state->tp_ops.ic_threadpool_start_thread(
                                    tp_state,
                                    &info_cs->thread_id,
                                    start_cs_func,
                                    &thread_data,
                                    IC_MEDIUM_STACK_SIZE,
                                    TRUE)))
        goto error;
      g_mutex_unlock(run_obj->state.protect_state);
      tp_state->tp_ops.ic_threadpool_run_thread(
                                  tp_state,
                                  info_cs->thread_id);
      g_mutex_lock(run_obj->state.protect_state);
    }
    g_mutex_unlock(run_obj->state.protect_state);
    if (num_threads_started == 0 && first_loop)
    {
      /*
        Startup is completed, we're the only Cluster Server, record state
        and proceed.
      */
      run_obj->state.master_cs_index= 0;
      run_obj->state.cs_started= TRUE;
      break;
    }
    else
    {
      g_mutex_lock(run_obj->state.protect_state);
      do
      {
        DEBUG_PRINT(CONFIG_LEVEL,
                    ("Wait until threads have completed startup phase"));
        g_cond_wait(run_obj->state.start_cond, run_obj->state.protect_state);
        if (!run_obj->state.cs_started && !run_obj->state.cs_starting)
        {
          DEBUG_PRINT(CONFIG_LEVEL, ("Start failed, we need to retry"));
          break;
        }
        if (run_obj->state.cs_started)
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Start successful, synch done in run part if needed"));
          break_flag= TRUE;
          break;
        }
      } while (1);
      g_mutex_unlock(run_obj->state.protect_state);
      if (break_flag)
        break;
      first_loop= FALSE;
    }
  } while (1);
  DEBUG_RETURN(0);
error:
  DEBUG_RETURN(error);
}

/* Used by Run Cluster Server module to read configuration from disk. */
static int
read_disk_config(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  int error;
  DEBUG_ENTRY("read_disk_config");

  if ((error= load_local_config(run_obj)))
    goto error;
  if ((error= set_up_apic(run_obj)))
    goto error;
  set_up_run_obj(run_obj);
  DEBUG_RETURN(0);
error:
  DEBUG_RETURN(error);
}

/*
  This method is used by the Run Cluster Server module to reread disk
  configuration after a successful update of the configuration and in
  start cases where the original cluster configuration proved to be
  out-of-date or even that our latest version were never committed.
*/
static int
reread_disk_config(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  IC_API_CONFIG_SERVER *apic;
  IC_MEMORY_CONTAINER *mc_ptr;
  int error;
  DEBUG_ENTRY("reread_disk_config");

  apic= (IC_API_CONFIG_SERVER*)run_obj->apic;
  run_obj->apic= NULL;
  apic->api_op.ic_free_config(apic); /* Free old apic version */
  mc_ptr= run_obj->conf_mc_ptr;
  /*
    Free memory but keep memory container, all memory allocated is
    freed, but the actual memory container is still there ready to
    use for the next configuration object we retrieve.
  */
  mc_ptr->mc_ops.ic_mc_reset(mc_ptr);
  if ((error= read_disk_config(run_obj)))
  {
    ic_print_error(error);
    DEBUG_RETURN(error);
  }
  DEBUG_RETURN(0);
}

/* Implements the start_cluster_server method */
static int
start_cluster_server(IC_RUN_CLUSTER_SERVER *ext_run_obj)
{
  IC_INT_RUN_CLUSTER_SERVER *run_obj= (IC_INT_RUN_CLUSTER_SERVER*)ext_run_obj;
  int error;
  IC_INFO_CLUSTER_SERVER *info_cs= get_my_cluster_info(run_obj);
  DEBUG_ENTRY("start_cluster_server");

  /* Try to lock the configuration and get configuration version */
  if ((error= ic_load_config_version(run_obj->config_dir,
                                     run_obj->process_name,
                                     &info_cs->config_version_number,
                                     &info_cs->state,
                                     &info_cs->pid)))
    goto error;
  /* Read configuration from disk configuration */
  if ((error= read_disk_config(run_obj)))
    goto error;

  set_up_run_obj_master(run_obj);

  /*
    Before we start-up our server connection to listen to incoming events
    we first create some socket connections to connect to our fellow
    Cluster Servers. In the start-up phase this is necessary to handle
    synchronisation which Cluster Server becomes the master and who is to
    deliver the current configuration state to the other Cluster Servers.

    After the start-up phase these connections are maintained in an open
    state to ensure we can communicate to the other Cluster Servers at all
    times for changes of the configuration. If we don't manage to set-up
    connections to a certain Cluster Server in the start-up phase we'll
    close this client connection and wait for it to connect to our server
    connection.

    We'll connect in separate threads, this is to ensure that we can also
    accept connections from other starting Cluster Server in the case where
    there are more than one Cluster Server that starts up in parallel.

    Thus we will have one thread per other cluster server in the startup
    phase plus this thread that is ready to start threads when cluster
    servers connect to it.

    If another node connects in this phase it will get an error message that
    the Cluster Server is still starting up.
  */
  if ((error= start_connect_other_cluster_servers(run_obj)))
    goto error;

  DEBUG_RETURN(0);

error:
  unlock_cv_file(run_obj);
  DEBUG_RETURN(error);
}

/*
  Protocol between Cluster Servers to start up.

  The first step is to connect to all Cluster Servers. We will wait until
  at least a quorum of Cluster Servers have connected before proceeding
  with the start of the Cluster Server.

  When a quorum of cluster servers have started up we will start the
  Cluster Server start protocol. Whoever of the two nodes which became
  server part in the setup will start by sending:

  start cluster server<CR>
  nodeid: node_number<CR>
  version: version_number<CR>
  status: status_code<CR>
  pid: pid_number<CR>
  <CR>

  In response to this message the receiving node sends:

  start cluster server reply<CR>
  nodeid: node_number<CR>
  version: version_number<CR>
  pid: pid_number<CR>
  master index size: size_number<CR>
  nodeid: node_number_of_master<CR>
  nodeid: first_slave_node_number<CR>
  nodeid: second_slave_node_number<CR>
  nodeid: third_slave_node_number<CR>
  <CR>

  If the node accepts the list of master information it sends:
  
  start cluster server ok<CR>
  <CR>

  Otherwise it sends the list which it think is the proper list of master
  nodes. They can differ if the node which became the server part was
  starting whereas the other node was already started and a running
  Cluster Server. This message is the same "start cluster server reply"
  message as above with a new nodeid list. In response to the the same
  "start cluster server ok" message is sent.

  At this point the starting node has information about its own need of
  synchronisation with another Cluster Server.

  The starting cluster server waits for replies from other Cluster Servers,
  it also waits for similar messages from other starting Cluster Servers.
  It waits for a maximum of 10 seconds before it proceeds.

  An already started Cluster Server responds to this message with the
  following reply if the Cluster Server is the current master.

  start cluster server master reply<CR>
  nodeid: node_number<CR>
  version: version_number<CR>
  status: status_code<CR>
  pid: pid_number<CR>
  master node: node_number_of_master<CR>
  slave node: first_slave_node_number<CR>
  slave node: second_slave_node_number<CR>
  slave node: third_slave_node_number<CR>

  Given that the maximum number of Cluster Servers are 4, it isn't possible
  to get more than 3 slave nodes in the list. When this message arrives, the
  new Cluster Server will immediately know its place in the set of Cluster
  Servers. The new node starting is immediately placed into the list of
  slave nodes. This message is actually sent to all Cluster Servers to ensure
  the information is in all Cluster Servers to ensure that all Cluster Servers
  will choose the new master in the same manner.

  To confirm the reception of this message all nodes should send the following
  message.

  start cluster server reply<CR>
  nodeid: node_number<CR>
  version: version_number<CR>
  pid: pid_number<CR>
  master index size: size_number<CR>
  nodeid: node_number_of_master<CR>
  nodeid: first_slave_node_number<CR>
  nodeid: second_slave_node_number<CR>
  nodeid: third_slave_node_number<CR>

  In the case where no master was assigned before the Cluster Server started,
  the Cluster Server will collect all messages received for 10 seconds. These
  messages should be other "start cluster server"-messages from other
  starting Cluster Servers. After 10 seconds each starting node will make a
  decision if they are to be the master. They will choose this path if they
  are the Cluster Server starting with lowest node number.

  If we receive another start message after selecting to become master we
  know that this node will wait for 10 seconds before attempting to become
  master. Thus we have time to ensure we can become the new master node
  and respond to the new nodes start message.

  The other nodes that decides to not become master will wait for another
  10 seconds for the master to send his start confirmed message. If no
  message arrive, they will make a renewed election of new master. This
  procedure will be repeated every 10 seconds until either the elected
  master will send his start reply message or until the node itself becomes
  master and will start the master start procedure.

  When the master have received replies from all other nodes about a starting
  node, the master starts the synchronisation procedure if a quorum of Cluster
  Servers have started. If no quorum has started, the master will wait for
  more Cluster Servers and use the heartbeat protocol which means sending
  the following message every 3 seconds:

  heartbeat<CR>
  
  and receive the reply:

  heartbeat reply<CR>

  from all other started Cluster Server.

  When a quorum of Cluster Servers are started, it means that at least one
  node participated in the latest update of the configuration. If all nodes
  did participate in this update and have same version and the same state,
  then the master will send the following message to indicate that
  synchronisation is done.

  synchronization done<CR>

  and receive the reply

  synchronization done reply<CR>

  When a slave receives this message and when the master sends this message
  the Cluster Servers are started and are ready to receive requests for new
  updates of the configuration including requests to get the configuration.

  If one of the nodes isn't up-to-date with state, but has the right version
  number, the Cluster Server will send a commit message to this node.

  cluster server commit<CR>
  nodeid: node_number<CR>
  version: version_number<CR>
  pid: pid_number<CR>

  and will receive the response when commit is done:

  cluster server commit reply<CR>
  nodeid: node_number<CR>
  version: version_number<CR>
  pid: pid_number<CR>

  When at least one of the nodes are not having the right version number or
  have the right version number but is in a state which isn't possible to
  commit from, then the node needs to receive the configuration from another
  node.

  To prepare for this the master first notifies the node to send the
  configuration with the message.

  prepare send configuration<CR>
  nodeid: node_number<CR>
  version: version_number<CR>
  pid: pid_number<CR>
  receiver node id: node_number<CR>

  The master sends one such message for each node to receive a new
  configuration. The node responds with the message (one response
  for each request).

  prepare send configuration reply<CR>
  nodeid: node_number<CR>
  version: version_number<CR>
  pid: pid_number<CR>
  receiver node id: node_number<CR>

  When this message have been received the node have been prepared for a
  message requesting the configuration on a new channel and will allow
  for such a message if it arrives from the node for which it has opened
  up for.

  The master will then request the node to receive the configuration to fetch
  it from the given node. It will request it through.

  fetch configuration<CR>
  nodeid: node_number<CR>
  version: version_number<CR>
  pid: pid_number<CR>
  sender node id: node_number<CR>

  The receiver will immediately respond to this message to the master through
  the message.

  fetch configuration reply<CR>
  nodeid: node_number<CR>
  version: version_number<CR>
  pid: pid_number<CR>
  sender node id: node_number<CR>

  After sending this message it will connect to the given Cluster Server using
  another connection and get a configuration using the normal procedure to
  fetch a configuration. When it has received this configuration it will
  save this version on disk.

  Finally when it has retrieved the new configuration and saved it on disk, it
  will respond to the master again by using the message

  cluster server commit reply<CR>
  nodeid: node_number<CR>
  version: version_number<CR>
  pid: pid_number<CR>

  During this time the heartbeat protocol will ensure that we detect nodes
  no longer working.

  When all nodes have completed their fetch of a new configuration we will
  use the "synchronization done" protocol to complete the Cluster Server
  start-up.

  A new node cannot accept get configuration requests or update configuration
  requests from any other node (other than when specifically told to do so)
  before receiving the "synchronization done" message. A starting master
  cannot start accepting before sending synchronization done to the other
  Cluster Servers.

  During the time of inclusion of a new Cluster Server we will temporarily
  disable the ability to update the configuration.
*/

/*
  Cluster Server Heartbeat MODULE
  -------------------------------
*/
static int send_heartbeat(IC_CONNECTION *conn,
                          IC_INT_RUN_CLUSTER_SERVER *run_obj);
static int rec_heartbeat(IC_CONNECTION *conn,
                         IC_INT_RUN_CLUSTER_SERVER *run_obj,
                         guint32 cs_index);
static void handle_failed_heartbeat(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                    guint32 cs_index);
static void handle_started_cs_node(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                   guint32 cs_index);

static int
handle_cs_heartbeat(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                    IC_THREAD_STATE *thread_state,
                    guint32 cs_index,
                    gboolean single_loop)
{
  IC_INFO_CLUSTER_SERVER *info_cs= get_cluster_info(run_obj, cs_index);
  IC_CONNECTION *conn= info_cs->conn;
  IC_THREADPOOL_STATE *tp_state= thread_state->ic_get_threadpool(thread_state);
  int error;
  DEBUG_ENTRY("handle_cs_heartbeat");

  conn->conn_op.ic_set_rec_wait_ms(conn, 6000); /* Set timeout to 6 seconds */

  if (info_cs->is_client_side)
  {
    /* We are the client side, means we are responsible initiating heartbeat */
    while (!tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
    {
      if ((error= send_heartbeat(conn, run_obj)) ||
          (error= rec_heartbeat(conn, run_obj, cs_index)))
      {
        handle_failed_heartbeat(run_obj, cs_index);
        DEBUG_RETURN(error);
      }
      if (single_loop)
      {
        DEBUG_RETURN(0);
      }
      ic_sleep(2);
    }
  }
  else
  {
    /* Server side waits for heartbeat */
    while (!tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
    {
      if ((error= rec_heartbeat(conn, run_obj, cs_index)) ||
          (error= send_heartbeat(conn, run_obj)))
      {
        handle_failed_heartbeat(run_obj, cs_index);
        DEBUG_RETURN(error);
      }
      if (single_loop)
      {
        DEBUG_RETURN(0);
      }
    }
  }
  DEBUG_RETURN(0);
}

static int
send_heartbeat(IC_CONNECTION *conn,
               IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  int error;
  const gchar *start_state_str= run_obj->state.cs_started ?
                                  ic_started_str :
                                  ic_starting_str;
  DEBUG_ENTRY("send_heartbeat");

  if ((error= ic_send_with_cr(conn, ic_heartbeat_str)) ||
      (error= ic_send_with_cr(conn, start_state_str)) ||
      (error= ic_send_empty_line(conn)))
    DEBUG_RETURN(error);
  DEBUG_RETURN(0);
}

static int
rec_heartbeat(IC_CONNECTION *conn,
              IC_INT_RUN_CLUSTER_SERVER *run_obj,
              guint32 cs_index)
{
  int error;
  gboolean found;
  IC_INFO_CLUSTER_SERVER *other_info_cs= get_cluster_info(run_obj, cs_index);
  DEBUG_ENTRY("rec_heartbeat");

  if (!(error= ic_rec_simple_str(conn, ic_heartbeat_str)))
  {
    if (!(error= ic_rec_simple_str_opt(conn, ic_started_str, &found)))
    {
      if (found)
      {
         DEBUG_PRINT(CONFIG_LEVEL, ("Found started string"));
         g_mutex_lock(run_obj->state.protect_state);
         if (other_info_cs->start_state == IC_CS_NOT_STARTED)
         {
            handle_started_cs_node(run_obj, cs_index);
         }
         g_mutex_unlock(run_obj->state.protect_state);
      }
      else
      {
         if (!(error= ic_rec_simple_str(conn, ic_starting_str)))
         {
           DEBUG_PRINT(CONFIG_LEVEL, ("Found starting string"));
           g_mutex_lock(run_obj->state.protect_state);
           if (other_info_cs->start_state == IC_CS_STARTED)
           {
             /*
               State change from started to starting
               This is a breach of the protocol and we treat it as if
               we had a heartbeat failure.
             */
             g_mutex_unlock(run_obj->state.protect_state);
             DEBUG_RETURN(IC_PROTOCOL_ERROR);
           }
           g_mutex_unlock(run_obj->state.protect_state);
         }
      }
    }
  }
  if (error)
    goto error;
  error= ic_rec_empty_line(conn);
error:
  DEBUG_RETURN(error);
}

static void
handle_failed_heartbeat(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                        guint32 cs_index)
{
  DEBUG_PRINT(CONFIG_LEVEL, ("Failed heartbeat for index=%u", cs_index));
  g_mutex_lock(run_obj->state.protect_state);
  get_cluster_info(run_obj, cs_index)->start_state= IC_CS_NOT_STARTED;
  g_mutex_unlock(run_obj->state.protect_state);
}

static void
handle_started_cs_node(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                       guint32 cs_index)
{
  DEBUG_ENTRY("handle_started_cs_node");

  /* State change from starting to started */
  DEBUG_PRINT(CONFIG_LEVEL,
    ("State change from starting to started for index=%u", cs_index));
  get_cluster_info(run_obj, cs_index)->start_state= IC_CS_STARTED;
  DEBUG_RETURN_EMPTY;
}

/*
  Handle Start Protocol MODULE
  ----------------------------

  This module takes care of the actions needed to start the Cluster
  Server by interacting with other Cluster Servers.
*/
static int send_start_cluster_server_req(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                         IC_CONNECTION *conn);
static int rec_start_cluster_server_req(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                        IC_CONNECTION *conn,
                                        guint32 *cs_index,
                                        guint32 *error_line);
static int send_start_cluster_server_reply(IC_INFO_CLUSTER_SERVER *my_info_cs,
                                           IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                           IC_CONNECTION *conn,
                                           guint32 *error_line);
static int rec_start_cluster_server_reply(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                          IC_CONNECTION *conn,
                                          guint32 cs_index);
static int rec_start_cluster_server_ok(IC_CONNECTION *conn);
static int send_start_cluster_server_ok(IC_CONNECTION *conn);
static int verify_master_index_view(IC_INT_RUN_CLUSTER_SERVER *run_obj);
static gboolean is_view_from_starting_node(IC_INFO_CLUSTER_SERVER *info_cs);

static int
handle_start_protocol(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                      IC_CONNECTION *conn,
                      gboolean is_client,
                      guint32 *cs_index)
{
  int error;
  guint32 error_line= 0;
  gboolean found;
  IC_INFO_CLUSTER_SERVER *my_info_cs= get_my_cluster_info(run_obj);
  gchar *err_msg;
  gchar buf[IC_MAX_ERROR_STRING_SIZE];
  DEBUG_ENTRY("handle_start_protocol");

  if (!is_client)
  {
    /* Server starts protocol */
    if ((error= send_start_cluster_server_req(run_obj, conn)))
      goto error;
  }
  else
  {
    /* Client waits for start request and sends response */
    if ((error= rec_start_cluster_server_req(run_obj,
                                             conn,
                                             cs_index,
                                             &error_line)) ||
        (error= send_start_cluster_server_reply(my_info_cs,
                                                run_obj,
                                                conn,
                                                &error_line)))
    {
      error_line= __LINE__;
      goto error;
    }
    /* Wait for ok or corrected response */
    if ((error= ic_rec_simple_str_opt(conn,
                                      ic_start_cluster_server_ok_str,
                                      &found)))
    {
      error_line= __LINE__;
      goto error;
    }
    if (found)
    {
      if ((error= ic_rec_empty_line(conn)))
      {
        error_line= __LINE__;
        goto error;
      }
      /*
        Copy our master index view to view by other node since he
        accepted our view
      */
      g_mutex_lock(run_obj->state.protect_state);
      memcpy(&get_cluster_info(run_obj, *cs_index)->master_index_view[0],
             &my_info_cs->master_index_view[0],
             sizeof(guint32)*IC_MAX_CLUSTER_SERVERS);
      g_mutex_unlock(run_obj->state.protect_state);
      DEBUG_RETURN(0);
    }
    /* Not found means we should expect to receive new reply */
  }

  /* Receive reply from other Cluster Server */
  if ((error= rec_start_cluster_server_reply(run_obj,
                                             conn,
                                             *cs_index)))
  {
    if (error != IC_ERROR_CHANGE_VIEW || is_client)
    {
      error_line= __LINE__;
      goto error;
    }
    /* We had a more up-to-date view on master node order */
    if ((error= send_start_cluster_server_reply(my_info_cs,
                                                run_obj,
                                                conn,
                                                &error_line)) ||
        (error= rec_start_cluster_server_ok(conn)))
    {
      error_line= __LINE__;
      goto error;
    }
  }
  else
  {
    /* Send ok */
    if ((error= send_start_cluster_server_ok(conn)))
    {
      error_line= __LINE__;
      goto error;
    }
  }
  DEBUG_RETURN(0);

error:
  err_msg= ic_common_fill_error_buffer(NULL,
                                       error_line,
                                       error,
                                       buf);
  ic_printf("%s\n", err_msg);
  DEBUG_RETURN(error);
}

/* Send start cluster server-message */
static int
send_start_cluster_server_req(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                              IC_CONNECTION *conn)
{
  int error;
  IC_INFO_CLUSTER_SERVER *info_cs= get_my_cluster_info(run_obj);
  DEBUG_ENTRY("send_start_cluster_server_req");

  if ((error= ic_send_with_cr(conn, ic_start_cluster_server_str)) ||
      (error= ic_send_with_cr_with_num(conn, nodeid_str,
                                       (guint64)run_obj->cs_nodeid)) ||
      (error= ic_send_with_cr_with_num(conn, ic_version_str,
                                   info_cs->config_version_number)) ||
      (error= ic_send_with_cr_with_num(conn, ic_status_str,
                                  (guint64)info_cs->state)) ||
      (error= ic_send_with_cr_with_num(conn, ic_pid_str,
                                  (guint64)info_cs->pid)) ||
      (error= ic_send_empty_line(conn)))
  {
    DEBUG_RETURN(error);
  }
  DEBUG_RETURN(0);
}

static int
rec_start_cluster_server_req(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                             IC_CONNECTION *conn,
                             guint32 *cs_index,
                             guint32 *error_line)
{
  guint32 node_id, status;
  guint64 pid, version_num;
  int error;
  guint32 found_index;
  IC_INFO_CLUSTER_SERVER *info_cs;
  DEBUG_ENTRY("rec_start_cluster_server_req");

  if ((error= ic_rec_simple_str(conn, ic_start_cluster_server_str)) ||
      (error= ic_rec_number(conn, nodeid_str, &node_id)) ||
      (error= ic_rec_long_number(conn, ic_version_str, &version_num)) ||
      (error= ic_rec_number(conn, ic_status_str, &status)) ||
      (error= ic_rec_long_number(conn, ic_pid_str, &pid)) ||
      (error= ic_rec_empty_line(conn)))
  {
    *error_line= __LINE__; 
    goto error;
  }
  if ((error= find_cs_index(run_obj, node_id, &found_index)))
  {
    *error_line= __LINE__; 
    goto error;
  }
  if (*cs_index != IC_MAX_CLUSTER_SERVERS &&
      *cs_index != found_index)
  {
    *error_line= __LINE__; 
    DEBUG_RETURN(IC_PROTOCOL_ERROR);
  }
  *cs_index= found_index;
  info_cs= get_cluster_info(run_obj, found_index);
  g_mutex_lock(run_obj->state.protect_state);
  info_cs->pid= (IC_PID_TYPE)pid;
  info_cs->state= (IC_CONF_STATE_TYPE)status;
  info_cs->config_version_number= (IC_CONF_VERSION_TYPE)version_num;
  g_mutex_unlock(run_obj->state.protect_state);
error:
  DEBUG_RETURN(error);
}

/*
  Code executed on behalf of starting Cluster Servers in already started
  Cluster Servers.
*/
static int
send_start_cluster_server_reply(IC_INFO_CLUSTER_SERVER *my_info_cs,
                                IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                IC_CONNECTION *conn,
                                guint32 *error_line)
{
  int error;
  guint32 i;
  DEBUG_ENTRY("send_start_cluster_server_reply");

  if ((error= ic_send_with_cr(conn, ic_start_cluster_server_reply_str)) ||
      (error= ic_send_with_cr_with_num(conn, nodeid_str,
                                       my_info_cs->node_id)) ||
      (error= ic_send_with_cr_with_num(conn, ic_version_str,
                                       my_info_cs->config_version_number)) ||
      (error= ic_send_with_cr_with_num(conn, ic_status_str,
                                       my_info_cs->state)) ||
      (error= ic_send_with_cr_with_num(conn, ic_pid_str,
                                       my_info_cs->pid)) ||
      (error= ic_send_with_cr_with_num(conn, ic_master_index_size_str,
                                       my_info_cs->master_index_size)))
  {
    *error_line= __LINE__;
    goto end;
  }
  for (i= 0; i < my_info_cs->master_index_size; i++)
  {
    if ((error= ic_send_with_cr_with_num(conn, nodeid_str,
                                         my_info_cs->master_index_view[i])))
    {
      *error_line= __LINE__;
      goto end;
    }
  }
  if ((error= ic_send_empty_line(conn)))
  {
    *error_line= __LINE__;
    goto end;
  }
end:
  g_mutex_unlock(run_obj->state.protect_state);
  DEBUG_RETURN(error);
}

/* Receive start cluster server reply-message */
static int
rec_start_cluster_server_reply(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                               IC_CONNECTION *conn,
                               guint32 cs_index)
{
  int error;
  guint32 i;
  guint64 version_num, pid;
  guint32 node_id, status, index_size;
  IC_INFO_CLUSTER_SERVER *info_cs;
  guint32 master_node_index[IC_MAX_CLUSTER_SERVERS];
  DEBUG_ENTRY("rec_start_cluster_server_reply");

  if ((error= ic_rec_simple_str(conn, ic_start_cluster_server_reply_str)) ||
      (error= ic_rec_number(conn, nodeid_str, &node_id)) ||
      (error= ic_rec_long_number(conn, ic_version_str, &version_num)) ||
      (error= ic_rec_number(conn, ic_status_str, &status)) ||
      (error= ic_rec_long_number(conn, ic_pid_str, &pid)) ||
      (error= ic_rec_number(conn, ic_master_index_size_str, &index_size)))
    goto no_lock_error;
  if (index_size > IC_MAX_CLUSTER_SERVERS)
    DEBUG_RETURN(IC_PROTOCOL_ERROR);
  for (i= 0; i < index_size; i++)
  {
    if ((error= ic_rec_number(conn, nodeid_str, &master_node_index[i])))
      goto no_lock_error;
  }
  if ((error= ic_rec_empty_line(conn)))
    goto no_lock_error;
  /*
    We have received a correct protocol message describing a started
    Cluster Server.
  */
  info_cs= get_cluster_info(run_obj, cs_index);
  g_mutex_lock(run_obj->state.protect_state);
  /* Verify node is using correct node id */
  if (info_cs->node_id != node_id)
  {
    error= IC_ERROR_WRONG_NODE_ID;
    goto error;
  }
  /* Copy master index into this node's view on it */
  info_cs->master_index_size= index_size;
  memcpy(&info_cs->master_index_view[0],
         &master_node_index[0],
         sizeof(guint32) * index_size);

  /* Note we have a connection to the node */
  info_cs->cs_connect_state= TRUE;

  if ((error= verify_master_index_view(run_obj)))
    goto error;

  /* Update status variables */
  info_cs->pid= (IC_PID_TYPE)pid;
  info_cs->state= (IC_CONF_STATE_TYPE)status;
  info_cs->config_version_number= (IC_CONF_VERSION_TYPE)version_num;
  error= 0;
error:
  g_mutex_unlock(run_obj->state.protect_state);
no_lock_error:
  DEBUG_RETURN(error);
}

static int
rec_start_cluster_server_ok(IC_CONNECTION *conn)
{
  int error;

  if ((error= ic_rec_simple_str(conn, ic_start_cluster_server_ok_str)) ||
      (error= ic_rec_empty_line(conn)))
    return error;
  return 0;
}

static int
send_start_cluster_server_ok(IC_CONNECTION *conn)
{
  int error;
  DEBUG_ENTRY("send_start_cluster_server_ok");

  if ((error= ic_send_with_cr(conn, ic_start_cluster_server_ok_str)) ||
      (error= ic_send_empty_line(conn)))
  {
    DEBUG_RETURN(error);
  }
  DEBUG_RETURN(0);
}

static int
verify_master_index_view(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  IC_INFO_CLUSTER_SERVER *first_info_cs= NULL;
  IC_INFO_CLUSTER_SERVER *curr_info_cs, *change_info_cs, *keep_info_cs;
  guint32 i;
  int error;
  DEBUG_ENTRY("verify_master_index_view");

  for (i= 0; i < IC_MAX_CLUSTER_SERVERS; i++)
  {
    curr_info_cs= get_cluster_info(run_obj, i);
    if (!curr_info_cs->cs_connect_state)
      continue;
    if (!first_info_cs)
    {
      first_info_cs= curr_info_cs;
      continue;
    }
    if (first_info_cs->master_index_size !=
        curr_info_cs->master_index_size)
      goto error;
    if (memcmp(&first_info_cs->master_index_view[0],
               &curr_info_cs->master_index_view[0],
               sizeof(guint32)*first_info_cs->master_index_size))
    {
      /*
        Differing views on master index view can happen when one node
        is already a master node and the others aren't aware of this
        yet.
      */
      if (is_view_from_starting_node(first_info_cs))
      {
        if (is_view_from_starting_node(curr_info_cs))
          goto error;
        change_info_cs= first_info_cs;
        keep_info_cs= curr_info_cs;
        if (curr_info_cs->master_index_view[0] ==
            curr_info_cs->node_id)
          goto change_view;
      }
      else
      {
        if (!is_view_from_starting_node(curr_info_cs))
          goto error;
        change_info_cs= curr_info_cs;
        keep_info_cs= first_info_cs;
        if (first_info_cs->master_index_view[0] ==
            first_info_cs->node_id)
          goto change_view;
      }
      goto error;
    }
  }
  error= 0;
  goto end;

change_view:
  memcpy(&change_info_cs->master_index_view[0],
         &keep_info_cs->master_index_view[0],
         sizeof(guint32)*keep_info_cs->master_index_size);
  error= IC_ERROR_CHANGE_VIEW;
  goto end;
error:
  error= IC_ERROR_MASTER_INDEX_VIEW_DIFFERS;
end:
  DEBUG_RETURN(error);
}

static gboolean
is_view_from_starting_node(IC_INFO_CLUSTER_SERVER *info_cs)
{
  guint32 i;
  
  for (i= 1; i < info_cs->master_index_size; i++)
  {
    if (info_cs->master_index_view[i] < info_cs->master_index_view[i-1])
      return FALSE;
  }
  return TRUE;
}

/*
  MODULE: Start threads for Cluster Server
  ----------------------------------------

  When starting a Cluster Server one thread per other Cluster Server is
  created to handle the communication with the correspondent Cluster
  Server.

  The start method of this thread is start_cs_func.
*/

static guint32 get_count_active_start_threads(
                     IC_INT_RUN_CLUSTER_SERVER *run_obj);
static guint32 get_count_connected_cluster_servers(
                     IC_INT_RUN_CLUSTER_SERVER *run_obj);
static void release_cs_connection(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                  IC_CONNECTION *conn,
                                  guint32 cs_index,
                                  gboolean lock);
static void set_up_initial_master_index_view(
                     IC_INT_RUN_CLUSTER_SERVER *run_obj);
static IC_CONF_VERSION_TYPE find_max_version(
                     IC_INT_RUN_CLUSTER_SERVER *run_obj);
static IC_CONF_VERSION_TYPE find_max_committed_version(
                     IC_INT_RUN_CLUSTER_SERVER *run_obj);
static int check_if_cs_started(IC_INT_RUN_CLUSTER_SERVER *run_obj);

static gpointer
start_cs_func(gpointer data)
{
  IC_THREAD_STATE *thread_state= (IC_THREAD_STATE*)data;
  IC_THREADPOOL_STATE *tp_state;
  THREAD_DATA *thread_data;
  IC_INT_RUN_CLUSTER_SERVER *run_obj;
  guint32 cs_index;
  IC_INT_API_CONFIG_SERVER *apic;
  IC_RUN_CLUSTER_STATE *rc_state;
  IC_INFO_CLUSTER_SERVER *this_info_cs, *my_info_cs;
  IC_INFO_CLUSTER_SERVER *info_cs;
  IC_CONNECTION *conn= NULL;
  guint32 active_start_threads, connected_servers, num_servers, i;
  int error;
  gchar *err_str;
  gboolean is_client;
  gboolean set_have_waited, have_waited, equal, found;
  gboolean running_server= FALSE;
  gboolean lock;
  DEBUG_THREAD_ENTRY("start_cs_func");
  tp_state= thread_state->ic_get_threadpool(thread_state);
  thread_data=
    (THREAD_DATA*)tp_state->ts_ops.ic_thread_get_object(thread_state);
  run_obj= thread_data->run_obj;
  cs_index= thread_data->cs_index;
  apic= run_obj->apic;
  rc_state= &run_obj->state;
  this_info_cs= get_cluster_info(run_obj, cs_index);
  my_info_cs= get_my_cluster_info(run_obj);
  is_client= (this_info_cs->node_id > my_info_cs->node_id);

  tp_state->ts_ops.ic_thread_started(thread_state);
  if (tp_state->ts_ops.ic_thread_startup_done(thread_state))
    goto early_end;

  while (1)
  {
    err_str= NULL;
    if (tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
      goto early_end;
    if (is_client)
    {
      if (!(error= connect_cluster_server(apic,
                                          &conn,
                                          cs_index,
                                          (guint32)1,
                                          is_client,
                                          &err_str)))
      {
        DEBUG_PRINT(CONFIG_LEVEL, ("Connect to Cluster Server index %u",
                    cs_index));
        this_info_cs->is_client_side= TRUE;
        this_info_cs->conn= conn;
        break;
      }
      DEBUG_PRINT(CONFIG_LEVEL,
       ("Failed client side connect to Cluster Server, index %u", cs_index));
      /* Another attempt to connect failed, continue trying */
    }
    else
    {
      if (!running_server)
      {
        /*
          We are currently not running the Server part of the Connect to
          Cluster Server code. Check if someone else is, if not we'll
          assume this role.
        */
        g_mutex_lock(rc_state->protect_state);
        if (!run_obj->running_server)
        {
          /*
            No one else is running the Socket server, we'll do that by
            marking that there is a running server and the local variable
            tells us it is us. We'll avoid special logic for this code and
            will start running the server socket in the next loop.
          */
          running_server= TRUE;
          run_obj->running_server= TRUE;
          g_mutex_unlock(rc_state->protect_state);
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Index %u running socket server", cs_index));
        }
        else
        {
          /*
            Check if we're already connected through the service of the connect
            server thread, otherwise wait for the connect server thread to
            complete its task or that the connect server thread got a
            connection to itself such that another thread (likely ours) will
            take over the socket server role.
          */
          if (!this_info_cs->conn)
          {
            this_info_cs->wait_server_connect_other_thread= TRUE;
            ic_cond_timed_wait(rc_state->connect_cond, rc_state->protect_state,
                               3 * IC_MICROSEC_PER_SECOND);
          }
          if (this_info_cs->conn)
          {
            /* The connect server thread connected for us */
            DEBUG_PRINT(CONFIG_LEVEL, ("Connect to Cluster Server index %u",
                        cs_index));
            my_info_cs->is_client_side= FALSE;
            break;
          }
        }
      }
      else
      {
        this_info_cs->wait_server_connect_other_thread= FALSE;
        if (!(error= connect_cluster_server(apic,
                                            &conn,
                                            cs_index,
                                            (guint32)3,
                                            is_client,
                                            &err_str)))
        {
          found= FALSE;
          DEBUG_PRINT(CONFIG_LEVEL,
                 ("Successful connect at server side to Cluster Server"));
          for (i= 0; i < run_obj->num_cluster_servers; i++)
          {
            equal= FALSE;
            info_cs= get_cluster_info(run_obj, i);
            if (info_cs->wait_server_connect_other_thread ||
                i == cs_index)
            {
              if (!(conn->conn_op.ic_check_connection(conn,
                   apic->cluster_conn.cluster_server_ips[i],
                   apic->cluster_conn.cluster_server_ports[i],
                   &equal)) && equal)
              {
                found= TRUE;
                g_mutex_lock(rc_state->protect_state);
                info_cs->conn= conn;
                info_cs->wait_server_connect_other_thread= FALSE;
                if (i == cs_index)
                {
                  /* Connection from node this thread is handling */
                  run_obj->running_server= FALSE;
                  info_cs->is_client_side= FALSE;
                  DEBUG_PRINT(CONFIG_LEVEL,
                              ("Connect to Cluster Server index %u",
                              cs_index));
                }
                g_cond_broadcast(rc_state->connect_cond);
                g_mutex_unlock(rc_state->protect_state);
              }
            }
          }
          if (!found)
          {
            /* Connect from unknown place or error */
            conn->conn_op.ic_free_connection(conn);
          }
          else if (!run_obj->running_server)
          {
            /* Connect to this part breaks out of loop */
            break;
          }
        }
      }
    }
    if (rc_state->cs_starting)
    {
      /*
        A quorum have already been reached without us, no use of continuing
        to connect to the node. When the node starts up we will have a port
        open to connect to.
      */
      g_cond_broadcast(rc_state->connect_cond);
      DEBUG_PRINT(CONFIG_LEVEL,
        ("Close thread for index %u, others already starting", cs_index));
      goto lock_end;
    }
  }
  /* Inform the other cluster server of our intentions */
  g_mutex_lock(rc_state->protect_state);
  conn= this_info_cs->conn;
  have_waited= FALSE;
  do
  {
    set_have_waited= FALSE;
    if (tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
    {
      DEBUG_PRINT(CONFIG_LEVEL, ("Close thread for index %u, stop flag set",
                  cs_index));
      goto end;
    }
    connected_servers= get_count_connected_cluster_servers(run_obj);
    num_servers= run_obj->num_cluster_servers;
    if ((connected_servers + 1) == num_servers)
    {
      /*
        All cluster servers have connected, we will proceed after waking
        all threads which have successfully connected to a Cluster Server.
      */
      g_cond_broadcast(rc_state->connect_cond);
      break;
    }
    else
    {
      /*
        We don't have a sufficient number of connections to start-up
        immediately. We will check if there is a sufficient number of
        threads still attempting the start, if not we will quit and
        make another attempt later.
        If there is a chance to start with a quorum we will wait for
        a few seconds, after this wait we will check if we've reached
        a quorum, if a quorum has been established we will start
        although not all Cluster servers have connected.
        If all active threads are already connected and they form
        a quorum we can start immediately.
      */
      active_start_threads= get_count_active_start_threads(run_obj);
      if ((active_start_threads + 1) < num_servers ||
          ((active_start_threads == 1) && (num_servers == 2)))
      {
        /* Too few threads to continue, this thread needs to stop */
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Too few threads to start, index %u, stop and restart", cs_index));
        goto end;
      }
      else if ((active_start_threads + 1 == num_servers) &&
               (num_servers > 2))
      {
        if (have_waited)
        {
          /* We're a quorum and we already waited, proceed */
          g_cond_broadcast(rc_state->connect_cond);
          break;
        }
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Quorum reached, wait 3 secs for other Cluster Servers index %u",
           cs_index));
        ic_cond_timed_wait(rc_state->connect_cond, rc_state->protect_state,
                           3 * IC_MICROSEC_PER_SECOND);
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Wakeup from waiting after quorum reached"));
        have_waited= TRUE;
        set_have_waited= TRUE;
      }
      else if (connected_servers == active_start_threads)
      {
        /*
          We are a quorum and the missing connection thread has already
          finished, thus we are ready to continue
        */
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Quorum reached, index %u other already stopped", cs_index));
        g_cond_broadcast(rc_state->connect_cond);
        break;
      }
      else
      {
        /*
           We will wait here for someone else to complete the connection
           set-up, whereafter we can continue with the Cluster Server start
           protocol. We'll use a timed wait to ensure we'll notice a
           stop or some other event needing attention.
        */
        DEBUG_PRINT(CONFIG_LEVEL,
          ("No quorum reached, index %u, wait for more nodes", cs_index));
        ic_cond_timed_wait(rc_state->connect_cond, rc_state->protect_state,
                           3 * IC_MICROSEC_PER_SECOND);
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Wakeup after waiting for quorum to be reached, we're connected"));
      }
    }
    if (!set_have_waited)
      have_waited= FALSE;
  } while (1);
  if (tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
  {
    DEBUG_PRINT(CONFIG_LEVEL, ("Close thread for index %u, stop flag set",
                cs_index));
    goto end;
  }
  DEBUG_PRINT(CONFIG_LEVEL,
    ("A quorum has been reached, set cs_starting to TRUE"));
  rc_state->cs_starting= TRUE;
  set_up_initial_master_index_view(run_obj);
  g_mutex_unlock(rc_state->protect_state);

  /*
    We are now connected to the Cluster Server this thread is appointed to
    handle. Also enough nodes have connected to form at least a quorum of
    Cluster Servers. We need to give our start up data and receive information
    about the other node's state. If any node fails as part of the start
    protocol we will disconnect all connections and try again.
  */
  if ((error= handle_start_protocol(run_obj,
                                    conn,
                                    is_client,
                                    &cs_index)))
  {
    /* Release connection, return and ensure we make another retry */
    DEBUG_PRINT(CONFIG_LEVEL, ("This thread failed with start protocol"));
    goto lock_end;
  }
  if (tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
    goto lock_end;

  /* Handling one round of heartbeat is part of start protocol */
  if (handle_cs_heartbeat(run_obj, thread_state, cs_index, TRUE))
    goto lock_end;

  if (tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
    goto lock_end;

  g_mutex_lock(rc_state->protect_state);
  do
  {
    if (!rc_state->cs_starting)
    {
      /*
        Some other connection failed with start protocol, return and
        ensure we make another attempt at starting up.
      */
      DEBUG_PRINT(CONFIG_LEVEL, ("Other thread failed with start protocol"));
      goto end;
    }
    if ((get_count_connected_cluster_servers(run_obj) +
         rc_state->start_threads_stopped + 1) ==
        run_obj->num_cluster_servers)
    {
      /*
        Startup is completed, we can now set state to started and
        start working as a Cluster Server to others in the Cluster.
      */
      g_assert(rc_state->start_threads_stopped < 2);
      g_assert(!(run_obj->num_cluster_servers == 2 &&
                 rc_state->start_threads_stopped > 0));
      if (check_if_cs_started(run_obj))
        goto end;
      g_cond_broadcast(rc_state->start_cond);
      DEBUG_PRINT(CONFIG_LEVEL, ("Completed startup of Cluster Server"));
      break;
    }
    else
    {
      /* Wait for all other Cluster Servers to complete */
      DEBUG_PRINT(CONFIG_LEVEL,
        ("Wait for other threads to complete startup"));
      ic_cond_timed_wait(rc_state->start_cond, rc_state->protect_state,
                         3 * IC_MICROSEC_PER_SECOND);
      g_mutex_unlock(rc_state->protect_state);
      if (tp_state->ts_ops.ic_thread_get_stop_flag(thread_state))
        goto lock_end;
      if (handle_cs_heartbeat(run_obj, thread_state, cs_index, TRUE))
        goto lock_end;
    }
  } while (1);
  g_mutex_unlock(rc_state->protect_state);
  /*
    We are now connected to the other Cluster Server and we have
    successfully exchanged start-up data. We keep this thread
    until the connection breaks or until we're about to quit.
  */
  handle_cs_heartbeat(run_obj, thread_state, cs_index, FALSE);
  DEBUG_THREAD_RETURN;

early_end:
  g_mutex_lock(rc_state->protect_state);
  rc_state->start_threads_stopped++;
  get_cluster_info(run_obj, cs_index)->is_start_thread_active= FALSE;
  g_mutex_unlock(rc_state->protect_state);
  DEBUG_THREAD_RETURN;

end:
  lock= TRUE;
  goto real_end;
lock_end:
  lock= FALSE;
real_end:
  release_cs_connection(run_obj, conn, cs_index, lock);
  DEBUG_THREAD_RETURN;
}

static guint32
get_count_active_start_threads(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 i;
  guint32 num_started= 0;

  for (i= 0; i < run_obj->num_cluster_servers; i++)
  {
    if (get_cluster_info(run_obj, i)->is_start_thread_active)
      num_started++;
  }
  return num_started;
}

static guint32
get_count_connected_cluster_servers(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 i;
  guint32 num_connected= 0;

  for (i= 0; i < run_obj->num_cluster_servers; i++)
  {
    if (get_cluster_info(run_obj, i)->conn)
      num_connected++;
  }
  return num_connected;
}

static void
release_cs_connection(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                      IC_CONNECTION *conn,
                      guint32 cs_index,
                      gboolean lock)
{
  IC_INFO_CLUSTER_SERVER *info_cs= get_cluster_info(run_obj, cs_index);
  if (!lock)
    g_mutex_lock(run_obj->state.protect_state);

  info_cs->is_start_thread_active= FALSE;
  info_cs->conn= NULL;
  run_obj->state.start_threads_stopped++;
  /*
    With only 2 Cluster Servers we know that losing one is sufficient
    to not be able to proceed with the starting. If we have more than
    2 (3 or 4 that is) then we can survive losing one of them, so
    e.g. if we have 3 Cluster Servers, 2 connections is fully connected,
    and 1 connection means there are 2 Cluster Servers that are connected
    which is sufficient, thus number of connections + 2 must not be
    smaller than the number of Cluster Servers.
  */
  if (run_obj->state.cs_starting &&
      (run_obj->num_cluster_servers == 2 ||
       get_count_connected_cluster_servers(run_obj) + 2 <
       run_obj->num_cluster_servers))
    run_obj->state.cs_starting= FALSE;

  g_cond_broadcast(run_obj->state.start_cond);
  g_mutex_unlock(run_obj->state.protect_state);
  if (conn)
    conn->conn_op.ic_free_connection(conn);
}

static void
set_up_initial_master_index_view(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 min_node_id= 0;
  guint32 lowest_node_id= IC_MAX_NODE_ID;
  guint32 index_size= 0;
  guint32 i, j;
  IC_INFO_CLUSTER_SERVER *my_info_cs= get_my_cluster_info(run_obj);
  IC_INFO_CLUSTER_SERVER *info_cs;

  /* Sort in lowest node number first */
  for (i= 0; i < run_obj->num_cluster_servers; i++)
  {
    for (j= 0; j < run_obj->num_cluster_servers; j++)
    {
       info_cs= get_cluster_info(run_obj, j);
       if (run_obj->state.cs_servers[j].node_id > min_node_id &&
           run_obj->state.cs_servers[j].node_id < lowest_node_id &&
           run_obj->state.cs_servers[j].conn)
         lowest_node_id= run_obj->state.cs_servers[j].node_id;
    }
    if (min_node_id != lowest_node_id)
      index_size++;
    my_info_cs->master_index_view[i]= lowest_node_id;
    min_node_id= lowest_node_id;
  }
  my_info_cs->master_index_size= index_size;
}

static IC_CONF_VERSION_TYPE
find_max_version(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 i;
  IC_CONF_VERSION_TYPE version_number= (IC_CONF_VERSION_TYPE)0;

  for (i= 0; i < run_obj->num_cluster_servers; i++)
  {
    if (run_obj->state.cs_servers[i].config_version_number > version_number)
      version_number= run_obj->state.cs_servers[i].config_version_number;
  }
  return version_number;
}

static IC_CONF_VERSION_TYPE
find_max_committed_version(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 i;
  IC_CONF_VERSION_TYPE version_number= (IC_CONF_VERSION_TYPE)0;

  for (i= 0; i < run_obj->num_cluster_servers; i++)
  {
    if (run_obj->state.cs_servers[i].state == CONFIG_STATE_BUSY &&
        run_obj->state.cs_servers[i].config_version_number > version_number)
      version_number= run_obj->state.cs_servers[i].config_version_number;
  }
  if (version_number == (IC_CONF_VERSION_TYPE)0)
  {
    /* All servers are in PREPARE state, max committed is -1 of max version */
    return (IC_CONF_VERSION_TYPE)(find_max_version(run_obj) - 1);
  }
  return version_number;
}

static void
verify_cs_started(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  guint32 out_of_date= 0;
  guint32 i; 
  IC_CONF_VERSION_TYPE max_committed_version;
  IC_RUN_CLUSTER_STATE *rc_state= &run_obj->state;

  g_mutex_lock(rc_state->protect_state);
  max_committed_version= find_max_committed_version(run_obj);
  for (i= 0; i < run_obj->num_cluster_servers; i++)
  {
    if (run_obj->state.cs_servers[i].config_version_number <
        max_committed_version)
      out_of_date++;
  }
  if (out_of_date > 1)
  {
    /* Impossible state occurred */
    abort();
  }
  g_mutex_unlock(rc_state->protect_state);
}

static int
check_if_cs_started(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  IC_INFO_CLUSTER_SERVER *my_info_cs= get_my_cluster_info(run_obj);
  IC_CONF_VERSION_TYPE config_version_number=
    my_info_cs->config_version_number;
  IC_CONF_STATE_TYPE status= my_info_cs->state;
  IC_CONF_VERSION_TYPE max_committed_version;
  guint32 i;
  int error;
  gchar *err_str;
  IC_CONF_VERSION_TYPE old_version;
  IC_API_CONFIG_SERVER *new_apic;
  IC_API_CLUSTER_CONNECTION api_cluster_conn;
  IC_RUN_CLUSTER_STATE *rc_state= &run_obj->state;
  DEBUG_ENTRY("check_if_cs_started");

  verify_cs_started(run_obj);
  for (i= 1; i < run_obj->num_cluster_servers; i++)
  {
    if (run_obj->state.cs_servers[i].config_version_number !=
        config_version_number ||
        run_obj->state.cs_servers[i].state != status)
    {
      max_committed_version= find_max_committed_version(run_obj);
      /* We haven't started yet, we still need to synchronize configs */
      if (my_info_cs->config_version_number > max_committed_version)
      {
        /*
          Our most recent version is a version in a prepared state which
          which will not be committed, we need to remove this version
          and set our state to committed of the previous version number.
          After this our Clusteonr Server is in synch with the committed
          version of the Cluster Servers. We can start our Cluster Server
          and accept reads of the configuration, but we cannot accept writes
          until all Cluster Servers are up-to-date.
        */
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Our version was never committed, go back one version"));
        (void)remove_config_files(run_obj->config_dir,
                                  run_obj->clu_infos,
                                  my_info_cs->config_version_number);
        if ((error= write_config_version_file(run_obj->config_dir,
                                      max_committed_version,
                                      CONFIG_STATE_BUSY,
                                      my_info_cs->pid)))
        {
          ic_print_error(error);
          ic_printf("Failed to write config version file at line %d",
                    __LINE__);
          DEBUG_RETURN(error);
        }
        my_info_cs->state= CONFIG_STATE_BUSY;
        my_info_cs->config_version_number= max_committed_version;
        /*
          At this point we have the wrong configuration version loaded,
          we need to get rid of the in-memory version and load the correct
          version instead.
        */
        if ((error= reread_disk_config(run_obj)))
        {
          DEBUG_RETURN(error);
        }
      }
      else if (my_info_cs->config_version_number < max_committed_version)
      {
        /*
          Our configuration is out-of-date. We don't have access to the
          latest committed configuration version. We need to get this using
          the get configuration protocol and then save this version on file
          before the Cluster Servers are ready to accept updates of the
          configuration. This can happen when our node failed and the
          remaining Cluster Servers formed a quorum that continued
          operating and performing updates.

          After reading the configuration from the network and writing it
          to disk, we reread it from disk to ensure that we allocated the
          configuration from the same code path in all instances of the
          Run Cluster Server module.
        */
        DEBUG_PRINT(CONFIG_LEVEL, ("Our version is out-of-date"));
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Fetch config from up-to-date Cluster Server"));
        if ((new_apic= ic_get_configuration(&api_cluster_conn,
                       run_obj->config_dir,
                       run_obj->cs_nodeid,
                       (guint32)1,
                       &run_obj->apic->cluster_conn.cluster_server_ips[1],
                       &run_obj->apic->cluster_conn.cluster_server_ports[1],
                       (guint32)3,
                       TRUE,
                       &error,
                       &err_str)))
        {
          ic_printf("Failed to get config from network: %s", err_str);
          ic_print_error(error);
          DEBUG_RETURN(error);
        }
        DEBUG_PRINT(CONFIG_LEVEL, ("Successful fetch of new configuration"));

        /* Remove old config files in case we missed several updates */
        DEBUG_PRINT(CONFIG_LEVEL, ("Remove old config files"));
        if (my_info_cs->state == CONFIG_STATE_PREPARE_UPDATE)
        {
          /*
            Make sure all config files are cleaned up,
            also the old committed
          */
          (void)remove_config_files(run_obj->config_dir,
                                    run_obj->clu_infos,
                                    my_info_cs->config_version_number - 1);
        }
        (void)remove_config_files(run_obj->config_dir,
                                  run_obj->clu_infos,
                                  my_info_cs->config_version_number);

        /* Write new configuration */
        DEBUG_PRINT(CONFIG_LEVEL, ("Write configuration to disk"));
        old_version= max_committed_version - 1;
        if ((error= ic_write_full_config_to_disk(run_obj->config_dir,
                      &old_version,
                      run_obj->clu_infos,
                      new_apic->api_op.ic_get_all_cluster_config(new_apic))))
        {
          DEBUG_RETURN(error);
        }
        my_info_cs->config_version_number= max_committed_version;
        my_info_cs->state= CONFIG_STATE_BUSY;

        DEBUG_PRINT(CONFIG_LEVEL, ("Reread configuration from disk"));
        if ((error= reread_disk_config(run_obj)))
        {
          DEBUG_RETURN(error);
        }
      }
      else if (my_info_cs->state == CONFIG_STATE_PREPARE_UPDATE)
      {
        /*
          We have the right version of the configuration but it hasn't been
          committed yet, since someone else have committed we are safe to
          commit it by updating the config version file.
        */
        DEBUG_PRINT(CONFIG_LEVEL, ("Commit our prepared version"));
        if ((error= write_config_version_file(run_obj->config_dir,
                                      my_info_cs->config_version_number,
                                      CONFIG_STATE_BUSY,
                                      my_info_cs->pid)))
        {
          ic_print_error(error);
          ic_printf("Failed to write config version file at line %d",
                    __LINE__);
          DEBUG_RETURN(error);
        }
        my_info_cs->state= CONFIG_STATE_BUSY;
      }
      else
      {
        /*
          Our cluster server is in synch, however someone else is not in
          in synch, we need to proceed with start and be ready to accept
          get config requests from this cluster server. We should be safe
          since the Cluster Server not in synch only can occur when we
          make updates with a quorum. So a quorum (1 of 1, 2 of 2, 2 of 3
          or 3 of 4) must have existed when the update occurred and these
          nodes made a transactional change and thus neither of these
          nodes will be out-of-date. Thus a maximum of one node can be
          out-of-date and the other nodes are ok and will start up without
          needing to get configuration from another node. All other nodes
          should be able to start without assistance and thus when a
          quorum starts, at least one node should be able to answer the
          get config request.
        */
        DEBUG_PRINT(CONFIG_LEVEL, ("Our Cluster Server is in synch"));
        DEBUG_RETURN(0);
      }
      /*
        We have changed our state, it's possible that all Cluster Servers are
        in synch now, so we retry check for this by a recursive call here.
        Since our state will be CONFIG_STATE_BUSY and equal to
        max_committed_version here, there is no risk of more than one
        recursive call here.
      */
      DEBUG_RETURN(check_if_cs_started(run_obj));
    }
  }
  DEBUG_PRINT(CONFIG_LEVEL, ("All Cluster Servers are in synch"));
  g_mutex_lock(rc_state->protect_state);
  run_obj->state.cs_started= TRUE;
  run_obj->state.cs_starting= FALSE;
  g_mutex_unlock(rc_state->protect_state);
  DEBUG_RETURN(0);
}


/*
  MODULE: Complete Startup Handling as part of Run Cluster Server
  ---------------------------------------------------------------

  This function is called from run_cluster_server to complete the startup
  handling. Some nodes may need to connect to other Cluster Servers to get
  configuration before the node is able to start. State of start is
  communicated through the heartbeat messages started in Cluster Server
  Start handling.
*/
static int
complete_startup_handling(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  int error;
  gchar *err_str;
  DEBUG_ENTRY("complete_startup_handling");

  /*
    Now we have discovered the proper configuration to use in the
    Cluster Server and we can create Data API object based on this
    configuration.
  */
  if (!(run_obj->apid_global= ic_create_apid_global(
                     (IC_API_CONFIG_SERVER*)run_obj->apic,
                     TRUE,
                     &error,
                     &err_str)))
  {
    ic_printf("%s", err_str);
    DEBUG_RETURN(error);
  }
  DEBUG_RETURN(0);
}

static void
report_startup_completed(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  DEBUG_PRINT(CONFIG_LEVEL, ("Startup is completed"));
  run_obj->state.cs_started= TRUE;
}

/*
  MODULE: Run Cluster Server
  --------------------------
    This is the module that provided with a configuration data structures
    ensures that anyone can request this configuration through a given
    socket and port.

    The module implements the IC_RUN_CLUSTER_SERVER interface.
    This interface has seven methods:

    ic_create_run_cluster: This creates the IC_RUN_CLUSTER_SERVER object
    ic_start_cluster_server: This starts the cluster server, implemented in
                           the routine start_cluster_server
                           Implemented in the start cluster server module.
    ic_fill_error_buffer:  This creates an error message in error cases.
                           Implemented in rcs_fill_error_buffer.
    ic_run_cluster_server: This runs the cluster server, it is implemented
                           in the routine run_cluster_server
    ic_get_api_config:     This gets the IC_API_CONFIG_SERVER object of
                           Cluster Server.
    ic_stop_cluster_server: Stops the cluster server in an orderly manner.
                            Implemented in stop cluster server
                            Implemented in the stop cluster server module.
    ic_free_run_cluster:   This routine frees the IC_RUN_CLUSTER_SERVER
                           object and all other data allocated by cluster
                           server.
*/

static void free_run_cluster(IC_RUN_CLUSTER_SERVER *run_obj);
static int run_cluster_server(IC_RUN_CLUSTER_SERVER *run_obj);
static gchar* rcs_fill_error_buffer(IC_RUN_CLUSTER_SERVER *run_obj,
                                    int error_code,
                                    gchar *error_buffer);
static IC_API_CONFIG_SERVER* get_api_config(
                             IC_RUN_CLUSTER_SERVER *ext_run_obj);
static void free_run_cluster_protect(IC_INT_RUN_CLUSTER_SERVER *run_obj);

/*
 Here starts the code part of the Run Cluster Server Module
 ============================================================
*/

IC_RUN_CLUSTER_SERVER*
ic_create_run_cluster(IC_STRING *config_dir,
                      const gchar *process_name,
                      gchar *server_name,
                      gchar *server_port,
                      guint32 my_node_id)
{
  IC_INT_RUN_CLUSTER_SERVER *run_obj= NULL;
  IC_CONNECTION *conn;
  IC_MEMORY_CONTAINER *conf_mc_ptr;
  IC_THREADPOOL_STATE *tp_state= NULL;
  DEBUG_ENTRY("ic_create_run_cluster");

  if (!(run_obj= (IC_INT_RUN_CLUSTER_SERVER*)ic_calloc(
                sizeof(IC_INT_RUN_CLUSTER_SERVER))))
  {
    DEBUG_RETURN(NULL);
  }
  if (!(conf_mc_ptr= ic_create_memory_container(MC_DEFAULT_BASE_SIZE,
                                                0, TRUE)))
    goto error;
  if (!(tp_state= ic_create_threadpool(IC_DEFAULT_MAX_THREADPOOL_SIZE, FALSE)))
    goto error;
  if (!(run_obj->apic=
        (IC_INT_API_CONFIG_SERVER*)conf_mc_ptr->mc_ops.ic_mc_calloc(
                conf_mc_ptr, sizeof(IC_INT_API_CONFIG_SERVER))))
    goto error;
  if (!(run_obj->state.protect_state= g_mutex_new()))
    goto error;
  if (!(run_obj->state.update_cond= g_cond_new()))
    goto error;
  if (!(run_obj->state.connect_cond= g_cond_new()))
    goto error;
  if (!(run_obj->state.start_cond= g_cond_new()))
    goto error;
  /*
    Initialise the Cluster Server state, the state is protected by a mutex to
    ensure when several connections receive requests only one at a time can
    change the Cluster Server state.
  */
  run_obj->conf_mc_ptr= conf_mc_ptr;

  run_obj->process_name= process_name;
  run_obj->config_dir= config_dir;
  run_obj->cs_nodeid= my_node_id;
  run_obj->locked_configuration= FALSE;
  run_obj->max_cluster_id= 0;
  run_obj->num_clusters= 0;
  run_obj->state.err_obj.err_num= 0;
  run_obj->state.err_obj.line_number= 0;
  run_obj->tp_state= tp_state;

  /* Create the socket object for the Cluster Server */
  if (!(run_obj->conn= ic_create_socket_object(
                           FALSE, /* Server connection */
                           FALSE, /* Don't use mutex */
                           FALSE, /* Don't use connect thread */
                           CONFIG_READ_BUF_SIZE,
                           NULL,  /* Don't use authentication function */
                           NULL))) /* No authentication object */
    goto error;

  conn= run_obj->conn;
  conn->conn_op.ic_prepare_server_connection(conn,
                                             server_name,
                                             server_port,
                                             NULL,
                                             NULL,
                                             0,
                                             TRUE);

  run_obj->run_op.ic_start_cluster_server= start_cluster_server;
  run_obj->run_op.ic_fill_error_buffer= rcs_fill_error_buffer;
  run_obj->run_op.ic_run_cluster_server= run_cluster_server;
  run_obj->run_op.ic_stop_cluster_server= stop_cluster_server;
  run_obj->run_op.ic_get_api_config= get_api_config;
  run_obj->run_op.ic_free_run_cluster= free_run_cluster;
  DEBUG_RETURN((IC_RUN_CLUSTER_SERVER*)run_obj);

error:
  if (conf_mc_ptr)
  {
    if (tp_state)
      tp_state->tp_ops.ic_threadpool_stop(tp_state);
    free_run_cluster_protect(run_obj);
    if (run_obj->conn)
      run_obj->conn->conn_op.ic_free_connection(run_obj->conn);
    conf_mc_ptr->mc_ops.ic_mc_free(conf_mc_ptr);
  }
  ic_free(run_obj);
  DEBUG_RETURN(NULL);
}

/* Implements the ic_fill_error_buffer method */
static gchar*
rcs_fill_error_buffer(IC_RUN_CLUSTER_SERVER *ext_run_obj,
                      int error_code,
                      gchar *error_buffer)
{
  IC_INT_RUN_CLUSTER_SERVER *run_obj= (IC_INT_RUN_CLUSTER_SERVER*)ext_run_obj;
  gchar *extra_err_str= NULL;
  guint32 err_line= 0;

  if (error_code != run_obj->state.err_obj.err_num &&
      run_obj->state.err_obj.err_num != 0)
  {
    extra_err_str= ic_get_error_message(error_code);
    err_line= run_obj->state.err_obj.line_number;
    error_code= run_obj->state.err_obj.err_num;
  }
  return ic_common_fill_error_buffer(extra_err_str,
                                     err_line,
                                     error_code,
                                     error_buffer);
}

/* Implements the method ic_get_api_config method */
static IC_API_CONFIG_SERVER*
get_api_config(IC_RUN_CLUSTER_SERVER *ext_run_obj)
{
  IC_INT_RUN_CLUSTER_SERVER *run_obj= (IC_INT_RUN_CLUSTER_SERVER*)ext_run_obj;
  return (IC_API_CONFIG_SERVER*)run_obj->apic;
}

/* Free IC_RUN_CLUSTER_SERVER object, implements ic_free_run_cluster method */
static void
free_run_cluster(IC_RUN_CLUSTER_SERVER *ext_run_obj)
{
  IC_INT_RUN_CLUSTER_SERVER *run_obj= (IC_INT_RUN_CLUSTER_SERVER*)ext_run_obj;
  guint32 i;
  IC_APID_GLOBAL *apid_global;
  IC_THREADPOOL_STATE *tp_state;
  IC_CONNECTION *conn;
  DEBUG_ENTRY("free_run_cluster");

  if (run_obj)
  {
    tp_state= run_obj->tp_state;
    if (tp_state)
      tp_state->tp_ops.ic_threadpool_stop(tp_state);
    conn= run_obj->conn;
    if (conn)
      conn->conn_op.ic_free_connection(conn);
    apid_global= run_obj->apid_global;
    if (apid_global)
      apid_global->apid_global_ops->ic_free_apid_global(apid_global);
    for (i= 0; i < IC_MAX_CLUSTER_ID; i++)
    {
      if (run_obj->conf_objects[i])
        ic_hashtable_destroy(run_obj->conf_objects[i]->comm_hash);
    }
    free_run_cluster_protect(run_obj);
    if (run_obj->conf_mc_ptr)
      run_obj->conf_mc_ptr->mc_ops.ic_mc_free(run_obj->conf_mc_ptr);
    ic_free(run_obj);
  }
  DEBUG_RETURN_EMPTY;
}

static void
free_run_cluster_protect(IC_INT_RUN_CLUSTER_SERVER *run_obj)
{
  if (run_obj->state.protect_state)
    g_mutex_free(run_obj->state.protect_state);
  if (run_obj->state.update_cond)
    g_cond_free(run_obj->state.update_cond);
  if (run_obj->state.connect_cond)
    g_cond_free(run_obj->state.connect_cond);
  if (run_obj->state.start_cond)
    g_cond_free(run_obj->state.start_cond);
}

/*
  MODULE: Run Cluster Server method
  ---------------------------------
 *
 * Uses the methods complete_startup_handling and report_startup_completed
 * which are part of the Cluster Server Start Module before completing
 * startup properly.
 *
 * The RUN CLUSTER SERVER has a number of support methods internal to its
 * implementation. The run_cluster_server method listens to the socket for
 * the cluster server. As soon as someone connects, it creates a thread to
 * service the client request. It's in this thread where most of the
 * complexity of this module resides.
 *
 * To start the new thread the start_cluster_server_thread is used, using
 * a socket object which was forked from the socket which was listened to
 * in run_cluster_server.
 *
 * The new thread is executed in the run_cluster_server_thread method.
 * This method implements the high level parts of the NDB Management Server
 * protocol. For each action that is available there is a method handling
 * that action. These are the handler routines:
 * check_config_req: Check for configuration request and call
 *                   handle_config_request in case it is
 * handle_get_cluster_list: Request to get a list of cluster id and names
 * handle_report_event: Report an event from the client to the Cluster Server
 * handle_get_mgmd_nodeid_req: Get node id of Cluster Server
 * handle_convert_transporter_request: Convert socket to NDB Protocol
 * handle_start_protocol: Handles connect from starting Cluster Server
 *   This method is handled by the Cluster Server Start Module.
 * handle_set_connection_parameter_req: Set connection parameters for new
 *   NDB Protocol socket
 * handle_config_request: A request from the client to get a cluster
 *   configuration
 *
 * One connection can contain a number of these protocol actions and the
 * socket can as mentioned above also be converted to a NDB Protocol
 * socket.
 *
 * The only routine above with some complexity is the handle_config_request.
 */


struct ic_rc_param
{
  guint64 node_number;
  guint64 version_number;
  guint64 node_type;
  guint64 cluster_id;
  guint64 client_nodeid;
};
typedef struct ic_rc_param IC_RC_PARAM;

static int start_cluster_server_thread(IC_INT_RUN_CLUSTER_SERVER* run_obj,
                                       IC_CONNECTION *conn,
                                       IC_THREADPOOL_STATE *tp_state,
                                       guint32 thread_id);
static gpointer run_cluster_server_thread(gpointer data);
static int handle_get_cluster_list(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                   IC_CONNECTION *conn);
static int handle_get_connection_parameter(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                           IC_CONNECTION *conn,
                                           gchar *read_buf,
                                           guint32 read_size,
                                           guint32 *error_line);
static int handle_report_event(IC_CONNECTION *conn);
static int handle_get_mgmd_nodeid_req(IC_CONNECTION *conn,
                                      guint32 cs_nodeid,
                                      gchar *read_buf,
                                      guint32 read_size,
                                      guint32 *error_line);
static int handle_set_connection_parameter_req(
                                IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                IC_CONNECTION *conn,
                                guint32 client_nodeid);
static int handle_convert_transporter_request(
                                       IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                       IC_CONNECTION *conn,
                                       IC_RC_PARAM *param,
                                       gchar *read_buf,
                                       guint32 read_size,
                                       guint32 *error_line);
static int handle_config_request(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                 IC_CONNECTION *conn,
                                 IC_RC_PARAM *param);
static int check_for_stopped_rcs_threads(void *obj, int not_used);

/* Implements ic_run_cluster_server method.  */
static int
run_cluster_server(IC_RUN_CLUSTER_SERVER *ext_run_obj)
{
  IC_INT_RUN_CLUSTER_SERVER *run_obj= (IC_INT_RUN_CLUSTER_SERVER*)ext_run_obj;
  IC_THREADPOOL_STATE *tp_state= run_obj->tp_state;
  int ret_code= 0;
  guint32 thread_id;
  IC_CONNECTION *conn, *fork_conn;
  int error;
  DEBUG_ENTRY("run_cluster_server");

  if ((error= complete_startup_handling(run_obj)))
  {
    DEBUG_RETURN(error);
  }

  report_startup_completed(run_obj);

  conn= run_obj->conn;
  if ((ret_code= conn->conn_op.ic_set_up_connection(conn,
                                               check_for_stopped_rcs_threads,
                                               (void*)tp_state)))
  {
    DEBUG_PRINT(CONFIG_LEVEL,
      ("Failed to set-up listening connection"));
    goto error;
  }
  do
  {
    if ((ret_code= tp_state->tp_ops.ic_threadpool_get_thread_id_wait(tp_state,
                                                     &thread_id,
                                                     IC_MAX_THREAD_WAIT_TIME)))
      goto error;
    if ((ret_code= conn->conn_op.ic_accept_connection(conn)))
    {
      DEBUG_PRINT(COMM_LEVEL,
        ("Failed to accept a new connection"));
      goto error;
    }
    DEBUG_PRINT(CONFIG_LEVEL,
      ("Run cluster server has set up connection and has received"
       " a connection"));
    if (!(fork_conn=
           conn->conn_op.ic_fork_accept_connection(conn,
                                          FALSE))) /* No mutex */
    {
      DEBUG_PRINT(COMM_LEVEL,
      ("Failed to fork a new connection from an accepted connection"));
      goto error;
    }
    if ((ret_code= start_cluster_server_thread(run_obj,
                                               fork_conn,
                                               tp_state,
                                               thread_id)))
    {
      DEBUG_PRINT(THREAD_LEVEL,
        ("Start new thread to handle configuration request failed"));
      goto error;
    }
    DEBUG_PRINT(CONFIG_LEVEL,
      ("Ready to accept a new connection"));
  } while (1);

error:
  DEBUG_RETURN(ret_code);
}

static int
check_for_stopped_rcs_threads(void *object, int not_used)
{
  IC_THREADPOOL_STATE *tp_state= (IC_THREADPOOL_STATE*)object;
  (void) not_used;

  tp_state->tp_ops.ic_threadpool_check_threads(tp_state);
  return 0;
}

/* Start a new Cluster Server thread */
static int
start_cluster_server_thread(IC_INT_RUN_CLUSTER_SERVER* run_obj,
                            IC_CONNECTION *conn,
                            IC_THREADPOOL_STATE *tp_state,
                            guint32 thread_id)
{
  int error;
  DEBUG_ENTRY("start_cluster_server_thread");

  conn->conn_op.ic_set_param(conn, (void*)run_obj);
  error= tp_state->tp_ops.ic_threadpool_start_thread_with_thread_id(
                                              tp_state,
                                              thread_id,
                                              run_cluster_server_thread,
                                              conn,
                                              IC_SMALL_STACK_SIZE,
                                              FALSE);
  DEBUG_RETURN(error);
}

/* Run a Cluster Server thread */
static gpointer
run_cluster_server_thread(gpointer data)
{
  IC_THREAD_STATE *thread_state= (IC_THREAD_STATE*)data;
  gchar *read_buf;
  guint32 read_size;
  IC_THREADPOOL_STATE *rcs_tp;
  IC_CONNECTION *conn;
  IC_INT_RUN_CLUSTER_SERVER *run_obj;
  int error;
  guint32 cs_index;
  guint32 error_line= 0;
  int state= INITIAL_STATE;
  IC_RC_PARAM param;
  DEBUG_THREAD_ENTRY("run_cluster_server_thread");
  rcs_tp= thread_state->ic_get_threadpool(thread_state);
  conn= (IC_CONNECTION*) rcs_tp->ts_ops.ic_thread_get_object(thread_state);

  rcs_tp->ts_ops.ic_thread_started(thread_state);
  run_obj= (IC_INT_RUN_CLUSTER_SERVER*)conn->conn_op.ic_get_param(conn);
  /*
    Continue receiving until stop or timeout occurs, if a timeout occurs
    we will continue also in the case of start ongoing yet.
  */
  while ((!(error= ic_rec_with_cr(conn, &read_buf, &read_size)) ||
          (run_obj->state.cs_starting &&
           error == IC_ERROR_RECEIVE_TIMEOUT)) &&
         !rcs_tp->ts_ops.ic_thread_get_stop_flag(thread_state))
  {
    switch (state)
    {
      case INITIAL_STATE:
        if (!ic_check_buf(read_buf, read_size, get_cluster_list_str,
                          strlen(get_cluster_list_str)))
        {
          if ((error= handle_get_cluster_list(run_obj, conn)))
          {
            error_line= __LINE__;
            goto error;
          }
          state= WAIT_GET_NODEID;
          break;
        }
        if (!ic_check_buf(read_buf, read_size, get_nodeid_str,
                          strlen(get_nodeid_str)))
        {
          if ((error= handle_config_request(run_obj, conn, &param)))
          {
            error_line= __LINE__;
            goto error;
          }
          state= WAIT_GET_MGMD_NODEID;
          break;
        }
        if (!ic_check_buf(read_buf, read_size, report_event_str,
                          strlen(report_event_str)))
        {
          if ((error= handle_report_event(conn)))
          {
            error_line= __LINE__;
            goto error;
          }
          break; /* The report event is always done in separate connection */
        }
        if (!ic_check_buf(read_buf, read_size, set_connection_parameter_str,
                          strlen(set_connection_parameter_str)))
        {
          if ((error= handle_set_connection_parameter_req(run_obj,
                                                          conn,
                                                          (guint32)0)))
          {
            error_line= __LINE__;
            goto error;
          }
          goto end; /* Always sent as only message in separate connection */
        }
        if (!ic_check_buf(read_buf, read_size,
                          ic_start_cluster_server_str,
                          strlen(ic_start_cluster_server_str)))
        {
          cs_index= IC_MAX_CLUSTER_SERVERS;
          if ((error= handle_start_protocol(run_obj,
                                            conn,
                                            FALSE,
                                            &cs_index)))
            goto error;
          run_obj->state.cs_servers[cs_index].is_client_side= FALSE;
          handle_cs_heartbeat(run_obj, thread_state, cs_index, FALSE);
          goto end;
        }
        /* Fall through since get connection parameter is also allowed */
      case GET_CONNECTION_PARAMETER:
        if ((error= handle_get_connection_parameter(run_obj,
                                                    conn,
                                                    read_buf,
                                                    read_size,
                                                    &error_line)))
          goto error;
        state= GET_CONNECTION_PARAMETER;
        break; /* Always handled in separate connection */
      case WAIT_GET_NODEID:
        if (!ic_check_buf(read_buf, read_size, get_nodeid_str,
                          strlen(get_nodeid_str)))
        {
          if ((error= handle_config_request(run_obj, conn, &param)))
          {
            error_line= __LINE__;
            goto error;
          }
          state= WAIT_GET_MGMD_NODEID;
          break;
        }
        error_line= __LINE__;
        goto error;
      case WAIT_GET_MGMD_NODEID:
        if ((error= handle_get_mgmd_nodeid_req(conn,
                                               run_obj->cs_nodeid,
                                               read_buf,
                                               read_size,
                                               &error_line)))
          goto error;
        state= WAIT_SET_CONNECTION;
        break;
      case WAIT_SET_CONNECTION:
        if (!ic_check_buf(read_buf, read_size, set_connection_parameter_str,
                          strlen(set_connection_parameter_str)))
        {
          if ((error= handle_set_connection_parameter_req(
                            run_obj,
                            conn,
                            (guint32)param.client_nodeid)))
          {
            error_line= __LINE__;
            goto error;
          }
          break;
        }
        /*
          Here it is ok to fall through, the WAIT_SET_CONNECTION is an
          optional state. We can receive zero or many set connection
          messages. At any time we can also receive a convert transporter
          message.
        */
      case WAIT_CONVERT_TRANSPORTER:
        if ((error= handle_convert_transporter_request(run_obj,
                                                       conn,
                                                       &param,
                                                       read_buf,
                                                       read_size,
                                                       &error_line)))
          goto error;
        conn= NULL;
        goto break_out;
      default:
        abort();
        break;
    }
  }
break_out:
  if (conn)
  {
    DEBUG_PRINT(CONFIG_LEVEL, ("Connection closed by other side"));
  }
  else
  {
    DEBUG_PRINT(CONFIG_LEVEL, ("Connection taken over by Data API"));
  }
end:
  if (conn)
    conn->conn_op.ic_free_connection(conn);
  rcs_tp->ts_ops.ic_thread_stops(thread_state);
  DEBUG_THREAD_RETURN;

error:
  read_buf[read_size]= 0;
  ic_printf("Protocol error line %d", error_line);
  ic_printf("Protocol message: %s", read_buf);
  goto end;
}

/*
  MODULE: Get cluster list
  ------------------------
  Handle get cluster list protocol action in Cluster Server
*/

static int
handle_get_cluster_list(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                        IC_CONNECTION *conn)
{
  gchar cluster_name_buf[128];
  guint32 i;
  int error;
  IC_CLUSTER_CONFIG *clu_conf;
  IC_STRING cluster_name;
  DEBUG_ENTRY("handle_get_cluster_list");

  if ((error= ic_send_with_cr(conn, get_cluster_list_reply_str)))
    goto error;
  for (i= 0; i <= run_obj->max_cluster_id; i++)
  {
    clu_conf= run_obj->conf_objects[i];
    if (!clu_conf)
      continue;
    cluster_name_buf[0]= 0;
    IC_INIT_STRING(&cluster_name, cluster_name_buf, 0, TRUE);
    ic_add_string(&cluster_name, cluster_name_string);
    ic_add_ic_string(&cluster_name, &clu_conf->clu_info.cluster_name);
    if ((error= ic_send_with_cr(conn, cluster_name.str)) ||
        (error= ic_send_with_cr_with_num(conn, cluster_id_str, (guint64)i)))
      goto error;
  }
  if ((error= ic_send_with_cr(conn, end_get_cluster_list_str)))
    goto error;
  DEBUG_RETURN(0);
error:
  DEBUG_PRINT(CONFIG_LEVEL,
              ("Protocol error in get cluster list, code = %d", error));
  PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
}

/*
  MODULE: Handle Get Connection Parameter Request
  -----------------------------------------------
*/
static int
handle_get_connection_parameter(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                IC_CONNECTION *conn,
                                gchar *read_buf,
                                guint32 read_size,
                                guint32 *error_line)
{
  int error;
  guint32 cluster_id, node1_id, node2_id, param;
  IC_CLUSTER_CONFIG *clu_conf;
  guint32 port_number;
  IC_SOCKET_LINK_CONFIG *comm_section;
  DEBUG_ENTRY("handle_get_connection_parameter");

  if (ic_check_buf(read_buf, read_size, get_connection_parameter_str,
                   strlen(get_connection_parameter_str)))
  {
    *error_line= __LINE__;
    DEBUG_RETURN(IC_PROTOCOL_ERROR);
  }
  if ((error= ic_rec_cluster_id(conn, &cluster_id)) ||
      (error= ic_rec_number(conn, node1_str, &node1_id)) ||
      (error= ic_rec_number(conn, node2_str, &node2_id)) ||
      (error= ic_rec_number(conn, param_str, &param)) ||
      (error= ic_rec_empty_line(conn)) ||
      (error= IC_ERROR_SET_CONNECTION_PARAMETER_WRONG_PARAM, FALSE) ||
      (param != SOCKET_SERVER_PORT_NUMBER))
    goto error;
  if (cluster_id > run_obj->max_cluster_id ||
      !(clu_conf= run_obj->conf_objects[cluster_id]))
  {
    error= IC_ERROR_NO_SUCH_CLUSTER;
    goto error;
  }
  if ((error= get_socket_link_config(node1_id,
                                     node2_id,
                                     &comm_section,
                                     clu_conf)))
    goto error;
  /*
    Found a valid communication section.
    We return the configured port number if it is nonzero in which case it
    is permanent. If it is zero we instead return the dynamic port number.
    This could be a zero as well which indicates we don't know the value
    currently in which case the node asking will have to ask again later.
  */
  if (comm_section->server_port_number)
    port_number= comm_section->server_port_number;
  else
    port_number= comm_section->dynamic_server_port_number;

  if ((error= ic_send_with_cr(conn, get_connection_parameter_reply_str)) ||
      (error= ic_send_with_cr_with_num(conn, node1_str, node1_id)) ||
      (error= ic_send_with_cr_with_num(conn, node2_str, node2_id)) ||
      (error= ic_send_with_cr_with_num(conn, param_str,
                                       SOCKET_SERVER_PORT_NUMBER)) ||
      (error= ic_send_with_cr_with_num(conn, value_str,
                                       (guint64)port_number)) ||
      (error= ic_send_empty_line(conn)))
    goto error;
  DEBUG_RETURN(0);
error:
  DEBUG_PRINT(CONFIG_LEVEL,
    ("Error from handle_get_connection_parameter, code = %u", error));
  *error_line= __LINE__;
  DEBUG_RETURN(error);
}

/*
  MODULE: report event
  --------------------

  This protocol is used by the data server nodes to report shutdown of their
  process.

  The data contained in the protocol message is the same as the data sent in
  a EVENT_REP signal used by the NDB Protocol but instead a separate NDB MGM
  Protocol connection is opened up and used to report this special event.

  Data[0]:
  Bit 0-15 contains Event Type (always 27 in this case which means a shutdown
           has been completed).
  Bit 16-31 contains the node id of the node being shutdown.
  Data[1]:
  0:       Means it isn't restarting
  1:       Means restart and not initial restart
  2:       Means start from initial state
  4:       Another variant of initial restart
  Data[2]:
  OS Signal which caused shutdown (e.g. 11 for segmentation fault)

  If the shutdown was caused by an error there are three more words, for
  graceful shutdown only the above words are set.

  Data[4]:
  Error number
  Data[5]:
  Start phase when error occurred
  Data[6]:
  Always equal to 0
  TODO: Should direct output to file instead
*/
static int
handle_report_event(IC_CONNECTION *conn)
{
  guint64 num_array[32], length;
  gchar *read_buf;
  guint32 read_size;
  int error;
  guint32 report_node_id, os_signal_num, error_num= 0, start_phase= 0;
  DEBUG_ENTRY("handle_report_event");

  if ((error= ic_rec_with_cr(conn, &read_buf, &read_size)) ||
      (ic_check_buf_with_int(read_buf, read_size, length_str,
                             strlen(length_str), &length)) ||
      (error= ic_rec_with_cr(conn, &read_buf, &read_size)) ||
      (ic_check_buf_with_many_int(read_buf, read_size, data_str,
                                  strlen(data_str), (guint32)length,
                                  &num_array[0])))
  {
    error= error ? error : IC_PROTOCOL_ERROR;
    PROTOCOL_CONN_CHECK_ERROR_GOTO(error);
  }
  if ((error= ic_rec_simple_str(conn, ic_empty_string)))
    PROTOCOL_CONN_CHECK_ERROR_GOTO(error);
  if ((error= ic_send_with_cr(conn, report_event_reply_str)) ||
      (error= ic_send_with_cr(conn, result_ok_str)) ||
      (error= ic_send_empty_line(conn)))
    goto error;
  report_node_id= (guint32)(num_array[0] >> 16);
  g_assert((num_array[0] & 0xFFFF) == 59);
  if (num_array[1] == 0)
    ic_printf("Node %u has shutdown", report_node_id);
  else if (num_array[1] == 1)
    ic_printf("Node %u has restarted", report_node_id);
  else
    ic_printf("Node %u has performed initial restart", report_node_id);
  if (length == (guint64)3)
  {
    ic_printf(" due to graceful shutdown");
  }
  else
  {
    g_assert(length == (guint64)6);
    g_assert(num_array[5] == 0);
    os_signal_num= (guint32)num_array[2];
    error_num= (guint32)num_array[3];
    start_phase= (guint32)num_array[4];
    ic_printf(" due to error %u, OS Signal %u in startphase %u",
              error_num, os_signal_num, start_phase);
  }
  DEBUG_RETURN(0);
error:
  DEBUG_PRINT(CONFIG_LEVEL,
    ("Error from handle_report_event, code = %u", error));
  DEBUG_RETURN(error);
}

/*
  MODULE: Handle Get Cluster Server node id request
  -------------------------------------------------
*/
static int
handle_get_mgmd_nodeid_req(IC_CONNECTION *conn,
                           guint32 cs_nodeid,
                           gchar *read_buf,
                           guint32 read_size,
                           guint32 *error_line)
{
  int error;
  DEBUG_ENTRY("handle_get_mgmd_nodeid_req");

  if (ic_check_buf(read_buf, read_size, get_mgmd_nodeid_str,
                   strlen(get_mgmd_nodeid_str)))
  {
    *error_line= __LINE__;
    DEBUG_RETURN(IC_PROTOCOL_ERROR);
  }
  if ((error= ic_rec_simple_str(conn, ic_empty_string)) ||
      (error= ic_send_with_cr(conn, get_mgmd_nodeid_reply_str)) ||
      (error= ic_send_with_cr_with_num(conn, nodeid_str,
                                       (guint64)cs_nodeid)) ||
      (error= ic_send_empty_line(conn)))
  {
    DEBUG_PRINT(CONFIG_LEVEL,
                ("Protocol error in get mgmd nodeid, code = %d", error));
    *error_line= __LINE__;
    DEBUG_RETURN(error);
  }
  DEBUG_RETURN(0);
}

/*
  MODULE: Handle Set Connection Parameter Request
  -----------------------------------------------
  This request can be sent from client nodes when they have set a dynamic
  port and want to spread the information about this dynamic assignment.
  It can also be received from another Cluster Server that needs to
  replicate this information.
*/
static int send_set_connection_param_message(IC_CONNECTION *conn,
                                             guint32 cluster_id,
                                             guint32 node1_id,
                                             guint32 node2_id,
                                             guint32 port_number);
static int rec_set_connection_param_message(IC_CONNECTION *conn);
static int send_set_connection_parameter(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                         guint32 cluster_id,
                                         guint32 node1_id,
                                         guint32 node2_id,
                                         guint32 port_number);

/*
  handle_set_connection_parameter_req

  client_nodeid:     Node id of client, 0 if sent by Cluster Server
*/
static int
handle_set_connection_parameter_req(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                    IC_CONNECTION *conn,
                                    guint32 client_nodeid)
{
  int error;
  guint32 cluster_id, node1_id, node2_id, param;
  int value;
  const gchar *the_result_str, *the_message_str;
  IC_SOCKET_LINK_CONFIG *comm_section;
  IC_CLUSTER_CONFIG *clu_conf;
  DEBUG_ENTRY("handle_set_connection_parameter_req");

  if ((error= ic_rec_cluster_id(conn, &cluster_id)) ||
      (error= ic_rec_number(conn, node1_str, &node1_id)) ||
      (error= ic_rec_number(conn, node2_str, &node2_id)) ||
      (error= ic_rec_number(conn, param_str, &param)) ||
      (error= ic_rec_int_number(conn, value_str, &value)) ||
      (error= ic_rec_empty_line(conn)))
    goto error;
  /*
    We received a correct set connection parameter protocol message.
    Now we need to verify also that the data is reasonable and also
    perform the action associated with it.
  */
 
  if (param != SOCKET_SERVER_PORT_NUMBER)
  {
    error= IC_ERROR_SET_CONNECTION_PARAMETER_WRONG_PARAM;
  }
  else if (client_nodeid != 0 && node1_id != client_nodeid)
  {
    error= IC_ERROR_SET_CONNECTION_PARAMETER_WRONG_NODES;
  }
  else if (cluster_id > run_obj->max_cluster_id ||
           !(clu_conf= run_obj->conf_objects[cluster_id]))
  {
    error= IC_ERROR_NO_SUCH_CLUSTER;
  }
  else if (!(error= get_socket_link_config(node1_id,
                                           node2_id,
                                           &comm_section,
                                           clu_conf)))
  {
    if (comm_section->server_port_number)
      error= IC_ERROR_SET_CONNECTION_NO_DYNAMIC;
  }

  if (!error && client_nodeid)
  {
    /*
      We have successfully received a request to update the dynamic
      port number. Before we make the actual update we need to update
      the other cluster servers successfully first since the request
      was received from a client node.
    */
    error= send_set_connection_parameter(run_obj,
                                         cluster_id,
                                         node1_id,
                                         node2_id,
                                         value);
  }
  if (error)
  {
    the_result_str= ic_error_str;
    the_message_str= ic_get_error_message(error);
  }
  else
  {
    /* Update the actual dynamic server port number */
    comm_section->dynamic_server_port_number= value;

    the_message_str= ic_empty_string;
    the_result_str= ic_ok_str;
    /*
      We have received information about a dynamic port assignment.
      We need to spread this information to all other cluster servers
      in the grid, otherwise they cannot assist nodes starting up.
      We also need to update the configuration in memory in the cluster
      server, this is done by accessing the communication object and
      updating it.
      
      We also update the configuration information on disk to ensure
      a cluster server crash doesn't lose important information. However
      it is important to synchronize this information with any alive
      cluster server at start since this information can be changed also
      when not all cluster servers are up and running. It cannot be changed
      however when no cluster server is up since no node can start without
      a cluster server to read configuration information from.
    */
  }
  /* Now it's time to send the prepared response */
  if ((error= ic_send_with_cr(conn, set_connection_parameter_reply_str)) ||
      (error= ic_send_with_cr_two_strings(conn, message_str,
                                          the_message_str)) ||
      (error= ic_send_with_cr_two_strings(conn, result_str,
                                          the_result_str)) ||
      (error= ic_send_empty_line(conn)))
    goto error;
  /*
    We have now received a new port number to use for the nodes the
    starting node will communicate with.
  */
  DEBUG_RETURN(0);
error:
  DEBUG_PRINT(CONFIG_LEVEL,
    ("Protocol error in set connection parameter, code = %d", error));
  DEBUG_RETURN(error);
}

static int
send_set_connection_parameter(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                              guint32 cluster_id,
                              guint32 node1_id,
                              guint32 node2_id,
                              guint32 port_number)
{
  IC_INT_API_CONFIG_SERVER *apic= run_obj->apic;
  guint32 i;
  int error= 0;
  IC_CONNECTION *conn;
  IC_RUN_CLUSTER_STATE *rc_state= &run_obj->state;
  gboolean is_master;
  DEBUG_ENTRY("send_set_connection_parameter");

  is_master= rc_start_update(rc_state);
  if (is_master)
  {
    for (i= 0; i < IC_MAX_CLUSTER_SERVERS; i++)
    {
      if (apic->cluster_conn.cs_nodeid[i] == run_obj->cs_nodeid ||
          run_obj->state.cs_servers[i].conn == NULL)
        continue;
      conn= run_obj->state.cs_servers[i].conn;
      if ((error= send_set_connection_param_message(conn,
                                                    cluster_id,
                                                    node1_id,
                                                    node2_id,
                                                    port_number)) ||
          (error= rec_set_connection_param_message(conn)))
      {
        ;
      }
    }
  }
  else
  {
  }
  rc_stop_update(rc_state);
  DEBUG_RETURN(error);
}

static int
send_set_connection_param_message(IC_CONNECTION *conn,
                                  guint32 cluster_id,
                                  guint32 node1_id,
                                  guint32 node2_id,
                                  guint32 port_number)
{
  int error;
  DEBUG_ENTRY("send_set_connection_param_message");

  if ((error= ic_send_with_cr(conn, set_connection_parameter_str)) ||
      (error= ic_send_cluster_id(conn, cluster_id, TRUE)) ||
      (error= ic_send_with_cr_with_num(conn, node1_str, node1_id)) ||
      (error= ic_send_with_cr_with_num(conn, node2_str, node2_id)) ||
      (error= ic_send_with_cr_with_num(conn, param_str,
                                       SOCKET_SERVER_PORT_NUMBER)) ||
      (error= ic_send_with_cr_with_num(conn, value_str, port_number)) ||
      (error= ic_send_empty_line(conn)))
  {
    ;
  }
  DEBUG_RETURN(error);
}

static int
rec_set_connection_param_message(IC_CONNECTION *conn)
{
  int error;
  gchar rec_message_buf[32];
  gchar rec_result_buf[IC_MAX_ERROR_STRING_SIZE];
  DEBUG_ENTRY("rec_set_connection_param_message");

  if ((error= ic_rec_simple_str(conn,
                                set_connection_parameter_reply_str)) ||
      (error= ic_rec_string(conn, message_str, rec_message_buf)) ||
      (error= ic_rec_string(conn, result_str, rec_result_buf)) ||
      (error= ic_rec_empty_line(conn)))
    DEBUG_RETURN(error);
  if (strcmp(ic_empty_string, rec_message_buf) != 0 ||
      strcmp(ic_ok_str, rec_result_buf) != 0)
    error= ic_translate_error_string(rec_result_buf);
  DEBUG_RETURN(error);
}

/*
  MODULE: Handle Convert Transporter Request
  ------------------------------------------
*/
static int
handle_convert_transporter_request(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                                   IC_CONNECTION *conn,
                                   IC_RC_PARAM *param,
                                   gchar *read_buf,
                                   guint32 read_size,
                                   guint32 *error_line)
{
  int error;
  int trp_type= IC_TCP_TRANSPORTER_TYPE;
  gchar client_buf[64], cs_buf[64];
  DEBUG_ENTRY("handle_convert_transporter_request");

  if (ic_check_buf(read_buf, read_size, convert_transporter_str,
                   strlen(convert_transporter_str)))
  {
    *error_line= __LINE__;
    DEBUG_RETURN(IC_PROTOCOL_ERROR);
  }
  g_snprintf(client_buf, 64, "%d %d",
             (guint32)param->client_nodeid, trp_type);
  g_snprintf(cs_buf, 64, "%d %d",
             (guint32)param->node_number, trp_type);
  if ((error= ic_rec_simple_str(conn, ic_empty_string)) ||
      (error= ic_rec_simple_str(conn, client_buf)) ||
      (error= ic_send_with_cr(conn, cs_buf)))
  {
    DEBUG_PRINT(CONFIG_LEVEL,
      ("Protocol error in converting to transporter, code = %d", error));
    DEBUG_RETURN(error);
  }
  /*
    At this point the connection is turned into a NDB Protocol
    connection. We do this by giving to a send node connection
    in the Data API part. We also need to inform the Cluster
    Server Data API thread that this connection exists. This
    thread is responsible for ensuring that heartbeats are
    sent properly but also all other traffic between cluster
    server and other nodes using the NDB Protocol.
  */
  DEBUG_PRINT(CONFIG_LEVEL,
    ("We are now converting connection to a NDB Protocol connection"));
  if ((error=
         run_obj->apid_global->apid_global_ops->ic_external_connect(
                   run_obj->apid_global,
                   (guint32)param->cluster_id,
                   (guint32)param->node_number,
                   conn)))
  {
    *error_line= __LINE__;
    DEBUG_RETURN(error);
  }
  DEBUG_RETURN(0);
}

/*
  MODULE: Handle Cluster Configuration Request
  --------------------------------------------

 * This is implemented through a number of subroutine levels.
 * At first there is a set of methods to handle the protocol actions which
 * is part of this piece of the protocol. These are:
 * rec_get_nodeid_req: 
 * send_get_nodeid_reply:
 * rec_get_config_req:
 * send_get_config_reply:
 *
 * All of the above protocol methods are fairly simple using the standard
 * techniques used in the iClaustron protocol methods.
 *
 * The final routine is the method to get the base64-encoded configuration
 * string. This is implemented in the routine:
 * ic_get_base64_config
 * This method in turn uses a routine that gets the configuration as a large
 * of unsigned 32 bit values. This is implemented in the routine:
 * ic_get_key_value_sections_config
 *
 * This routine firsts calculates the length of the array and this is 
 * supported by the routines:
 * get_length_of_section: Calculates length of a configuration section
 * get_comm_section: Gets a communication section to calculate its length
 * ndb_mgm_str_word_len: Calculates length of ?
 * 
 * Finally the array is filled in, this is supported by the method:
 * fill_key_value_section: Fill in the key-value pairs for one section
 * This method uses the support method:
 * is_iclaustron_version
 * 
*/

static int rec_get_nodeid_req(IC_CONNECTION *conn,
                              guint64 *node_number,
                              guint64 *version_number,
                              guint64 *node_type,
                              guint64 *cluster_id);
static int send_get_nodeid_reply(IC_CONNECTION *conn, guint32 node_id);
static int rec_get_config_req(IC_CONNECTION *conn, guint64 version_number,
                              guint64 node_type);
static int send_get_config_reply(IC_CONNECTION *conn, gchar *config_base64_str,
                                 guint32 config_len);
static int ic_get_base64_config(IC_CLUSTER_CONFIG *clu_conf,
                                guint8 **base64_array,
                                guint32 *base64_array_len,
                                guint64 version_number);
static int ic_get_key_value_sections_config(IC_CLUSTER_CONFIG *clu_conf,
                                            guint32 **key_value_array,
                                            guint32 *key_value_array_len,
                                            guint64 version_number);
static guint32 get_length_of_section(IC_CONFIG_TYPES config_type,
                                     gchar *conf, guint64 version_number);
static IC_SOCKET_LINK_CONFIG*
get_comm_section(IC_CLUSTER_CONFIG *clu_conf,
                 IC_SOCKET_LINK_CONFIG *comm_section,
                 guint32 node1, guint32 node2);
static guint32 ndb_mgm_str_word_len(guint32 str_len);
static int fill_key_value_section(IC_CONFIG_TYPES config_type,
                                  gchar *conf,
                                  guint32 sect_id,
                                  guint32 *key_value_array,
                                  guint32 *key_value_array_len,
                                  guint64 version_number);
static gboolean is_iclaustron_version(guint64 version_number);

static int
handle_config_request(IC_INT_RUN_CLUSTER_SERVER *run_obj,
                      IC_CONNECTION *conn,
                      IC_RC_PARAM *param)
{
  int ret_code;
  guint8 *config_base64_str;
  guint32 config_len;
  IC_RUN_CLUSTER_STATE *rcs_state= &run_obj->state;
  GMutex *state_mutex= rcs_state->protect_state;
  DEBUG_ENTRY("handle_config_request");

  if ((ret_code= rec_get_nodeid_req(conn,
                                    &param->node_number,
                                    &param->version_number,
                                    &param->node_type,
                                    &param->cluster_id)))
    goto end;
  g_mutex_lock(state_mutex);
  if (rcs_state->cs_started &&
      is_cs_master(rcs_state))
  {
    ;
  }
  else if (rcs_state->cs_started &&
           !is_cs_master(rcs_state))
  {
    /* Send an error message to indicate we're not master */
    ;
  }
  else
  {
    /* Send an error message to indicate we're still in start-up phase */
    ;
  }
  g_mutex_unlock(state_mutex);
  if (param->node_number == 0)
  {
    /* Here we need to discover which node id to use */
    param->client_nodeid= 1; /* Fake for now */
  }
  else
  {
    /* Here we ensure that the requested node id is correct */
    param->client_nodeid= param->node_number;
  }
  if ((ret_code= send_get_nodeid_reply(conn, (guint32)param->client_nodeid)) ||
      (ret_code= rec_get_config_req(conn, param->version_number,
                                    param->node_type)) ||
      (ret_code= ic_get_base64_config(run_obj->conf_objects[param->cluster_id],
                                      &config_base64_str,
                                      &config_len,
                                      param->version_number)))
    goto end;
  DEBUG_PRINT(CONFIG_LEVEL,
    ("Converted configuration to a base64 representation"));
  ret_code= send_get_config_reply(conn, (gchar*)config_base64_str, config_len);
  ic_free((gchar*)config_base64_str);
end:
  if (ret_code)
  {
    DEBUG_PRINT(CONFIG_LEVEL,
      ("Error from handle_config_request, code = %u", ret_code));
  }
  DEBUG_RETURN(ret_code);
}

/* Handle receive of get node id request */
static int
rec_get_nodeid_req(IC_CONNECTION *conn,
                   guint64 *node_number,
                   guint64 *version_number,
                   guint64 *node_type,
                   guint64 *cluster_id)
{
  gchar *read_buf;
  guint32 read_size;
  guint32 state= VERSION_REQ_STATE; /* get nodeid already received */
  int error;
  DEBUG_ENTRY("rec_get_nodeid_req");

  while (!(error= ic_rec_with_cr(conn, &read_buf, &read_size)))
  {
    switch (state)
    {
      case VERSION_REQ_STATE:
        if (ic_check_buf_with_int(read_buf, read_size, ic_version_str,
                                  VERSION_REQ_LEN, version_number))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in version request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= NODETYPE_REQ_STATE;
        break;
      case NODETYPE_REQ_STATE:
        if (ic_check_buf_with_int(read_buf, read_size, nodetype_str,
                                  NODETYPE_REQ_LEN, node_type))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in nodetype request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= NODEID_REQ_STATE;
        break;
      case NODEID_REQ_STATE:
        if (ic_check_buf_with_int(read_buf, read_size, nodeid_str,
                                  NODEID_LEN, node_number) ||
            (*node_number > IC_MAX_NODE_ID))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in nodeid request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= USER_REQ_STATE;
        break;
      case USER_REQ_STATE:
        if (ic_check_buf(read_buf, read_size, user_str, USER_REQ_LEN))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in user request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= PASSWORD_REQ_STATE;
        break;
      case PASSWORD_REQ_STATE:
        if (ic_check_buf(read_buf, read_size, password_str, PASSWORD_REQ_LEN))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in password request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= PUBLIC_KEY_REQ_STATE;
        break;
      case PUBLIC_KEY_REQ_STATE:
        if (ic_check_buf(read_buf, read_size, public_key_str,
                         PUBLIC_KEY_REQ_LEN))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in public key request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= ENDIAN_REQ_STATE;
        break;
      case ENDIAN_REQ_STATE:
        if ((read_size < ENDIAN_REQ_LEN) ||
            (memcmp(read_buf, endian_str, ENDIAN_REQ_LEN) != 0))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in endian request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        if (!((read_size == ENDIAN_REQ_LEN + LITTLE_ENDIAN_LEN &&
              memcmp(read_buf+ENDIAN_REQ_LEN, little_endian_str,
                     LITTLE_ENDIAN_LEN) == 0) ||
             (read_size == ENDIAN_REQ_LEN + BIG_ENDIAN_LEN &&
              memcmp(read_buf+ENDIAN_REQ_LEN, big_endian_str,
                     BIG_ENDIAN_LEN) == 0)))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Failure in representation of what endian type"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= LOG_EVENT_REQ_STATE;
        break;
      case LOG_EVENT_REQ_STATE:
        if (ic_check_buf(read_buf, read_size, log_event_str,
                         LOG_EVENT_REQ_LEN))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in log_event request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        if (!ic_is_bit_set(*version_number, IC_PROTOCOL_BIT))
        {
          state= EMPTY_LINE_REQ_STATE;
          *cluster_id= 0;
        }
        else
          state= CLUSTER_ID_REQ_STATE;
        break;
      case CLUSTER_ID_REQ_STATE:
        if (ic_check_buf_with_int(read_buf, read_size, cluster_id_str,
                                  CLUSTER_ID_REQ_LEN, cluster_id))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in cluster id request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= EMPTY_LINE_REQ_STATE;
        break;
      case EMPTY_LINE_REQ_STATE:
        if (read_size == 0)
        {
          DEBUG_RETURN(0);
        }
        DEBUG_PRINT(CONFIG_LEVEL,
          ("Protocol error in empty line state"));
        PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        break;
      default:
        abort();
        break;
    }
  }
  DEBUG_PRINT(CONFIG_LEVEL,
    ("Error in receiving get node id request, error = %d", error));
  DEBUG_RETURN(error);
}

/* Handle send get node id reply protocol part of get configuration */
static int
send_get_nodeid_reply(IC_CONNECTION *conn, guint32 node_id)
{
  int error= 0;
  DEBUG_ENTRY("send_get_nodeid_reply");

  if (ic_send_with_cr(conn, get_nodeid_reply_str) ||
      ic_send_with_cr_with_num(conn, nodeid_str, (guint64)node_id) ||
      ic_send_with_cr(conn, result_ok_str) ||
      ic_send_empty_line(conn))
    error= conn->conn_op.ic_get_error_code(conn);
  DEBUG_RETURN(error);
}

/* Handle receive get configuration request protocol action */
static int
rec_get_config_req(IC_CONNECTION *conn, guint64 version_number,
                   guint64 node_type)
{
  gchar *read_buf;
  guint32 read_size;
  guint32 state= GET_CONFIG_REQ_STATE;
  guint64 read_version_num;
  guint64 read_node_type;
  int error;
  DEBUG_ENTRY("rec_get_config_req");

  while (!(error= ic_rec_with_cr(conn, &read_buf, &read_size)))
  {
    switch(state)
    {
      case GET_CONFIG_REQ_STATE:
        if (ic_check_buf(read_buf, read_size, get_config_str, GET_CONFIG_LEN))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in get config request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= VERSION_REQ_STATE;
        break;
      case VERSION_REQ_STATE:
        if (ic_check_buf_with_int(read_buf, read_size, ic_version_str,
                                  VERSION_REQ_LEN, &read_version_num) ||
            (version_number != read_version_num))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in version request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= NODETYPE_REQ_STATE;
        break;
      case NODETYPE_REQ_STATE:
        if (ic_check_buf_with_int(read_buf, read_size, nodetype_str,
                                  NODETYPE_REQ_LEN, &read_node_type) ||
            (node_type != read_node_type))
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in nodetype request state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        state= EMPTY_STATE;
        break;
      case EMPTY_STATE:
        if (read_size != 0)
        {
          DEBUG_PRINT(CONFIG_LEVEL,
            ("Protocol error in wait empty state"));
          PROTOCOL_CONN_CHECK_DEBUG_RETURN(FALSE);
        }
        DEBUG_RETURN(0);
      default:
        abort();
        break;
    }
  }
  DEBUG_PRINT(CONFIG_LEVEL,
    ("Error in receiving get config request, error = %d", error));
  DEBUG_RETURN(error);
}

/* Handle send configuration reply protocol action */
static int
send_get_config_reply(IC_CONNECTION *conn, gchar *config_base64_str,
                      guint32 config_len)
{
  gchar content_buf[64];
  int error= 0;
  DEBUG_ENTRY("send_get_config_reply");
 
  g_snprintf(content_buf, 64, "%s%u", content_len_str, config_len);
  if (ic_send_with_cr(conn, get_config_reply_str) ||
      ic_send_with_cr(conn, result_ok_str) ||
      ic_send_with_cr(conn, content_buf) ||
      ic_send_with_cr(conn, octet_stream_str) ||
      ic_send_with_cr(conn, content_encoding_str) ||
      ic_send_empty_line(conn) ||
      conn->conn_op.ic_write_connection(conn, (const void*)config_base64_str,
                                        config_len, 1) ||
      ic_send_empty_line(conn))
    error= conn->conn_op.ic_get_error_code(conn);
  DEBUG_RETURN(error);
}

/* Get base64 encoded string to send to client */
static int
ic_get_base64_config(IC_CLUSTER_CONFIG *clu_conf,
                     guint8 **base64_array,
                     guint32 *base64_array_len,
                     guint64 version_number)
{
  guint32 *key_value_array;
  guint32 key_value_array_len= 0;
  int ret_code;
  DEBUG_ENTRY("ic_get_base64_config");

  *base64_array= 0;
  if ((ret_code= ic_get_key_value_sections_config(clu_conf, &key_value_array,
                                                  &key_value_array_len,
                                                  version_number)))
    DEBUG_RETURN(ret_code);
  ret_code= ic_base64_encode(base64_array,
                             base64_array_len,
                             (const guint8*)key_value_array,
                             key_value_array_len*4);
  DEBUG_PRINT_BUF(CONFIG_LEVEL, *(gchar**)base64_array);
  ic_free(key_value_array);
  DEBUG_RETURN(ret_code);
}

/*
 * This routine is used to create an array of guint32 values which
 * can be base64-encoded for distribution to any node in an
 * iClaustron grid.
 *
 * It's ok for several threads in the iClaustron Cluster Server to
 * concurrently use this routine.
 */
static int
ic_get_key_value_sections_config(IC_CLUSTER_CONFIG *clu_conf,
                                 guint32 **key_value_array,
                                 guint32 *key_value_array_len,
                                 guint64 version_number)
{
  guint32 len= 0, num_comms= 0, api_nodes= 0;
  guint32 node_sect_len, i, j, checksum, system_len, data_server_section;
  guint32 section_id, comm_meta_section, node_meta_section;
  guint32 system_meta_section, data_server_start_section;
  guint32 *loc_key_value_array;
  guint32 loc_key_value_array_len= 0;
  int ret_code;
  IC_SOCKET_LINK_CONFIG test1, *comm_section;
  DEBUG_ENTRY("ic_get_key_value_sections_config");

  /*
   * Add 2 words for verification string in beginning
   * Add 3 key-value pairs for section 0
   * Add one key-value pair for each node section
   *   - This is section 1
   */
  len+= 2;
  len+= 6;
  len+= clu_conf->num_nodes * 2;
  DEBUG_PRINT(CONFIG_LEVEL, ("1: len=%u", len));
  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    /* Add length of each node section */
    if (clu_conf->node_config[i])
    {
      node_sect_len= get_length_of_section(
                          (IC_CONFIG_TYPES)clu_conf->node_types[i],
                                           clu_conf->node_config[i],
                                           version_number);
      if (node_sect_len == 0)
        return IC_ERROR_INCONSISTENT_DATA;
      len+= node_sect_len;
      DEBUG_PRINT(CONFIG_LEVEL, ("2: len=%u", len));
      if (clu_conf->node_types[i] != IC_DATA_SERVER_NODE)
        api_nodes++;
    }
  }
  /* Add length of each comm section */
  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    if (clu_conf->node_config[i])
    {
      for (j= i+1; j <= clu_conf->max_node_id; j++)
      {
        if (clu_conf->node_config[j])
        {
          /* iClaustron uses a fully connected cluster */
          if (clu_conf->node_types[i] == IC_DATA_SERVER_NODE ||
              clu_conf->node_types[j] == IC_DATA_SERVER_NODE ||
              ic_is_bit_set(version_number, IC_PROTOCOL_BIT))
          {
            /* We have found two nodes needing a comm section */
            comm_section= get_comm_section(clu_conf, &test1, i, j);
            len+= get_length_of_section(IC_COMM_TYPE, (gchar*)comm_section,
                                        version_number);
            num_comms++;
            DEBUG_PRINT(CONFIG_LEVEL, ("3: len=%u", len));
          }
        }
      }
    }
  }
  /* Add one key-value pair for meta section of system section */
  len+= 2;
  /* Add length of the system section */
  system_len= get_length_of_section(IC_SYSTEM_TYPE,
                                    (gchar*)&clu_conf->sys_conf,
                                    version_number);
  if (system_len == 0)
    DEBUG_RETURN(IC_ERROR_INCONSISTENT_DATA);
  len+= system_len;
  DEBUG_PRINT(CONFIG_LEVEL, ("4: len=%u", len));
  /*
   * Add one key-value pair for each comm section
   *   - This is meta section for communication
   */
  len+= num_comms * 2;
  DEBUG_PRINT(CONFIG_LEVEL, ("5: len=%u", len));
  /* Finally add 1 word for checksum at the end */
  len+= 1;

  DEBUG_PRINT(CONFIG_LEVEL, ("6: len=%u", len));
  /*
     Allocate memory for key-value pairs, this memory is only temporary for
     this method and its caller, so memory will be freed soon again
  */
  if (!num_comms)
    abort();
  if (!(loc_key_value_array= (guint32*)ic_calloc(4*len)))
    return IC_ERROR_MEM_ALLOC;
  *key_value_array= loc_key_value_array;
  /*
    Put in verification section
  */
  memcpy((gchar*)loc_key_value_array, ver_string, 8);

  /*
    Fill Section 0
      Id 2000 specifies section 1 as a section that specifies node sections
      Id 3000 specifies section number of the section that describes the
      communication sections
  */
  section_id= 0;
  node_meta_section= 1;
  system_meta_section= 2 + api_nodes;
  comm_meta_section= 2 + system_meta_section;
  loc_key_value_array[2]= 
     g_htonl((IC_SECTION_TYPE << IC_CL_KEY_SHIFT) +
             (section_id << IC_CL_SECT_SHIFT) +
             1000);
  loc_key_value_array[3]= g_htonl(system_meta_section << IC_CL_SECT_SHIFT);
  loc_key_value_array[4]= 
     g_htonl((IC_SECTION_TYPE << IC_CL_KEY_SHIFT) +
             (section_id << IC_CL_SECT_SHIFT) +
             2000);
  loc_key_value_array[5]= g_htonl(node_meta_section << IC_CL_SECT_SHIFT);

  loc_key_value_array[6]= 
    g_htonl((IC_SECTION_TYPE << IC_CL_KEY_SHIFT) +
            (section_id << IC_CL_SECT_SHIFT) +
            3000);
  loc_key_value_array[7]= g_htonl(comm_meta_section << IC_CL_SECT_SHIFT);
  loc_key_value_array_len= 8;

  /*
    Fill Section 1
    One key-value for each section that specifies a node, starting at
    section 2 and ending at section 2+num_nodes-1. First fill in
    API nodes and then the ones for Data Server nodes.
  */
  section_id++;
  for (i= 0; i < api_nodes; i++)
  {
    loc_key_value_array[loc_key_value_array_len++]=
              g_htonl((IC_CL_INT32_TYPE << IC_CL_KEY_SHIFT) +
                      (section_id << IC_CL_SECT_SHIFT) +
                      i);
    loc_key_value_array[loc_key_value_array_len++]=
              g_htonl((2+i) << IC_CL_SECT_SHIFT);
  }
  data_server_section= comm_meta_section + num_comms + 1;
  data_server_start_section= data_server_section;
  for (i= api_nodes; i < clu_conf->num_nodes; i++)
  {
    loc_key_value_array[loc_key_value_array_len++]=
              g_htonl((IC_CL_INT32_TYPE << IC_CL_KEY_SHIFT) +
                      (section_id << IC_CL_SECT_SHIFT) +
                      i);
    loc_key_value_array[loc_key_value_array_len++]=
      g_htonl(data_server_section << IC_CL_SECT_SHIFT);
    data_server_section++;
  }
  DEBUG_PRINT(CONFIG_LEVEL,
    ("1: fill_len=%u", loc_key_value_array_len));

  /* Fill API node sections */
  section_id++;
  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    if (clu_conf->node_config[i] &&
        clu_conf->node_types[i] != IC_DATA_SERVER_NODE &&
        (ret_code= fill_key_value_section(clu_conf->node_types[i],
                                          clu_conf->node_config[i],
                                          section_id++,
                                          loc_key_value_array,
                                          &loc_key_value_array_len,
                                          version_number)))
      goto error;
    DEBUG_PRINT(CONFIG_LEVEL, ("2: fill_len=%u", loc_key_value_array_len));
  }

  /* Fill system meta section */
  g_assert(system_meta_section == section_id);
  loc_key_value_array[loc_key_value_array_len++]=
                  g_htonl((IC_CL_INT32_TYPE << IC_CL_KEY_SHIFT) +
                          ((system_meta_section) << IC_CL_SECT_SHIFT));
  loc_key_value_array[loc_key_value_array_len++]=
                  g_htonl((system_meta_section + 1) << IC_CL_SECT_SHIFT);

  section_id++;
  DEBUG_PRINT(CONFIG_LEVEL, ("3: fill_len=%u", loc_key_value_array_len));
  /* Fill system section */
  if ((ret_code= fill_key_value_section(IC_SYSTEM_TYPE,
                                        (gchar*)&clu_conf->sys_conf,
                                        section_id,
                                        loc_key_value_array,
                                        &loc_key_value_array_len,
                                        version_number)))
    goto error;
  section_id++;
  DEBUG_PRINT(CONFIG_LEVEL, ("4: fill_len=%u", loc_key_value_array_len));
  /*
    Fill the communication sections, one for each pair of nodes
    that need to communicate and one meta section with pointers to
    each communication section.
  */
  g_assert(comm_meta_section == section_id);
  for (i= 0; i < num_comms; i++)
  {
    loc_key_value_array[loc_key_value_array_len++]= g_htonl(
                                   (IC_UINT32 << IC_CL_KEY_SHIFT) +
                                   (comm_meta_section << IC_CL_SECT_SHIFT) +
                                   i);
    loc_key_value_array[loc_key_value_array_len++]= g_htonl(
                              (comm_meta_section+i+1) << IC_CL_SECT_SHIFT);
  }

  DEBUG_PRINT(CONFIG_LEVEL,
    ("5: fill_len=%u", loc_key_value_array_len));
  section_id++;
  /* Fill comm sections */
  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    if (clu_conf->node_config[i])
    {
      for (j= i+1; j <= clu_conf->max_node_id; j++)
      {
        if (clu_conf->node_config[j])
        {
          /* iClaustron uses a fully connected cluster */
          if (clu_conf->node_types[i] == IC_DATA_SERVER_NODE ||
              clu_conf->node_types[j] == IC_DATA_SERVER_NODE ||
              ic_is_bit_set(version_number, IC_PROTOCOL_BIT))
          {
            /* We have found two nodes needing a comm section */
            comm_section= get_comm_section(clu_conf, &test1, i, j);
            if ((ret_code= fill_key_value_section(IC_COMM_TYPE,
                                                  (gchar*)comm_section,
                                                  section_id++,
                                                  loc_key_value_array,
                                                  &loc_key_value_array_len,
                                                  version_number)))
              goto error;
            DEBUG_PRINT(CONFIG_LEVEL,
              ("6: fill_len=%u", loc_key_value_array_len));
          }
        }
      }
    }
  }
  /* Fill in Data Server node sections */
  g_assert(data_server_start_section == section_id);
  for (i= 1; i <= clu_conf->max_node_id; i++)
  {
    if (clu_conf->node_config[i] &&
        clu_conf->node_types[i] == IC_DATA_SERVER_NODE &&
        (ret_code= fill_key_value_section(clu_conf->node_types[i],
                                          clu_conf->node_config[i],
                                          section_id++,
                                          loc_key_value_array,
                                          &loc_key_value_array_len,
                                          version_number)))
      goto error;
    DEBUG_PRINT(CONFIG_LEVEL, ("7: fill_len=%u", loc_key_value_array_len));
  }
  /* Calculate and fill out checksum */
  checksum= 0;
  for (i= 0; i < loc_key_value_array_len; i++)
    checksum^= g_ntohl(loc_key_value_array[i]);
  loc_key_value_array[loc_key_value_array_len++]= g_ntohl(checksum);
  DEBUG_PRINT(CONFIG_LEVEL,
    ("8: fill_len=%u", loc_key_value_array_len));
  /* Perform final set of checks */
  *key_value_array_len= loc_key_value_array_len;
  if (len == loc_key_value_array_len)
    DEBUG_RETURN(0);

  ret_code= IC_ERROR_INCONSISTENT_DATA;
error:
  ic_free(*key_value_array);
  DEBUG_RETURN(ret_code);
}

/*
 * Get communication section for calculation of its section length
 * This routine depends on that node1 < node2
 */
static IC_SOCKET_LINK_CONFIG*
get_comm_section(IC_CLUSTER_CONFIG *clu_conf,
                 IC_SOCKET_LINK_CONFIG *comm_section,
                 guint32 node1, guint32 node2)
{
  IC_SOCKET_LINK_CONFIG *local_comm_section;
  IC_DATA_SERVER_CONFIG *server_conf;
  IC_DATA_SERVER_CONFIG *client_conf;

  comm_section->first_node_id= node1;
  comm_section->second_node_id= node2;
  if ((local_comm_section= (IC_SOCKET_LINK_CONFIG*)
                           ic_hashtable_search(clu_conf->comm_hash,
                                               (void*)comm_section)))
    return local_comm_section;
  /* Check if we ever get here now */
  ic_require(FALSE);
  init_config_object((gchar*)comm_section, sizeof(IC_COMM_LINK_CONFIG),
                     IC_COMM_TYPE);
  comm_section->first_node_id= node1;
  comm_section->second_node_id= node2;
  if (clu_conf->node_types[node1] == IC_DATA_SERVER_NODE ||
      clu_conf->node_types[node2] != IC_DATA_SERVER_NODE)
  {
    comm_section->server_node_id= node1;
    server_conf= (IC_DATA_SERVER_CONFIG*)clu_conf->node_config[node1];
    client_conf= (IC_DATA_SERVER_CONFIG*)clu_conf->node_config[node2];
  }
  else
  {
    comm_section->server_node_id= node2;
    server_conf= (IC_DATA_SERVER_CONFIG*)clu_conf->node_config[node2];
    client_conf= (IC_DATA_SERVER_CONFIG*)clu_conf->node_config[node1];
  }
  comm_section->server_port_number= server_conf->port_number;
  comm_section->client_port_number= client_conf->port_number;
  comm_section->first_hostname= server_conf->hostname;
  comm_section->second_hostname= client_conf->hostname;
  return comm_section;
}

/* Special handling of string lengths in NDB Management Protocol */
static guint32
ndb_mgm_str_word_len(guint32 str_len)
{
  guint32 word_len;
  str_len++; /* To accomodate for final NULL */
  /* str_len++; */ /* For bug compatability with NDB MGM Protocol */
  word_len= (str_len+3)/4;
  return word_len;
}

/* Calculate length of a node, communication section */
static guint32
get_length_of_section(IC_CONFIG_TYPES config_type,
                      gchar *conf, guint64 version_number)
{
  IC_CONFIG_ENTRY *conf_entry;
  gchar **charptr;
  guint32 len= 0, i, str_len;

  for (i= 0; i < MAX_CONFIG_ID; i++)
  {
    conf_entry= &glob_conf_entry[i];
    if ((conf_entry->config_types & (1 << ((guint32)config_type))) &&
        (!conf_entry->is_not_sent) &&
        is_entry_used_in_version(conf_entry, version_number))
    {
      switch (conf_entry->data_type)
      {
        case IC_BOOLEAN:
        case IC_UINT16:
        case IC_UINT32:
          break;
        case IC_UINT64:
          len++;
          break;
        case IC_CHARPTR:
        {
          charptr= (gchar**)(conf+conf_entry->offset);
          str_len= 0;
          if (*charptr)
            str_len= strlen(*charptr);
          len+= ndb_mgm_str_word_len(str_len);
          break;
        }
        default:
          abort();
          break;
      }
      len+= 2;
    }
  }
  len+= 2; /* One key-value pair for node type */
  len+= 2; /* One key-value pair for parent node id */
  return len;
}

/* Fill in key-value pairs for a node or communication section */
static int
fill_key_value_section(IC_CONFIG_TYPES config_type,
                       gchar *conf,
                       guint32 sect_id,
                       guint32 *key_value_array,
                       guint32 *key_value_array_len,
                       guint64 version_number)
{
  IC_CONFIG_ENTRY *conf_entry;
  guint32 len= 0, i, key, config_id, value, data_type, str_len;
  guint32 *assign_array;
  gchar **charptr;
  guint32 loc_key_value_array_len= *key_value_array_len;
  DEBUG_ENTRY("fill_key_value_section");

  for (i= 0; i < MAX_CONFIG_ID; i++)
  {
    conf_entry= &glob_conf_entry[i];
    if ((conf_entry->config_types & (1 << ((guint32)config_type))) &&
        (!conf_entry->is_not_sent) &&
        is_entry_used_in_version(conf_entry, version_number))
    {
      assign_array= &key_value_array[loc_key_value_array_len];
      switch (conf_entry->data_type)
      {
        case IC_BOOLEAN:
        case IC_CHAR:
        {
          guint8 *entry= (guint8*)(conf+conf_entry->offset);
          value= (guint32)*entry;
          data_type= IC_CL_INT32_TYPE;
          break;
        }
        case IC_UINT16:
        {
          guint16 *entry= (guint16*)(conf+conf_entry->offset);
          value= (guint32)*entry;
          data_type= IC_CL_INT32_TYPE;
          break;
        }
        case IC_UINT32:
        {
          guint32 *entry= (guint32*)(conf+conf_entry->offset);
          value= (guint32)*entry;
          data_type= IC_CL_INT32_TYPE;
          break;
        }
        case IC_UINT64:
        {
          guint64 *entry= (guint64*)(conf+conf_entry->offset);
          value= *entry & 0xFFFFFFFF;
          assign_array[2]= g_htonl(value);
          value= (guint32)((guint64)(*entry >> 32));
          loc_key_value_array_len++;
          data_type= IC_CL_INT64_TYPE;
          DEBUG_PRINT(CONFIG_LEVEL,
                      ("64-bit value (low part) = %u, high part in next line",
                       g_ntohl(assign_array[2])));
          break;
        }
        case IC_CHARPTR:
        {
          charptr= (gchar**)(conf+conf_entry->offset);
          str_len= 0;
          if (*charptr)
            str_len= strlen(*charptr);
          value= str_len + 1; /* Reported length includes NULL byte */
          /* 
             Adjust to number of words with one word removed and
             an extra null byte calculated for
           */
          len= ndb_mgm_str_word_len(str_len);
          /* We don't need to copy null byte since we initialised to 0 */
          if (str_len)
            memcpy((gchar*)&assign_array[2],
                   *charptr,
                   str_len);
          DEBUG_PRINT(CONFIG_LEVEL,
                      ("String value = %s, str_len= %u",
                       (gchar*)&assign_array[2], str_len));
          loc_key_value_array_len+= len;
          data_type= IC_CL_CHAR_TYPE;
          break;
        }
        default:
        {
          DEBUG_RETURN(IC_ERROR_INCONSISTENT_DATA);
        }
      }
      /*
         Assign the key consisting of:
         1) Data Type
         2) Section id
         3) Config id
       */
      config_id= map_inx_to_config_id[i];
      key= (data_type << IC_CL_KEY_SHIFT) +
           (sect_id << IC_CL_SECT_SHIFT) +
           (config_id);
      assign_array[0]= g_htonl(key);
      assign_array[1]= g_htonl(value);
      DEBUG_PRINT(CONFIG_LEVEL,
                  ("sectid = %u, data_type = %u, config_id = %u, value = %u",
                   sect_id, data_type, config_id, value));
      loc_key_value_array_len+= 2;
    }
  }
  /* Add node type for all sections */
  assign_array= &key_value_array[loc_key_value_array_len];
  config_id= IC_NODE_TYPE;
  key= (IC_CL_INT32_TYPE << IC_CL_KEY_SHIFT) +
       (sect_id << IC_CL_SECT_SHIFT) +
       config_id;
  switch (config_type)
  {
    case IC_COMM_TYPE:
      value= 0;
      break;
    case IC_DATA_SERVER_TYPE:
    case IC_CLUSTER_SERVER_TYPE:
    /* config_type already contains the correct value */
      value= config_type;
      break;
    default:
      if (!is_iclaustron_version(version_number))
        value= IC_CLIENT_TYPE;
      else
        value= config_type;
      break;
  }
  DEBUG_PRINT(CONFIG_LEVEL,
              ("sectid = %u, config_id = %u, value = %u",
                sect_id, config_id, value));
  assign_array[0]= g_htonl(key);
  assign_array[1]= g_htonl(value);
  loc_key_value_array_len+= 2;

  /* Add parent id == 0 for all sections */
  assign_array= &key_value_array[loc_key_value_array_len];
  config_id= IC_PARENT_ID;
  key= (IC_CL_INT32_TYPE << IC_CL_KEY_SHIFT) +
       (sect_id << IC_CL_SECT_SHIFT) +
       config_id;
  value= (guint32)0;
  DEBUG_PRINT(CONFIG_LEVEL,
              ("sectid = %u, config_id = %u, value = %u",
                sect_id, config_id, value));
  assign_array[0]= g_htonl(key);
  assign_array[1]= g_htonl(value);
  loc_key_value_array_len+= 2;

  *key_value_array_len= loc_key_value_array_len;
  DEBUG_RETURN(0);
}

static gboolean
is_iclaustron_version(guint64 version_number)
{
  if (ic_is_bit_set(version_number, IC_PROTOCOL_BIT))
    return TRUE;
  return FALSE;
}
